{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6664,"status":"ok","timestamp":1730750590620,"user":{"displayName":"Mehak Bindra","userId":"14665907004427636351"},"user_tz":300},"id":"qK9gcyUWT6A1","outputId":"88aeb85b-1f78-41d2-e967-7e412c517ab1"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-11-27 19:20:44.337230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import scanpy as sc\n","from sklearn.model_selection import train_test_split\n","import torch\n","from model import *\n","import torch.optim as optim\n","from sklearn.metrics import mean_squared_error\n","from skimage.metrics import structural_similarity as ssim\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ya59I4ECkFub"},"outputs":[],"source":["data_dir = 'spatial_datasets/GSE213264_RAW/'\n","results_dir = 'results_spatial_coord_v2/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Eso-j6mtYVh","outputId":"2068e5f8-7855-4094-c73b-ba7b1a83036d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Epoch [1/100], Loss: 920.4719\n","Epoch [2/100], Loss: 362.5630\n","Epoch [3/100], Loss: 312.0036\n","Epoch [4/100], Loss: 287.6354\n","Epoch [5/100], Loss: 268.0638\n","Epoch [6/100], Loss: 251.3222\n","Epoch [7/100], Loss: 245.2160\n","Epoch [8/100], Loss: 236.7510\n","Epoch [9/100], Loss: 231.1918\n","Epoch [10/100], Loss: 225.2564\n","Epoch [11/100], Loss: 221.7005\n","Epoch [12/100], Loss: 219.7999\n","Epoch [13/100], Loss: 215.8561\n","Epoch [14/100], Loss: 216.2473\n","Epoch [15/100], Loss: 213.9908\n","Epoch [16/100], Loss: 211.7604\n","Epoch [17/100], Loss: 212.0762\n","Epoch [18/100], Loss: 209.9336\n","Epoch [19/100], Loss: 209.7597\n","Epoch [20/100], Loss: 206.8699\n","Epoch [21/100], Loss: 206.7922\n","Epoch [22/100], Loss: 206.2156\n","Epoch [23/100], Loss: 206.0194\n","Epoch [24/100], Loss: 205.0864\n","Epoch [25/100], Loss: 203.0295\n","Epoch [26/100], Loss: 203.5865\n","Epoch [27/100], Loss: 203.1798\n","Epoch [28/100], Loss: 203.1830\n","Epoch [29/100], Loss: 203.4373\n","Epoch [30/100], Loss: 203.0990\n","Epoch [31/100], Loss: 202.6020\n","Epoch [32/100], Loss: 201.0721\n","Epoch [33/100], Loss: 200.6862\n","Epoch [34/100], Loss: 201.1821\n","Epoch [35/100], Loss: 200.7539\n","Epoch [36/100], Loss: 199.9469\n","Epoch [37/100], Loss: 201.0544\n","Epoch [38/100], Loss: 200.9105\n","Epoch [39/100], Loss: 200.8459\n","Epoch [40/100], Loss: 199.6985\n","Epoch [41/100], Loss: 200.5314\n","Epoch [42/100], Loss: 199.7904\n","Epoch [43/100], Loss: 199.7654\n","Epoch [44/100], Loss: 200.1571\n","Epoch [45/100], Loss: 198.1246\n","Epoch [46/100], Loss: 199.7735\n","Epoch [47/100], Loss: 199.2702\n","Epoch [48/100], Loss: 199.4593\n","Epoch [49/100], Loss: 201.4952\n","Epoch [50/100], Loss: 199.6746\n","Epoch [51/100], Loss: 198.2224\n","Epoch [52/100], Loss: 198.5985\n","Epoch [53/100], Loss: 198.3585\n","Epoch [54/100], Loss: 198.8374\n","Epoch [55/100], Loss: 197.7328\n","Epoch [56/100], Loss: 198.5610\n","Epoch [57/100], Loss: 199.7281\n","Epoch [58/100], Loss: 197.7164\n","Epoch [59/100], Loss: 198.6825\n","Epoch [60/100], Loss: 196.9845\n","Epoch [61/100], Loss: 196.2873\n","Epoch [62/100], Loss: 195.7652\n","Epoch [63/100], Loss: 196.5786\n","Epoch [64/100], Loss: 194.4476\n","Epoch [65/100], Loss: 193.8716\n","Epoch [66/100], Loss: 192.9452\n","Epoch [67/100], Loss: 193.7111\n","Epoch [68/100], Loss: 192.4252\n","Epoch [69/100], Loss: 190.4449\n","Epoch [70/100], Loss: 190.7926\n","Epoch [71/100], Loss: 188.9281\n","Epoch [72/100], Loss: 189.8256\n","Epoch [73/100], Loss: 190.1049\n","Epoch [74/100], Loss: 190.2171\n","Epoch [75/100], Loss: 188.7284\n","Epoch [76/100], Loss: 188.0771\n","Epoch [77/100], Loss: 188.4850\n","Epoch [78/100], Loss: 187.1165\n","Epoch [79/100], Loss: 186.7758\n","Epoch [80/100], Loss: 187.7227\n","Epoch [81/100], Loss: 186.5626\n","Epoch [82/100], Loss: 187.9217\n","Epoch [83/100], Loss: 186.9016\n","Epoch [84/100], Loss: 187.0915\n","Epoch [85/100], Loss: 186.3199\n","Epoch [86/100], Loss: 186.1946\n","Epoch [87/100], Loss: 186.0771\n","Epoch [88/100], Loss: 186.4006\n","Epoch [89/100], Loss: 185.6717\n","Epoch [90/100], Loss: 185.3094\n","Epoch [91/100], Loss: 186.2300\n","Epoch [92/100], Loss: 186.3661\n","Epoch [93/100], Loss: 185.3210\n","Epoch [94/100], Loss: 185.5762\n","Epoch [95/100], Loss: 184.9183\n","Epoch [96/100], Loss: 185.7488\n","Epoch [97/100], Loss: 184.7342\n","Epoch [98/100], Loss: 185.0752\n","Epoch [99/100], Loss: 185.2488\n","Epoch [100/100], Loss: 184.1515\n","Processed humanGBM successfully.\n","Epoch [1/100], Loss: 1061.4823\n","Epoch [2/100], Loss: 649.3084\n","Epoch [3/100], Loss: 603.3153\n","Epoch [4/100], Loss: 568.8821\n","Epoch [5/100], Loss: 546.8450\n","Epoch [6/100], Loss: 528.0631\n","Epoch [7/100], Loss: 514.0620\n","Epoch [8/100], Loss: 505.4692\n","Epoch [9/100], Loss: 499.9608\n","Epoch [10/100], Loss: 490.4321\n","Epoch [11/100], Loss: 487.9204\n","Epoch [12/100], Loss: 485.4071\n","Epoch [13/100], Loss: 480.3837\n","Epoch [14/100], Loss: 480.7091\n","Epoch [15/100], Loss: 473.8813\n","Epoch [16/100], Loss: 467.2069\n","Epoch [17/100], Loss: 463.1543\n","Epoch [18/100], Loss: 455.1352\n","Epoch [19/100], Loss: 449.7554\n","Epoch [20/100], Loss: 444.8670\n","Epoch [21/100], Loss: 439.1796\n","Epoch [22/100], Loss: 437.0811\n","Epoch [23/100], Loss: 434.5214\n","Epoch [24/100], Loss: 431.0903\n","Epoch [25/100], Loss: 428.2463\n","Epoch [26/100], Loss: 428.9192\n","Epoch [27/100], Loss: 424.1897\n","Epoch [28/100], Loss: 423.4850\n","Epoch [29/100], Loss: 423.9007\n","Epoch [30/100], Loss: 423.3166\n","Epoch [31/100], Loss: 421.7890\n","Epoch [32/100], Loss: 419.2351\n","Epoch [33/100], Loss: 420.6644\n","Epoch [34/100], Loss: 418.2579\n","Epoch [35/100], Loss: 418.8563\n","Epoch [36/100], Loss: 417.7647\n","Epoch [37/100], Loss: 417.3140\n","Epoch [38/100], Loss: 416.5748\n","Epoch [39/100], Loss: 416.9609\n","Epoch [40/100], Loss: 414.3881\n","Epoch [41/100], Loss: 415.5276\n","Epoch [42/100], Loss: 415.7124\n","Epoch [43/100], Loss: 414.1858\n","Epoch [44/100], Loss: 414.3984\n","Epoch [45/100], Loss: 413.4105\n","Epoch [46/100], Loss: 412.8672\n","Epoch [47/100], Loss: 412.7400\n","Epoch [48/100], Loss: 412.8238\n","Epoch [49/100], Loss: 412.7072\n","Epoch [50/100], Loss: 411.5092\n","Epoch [51/100], Loss: 411.9260\n","Epoch [52/100], Loss: 412.0891\n","Epoch [53/100], Loss: 411.2671\n","Epoch [54/100], Loss: 412.7590\n","Epoch [55/100], Loss: 410.8980\n","Epoch [56/100], Loss: 410.3264\n","Epoch [57/100], Loss: 409.3224\n","Epoch [58/100], Loss: 409.8970\n","Epoch [59/100], Loss: 410.4854\n","Epoch [60/100], Loss: 409.1471\n","Epoch [61/100], Loss: 409.7366\n","Epoch [62/100], Loss: 409.4023\n","Epoch [63/100], Loss: 409.3300\n","Epoch [64/100], Loss: 408.2821\n","Epoch [65/100], Loss: 408.4308\n","Epoch [66/100], Loss: 407.1895\n","Epoch [67/100], Loss: 406.2161\n","Epoch [68/100], Loss: 406.0479\n","Epoch [69/100], Loss: 404.9817\n","Epoch [70/100], Loss: 404.5712\n","Epoch [71/100], Loss: 403.4665\n","Epoch [72/100], Loss: 402.6025\n","Epoch [73/100], Loss: 401.5700\n","Epoch [74/100], Loss: 402.4315\n","Epoch [75/100], Loss: 400.9894\n","Epoch [76/100], Loss: 400.9628\n","Epoch [77/100], Loss: 399.4155\n","Epoch [78/100], Loss: 400.3644\n","Epoch [79/100], Loss: 398.9537\n","Epoch [80/100], Loss: 400.1816\n","Epoch [81/100], Loss: 399.7705\n","Epoch [82/100], Loss: 400.3185\n","Epoch [83/100], Loss: 400.2905\n","Epoch [84/100], Loss: 399.1249\n","Epoch [85/100], Loss: 399.7618\n","Epoch [86/100], Loss: 398.7726\n","Epoch [87/100], Loss: 399.0743\n","Epoch [88/100], Loss: 398.4330\n","Epoch [89/100], Loss: 399.2993\n","Epoch [90/100], Loss: 397.2517\n","Epoch [91/100], Loss: 397.7262\n","Epoch [92/100], Loss: 397.7743\n","Epoch [93/100], Loss: 396.4036\n","Epoch [94/100], Loss: 396.3147\n","Epoch [95/100], Loss: 397.3457\n","Epoch [96/100], Loss: 396.4734\n","Epoch [97/100], Loss: 396.6870\n","Epoch [98/100], Loss: 395.6822\n","Epoch [99/100], Loss: 396.0655\n","Epoch [100/100], Loss: 396.0218\n","Processed humanskin successfully.\n","Epoch [1/100], Loss: 877.8228\n","Epoch [2/100], Loss: 315.9412\n","Epoch [3/100], Loss: 257.3800\n","Epoch [4/100], Loss: 220.7592\n","Epoch [5/100], Loss: 197.9753\n","Epoch [6/100], Loss: 184.5288\n","Epoch [7/100], Loss: 173.5591\n","Epoch [8/100], Loss: 165.8537\n","Epoch [9/100], Loss: 161.2821\n","Epoch [10/100], Loss: 157.3884\n","Epoch [11/100], Loss: 151.6189\n","Epoch [12/100], Loss: 150.6072\n","Epoch [13/100], Loss: 148.8287\n","Epoch [14/100], Loss: 146.4745\n","Epoch [15/100], Loss: 143.9625\n","Epoch [16/100], Loss: 143.9955\n","Epoch [17/100], Loss: 143.0022\n","Epoch [18/100], Loss: 142.1890\n","Epoch [19/100], Loss: 139.8477\n","Epoch [20/100], Loss: 138.4026\n","Epoch [21/100], Loss: 139.0343\n","Epoch [22/100], Loss: 139.2891\n","Epoch [23/100], Loss: 137.6584\n","Epoch [24/100], Loss: 138.0998\n","Epoch [25/100], Loss: 136.2943\n","Epoch [26/100], Loss: 136.7335\n","Epoch [27/100], Loss: 136.6948\n","Epoch [28/100], Loss: 136.7701\n","Epoch [29/100], Loss: 134.9205\n","Epoch [30/100], Loss: 135.7786\n","Epoch [31/100], Loss: 134.2879\n","Epoch [32/100], Loss: 135.3196\n","Epoch [33/100], Loss: 135.4395\n","Epoch [34/100], Loss: 134.5158\n","Epoch [35/100], Loss: 134.4269\n","Epoch [36/100], Loss: 133.3375\n","Epoch [37/100], Loss: 134.1474\n","Epoch [38/100], Loss: 133.0297\n","Epoch [39/100], Loss: 133.7131\n","Epoch [40/100], Loss: 133.4103\n","Epoch [41/100], Loss: 132.5040\n","Epoch [42/100], Loss: 133.2642\n","Epoch [43/100], Loss: 132.7886\n","Epoch [44/100], Loss: 133.0689\n","Epoch [45/100], Loss: 132.9231\n","Epoch [46/100], Loss: 133.1715\n","Epoch [47/100], Loss: 133.1586\n","Epoch [48/100], Loss: 132.6622\n","Epoch [49/100], Loss: 131.9471\n","Epoch [50/100], Loss: 132.0055\n","Epoch [51/100], Loss: 132.2006\n","Epoch [52/100], Loss: 131.9217\n","Epoch [53/100], Loss: 132.4535\n","Epoch [54/100], Loss: 131.6889\n","Epoch [55/100], Loss: 131.3781\n","Epoch [56/100], Loss: 132.9789\n","Epoch [57/100], Loss: 131.5360\n","Epoch [58/100], Loss: 131.2556\n","Epoch [59/100], Loss: 131.7912\n","Epoch [60/100], Loss: 131.1528\n","Epoch [61/100], Loss: 131.4781\n","Epoch [62/100], Loss: 131.9966\n","Epoch [63/100], Loss: 131.3171\n","Epoch [64/100], Loss: 131.0269\n","Epoch [65/100], Loss: 131.1937\n","Epoch [66/100], Loss: 131.4136\n","Epoch [67/100], Loss: 131.6020\n","Epoch [68/100], Loss: 130.8746\n","Epoch [69/100], Loss: 131.2871\n","Epoch [70/100], Loss: 129.6981\n","Epoch [71/100], Loss: 130.8025\n","Epoch [72/100], Loss: 130.8981\n","Epoch [73/100], Loss: 130.0154\n","Epoch [74/100], Loss: 130.5874\n","Epoch [75/100], Loss: 130.6258\n","Epoch [76/100], Loss: 131.1761\n","Epoch [77/100], Loss: 130.2037\n","Epoch [78/100], Loss: 130.9176\n","Epoch [79/100], Loss: 130.4166\n","Epoch [80/100], Loss: 129.7149\n","Epoch [81/100], Loss: 129.7999\n","Epoch [82/100], Loss: 129.6307\n","Epoch [83/100], Loss: 130.3199\n","Epoch [84/100], Loss: 130.1582\n","Epoch [85/100], Loss: 130.1632\n","Epoch [86/100], Loss: 129.7238\n","Epoch [87/100], Loss: 129.3317\n","Epoch [88/100], Loss: 130.0126\n","Epoch [89/100], Loss: 130.2951\n","Epoch [90/100], Loss: 129.7162\n","Epoch [91/100], Loss: 129.9191\n","Epoch [92/100], Loss: 129.4658\n","Epoch [93/100], Loss: 129.5187\n","Epoch [94/100], Loss: 130.3955\n","Epoch [95/100], Loss: 129.7347\n","Epoch [96/100], Loss: 128.7221\n","Epoch [97/100], Loss: 129.8648\n","Epoch [98/100], Loss: 129.1273\n","Epoch [99/100], Loss: 129.0352\n","Epoch [100/100], Loss: 128.5971\n","Processed humanthymus successfully.\n","Epoch [1/100], Loss: 903.3104\n","Epoch [2/100], Loss: 476.7489\n","Epoch [3/100], Loss: 425.3799\n","Epoch [4/100], Loss: 392.8878\n","Epoch [5/100], Loss: 372.3985\n","Epoch [6/100], Loss: 359.4216\n","Epoch [7/100], Loss: 350.3462\n","Epoch [8/100], Loss: 343.3257\n","Epoch [9/100], Loss: 337.1587\n","Epoch [10/100], Loss: 334.1381\n","Epoch [11/100], Loss: 332.9795\n","Epoch [12/100], Loss: 330.5424\n","Epoch [13/100], Loss: 329.1092\n","Epoch [14/100], Loss: 326.6124\n","Epoch [15/100], Loss: 324.7755\n","Epoch [16/100], Loss: 323.7295\n","Epoch [17/100], Loss: 324.0175\n","Epoch [18/100], Loss: 320.1773\n","Epoch [19/100], Loss: 321.7863\n","Epoch [20/100], Loss: 320.1507\n","Epoch [21/100], Loss: 319.5055\n","Epoch [22/100], Loss: 319.2674\n","Epoch [23/100], Loss: 319.1236\n","Epoch [24/100], Loss: 317.8848\n","Epoch [25/100], Loss: 317.6662\n","Epoch [26/100], Loss: 317.5383\n","Epoch [27/100], Loss: 317.0887\n","Epoch [28/100], Loss: 316.9179\n","Epoch [29/100], Loss: 317.3505\n","Epoch [30/100], Loss: 316.1599\n","Epoch [31/100], Loss: 315.9010\n","Epoch [32/100], Loss: 316.7247\n","Epoch [33/100], Loss: 315.5037\n","Epoch [34/100], Loss: 315.1144\n","Epoch [35/100], Loss: 315.0665\n","Epoch [36/100], Loss: 314.5576\n","Epoch [37/100], Loss: 312.9031\n","Epoch [38/100], Loss: 312.0636\n","Epoch [39/100], Loss: 309.2455\n","Epoch [40/100], Loss: 308.5977\n","Epoch [41/100], Loss: 306.4656\n","Epoch [42/100], Loss: 305.7854\n","Epoch [43/100], Loss: 305.9353\n","Epoch [44/100], Loss: 304.0259\n","Epoch [45/100], Loss: 304.9316\n","Epoch [46/100], Loss: 302.5756\n","Epoch [47/100], Loss: 303.2659\n","Epoch [48/100], Loss: 302.2722\n","Epoch [49/100], Loss: 302.0843\n","Epoch [50/100], Loss: 301.0596\n","Epoch [51/100], Loss: 301.0887\n","Epoch [52/100], Loss: 300.7535\n","Epoch [53/100], Loss: 300.6958\n","Epoch [54/100], Loss: 300.1868\n","Epoch [55/100], Loss: 299.8069\n","Epoch [56/100], Loss: 300.1627\n","Epoch [57/100], Loss: 299.9977\n","Epoch [58/100], Loss: 299.7348\n","Epoch [59/100], Loss: 299.5218\n","Epoch [60/100], Loss: 298.8752\n","Epoch [61/100], Loss: 299.3715\n","Epoch [62/100], Loss: 298.2820\n","Epoch [63/100], Loss: 298.5564\n","Epoch [64/100], Loss: 298.5690\n","Epoch [65/100], Loss: 298.2188\n","Epoch [66/100], Loss: 298.2170\n","Epoch [67/100], Loss: 298.8121\n","Epoch [68/100], Loss: 299.2816\n","Epoch [69/100], Loss: 298.4338\n","Epoch [70/100], Loss: 297.9096\n","Epoch [71/100], Loss: 298.6179\n","Epoch [72/100], Loss: 297.7317\n","Epoch [73/100], Loss: 297.3736\n","Epoch [74/100], Loss: 297.2166\n","Epoch [75/100], Loss: 296.4712\n","Epoch [76/100], Loss: 297.2249\n","Epoch [77/100], Loss: 296.8874\n","Epoch [78/100], Loss: 297.1212\n","Epoch [79/100], Loss: 297.9861\n","Epoch [80/100], Loss: 297.2456\n","Epoch [81/100], Loss: 297.2409\n","Epoch [82/100], Loss: 296.8666\n","Epoch [83/100], Loss: 296.4711\n","Epoch [84/100], Loss: 297.0639\n","Epoch [85/100], Loss: 296.0953\n","Epoch [86/100], Loss: 296.5626\n","Epoch [87/100], Loss: 296.3262\n","Epoch [88/100], Loss: 296.5217\n","Epoch [89/100], Loss: 296.4245\n","Epoch [90/100], Loss: 295.7905\n","Epoch [91/100], Loss: 296.5771\n","Epoch [92/100], Loss: 296.1149\n","Epoch [93/100], Loss: 296.5883\n","Epoch [94/100], Loss: 296.1394\n","Epoch [95/100], Loss: 295.7826\n","Epoch [96/100], Loss: 295.3818\n","Epoch [97/100], Loss: 295.5557\n","Epoch [98/100], Loss: 295.3457\n","Epoch [99/100], Loss: 295.5009\n","Epoch [100/100], Loss: 295.6273\n","Processed humanspleen successfully.\n","Epoch [1/100], Loss: 810.0190\n","Epoch [2/100], Loss: 306.2710\n","Epoch [3/100], Loss: 246.7265\n","Epoch [4/100], Loss: 213.3402\n","Epoch [5/100], Loss: 192.2488\n","Epoch [6/100], Loss: 178.2537\n","Epoch [7/100], Loss: 169.2051\n","Epoch [8/100], Loss: 161.9875\n","Epoch [9/100], Loss: 158.4235\n","Epoch [10/100], Loss: 152.7729\n","Epoch [11/100], Loss: 150.6354\n","Epoch [12/100], Loss: 149.8117\n","Epoch [13/100], Loss: 146.0610\n","Epoch [14/100], Loss: 144.0835\n","Epoch [15/100], Loss: 142.6512\n","Epoch [16/100], Loss: 142.2088\n","Epoch [17/100], Loss: 140.9762\n","Epoch [18/100], Loss: 140.0116\n","Epoch [19/100], Loss: 139.3250\n","Epoch [20/100], Loss: 139.6068\n","Epoch [21/100], Loss: 137.9950\n","Epoch [22/100], Loss: 136.5254\n","Epoch [23/100], Loss: 137.0669\n","Epoch [24/100], Loss: 136.0829\n","Epoch [25/100], Loss: 136.0876\n","Epoch [26/100], Loss: 134.8034\n","Epoch [27/100], Loss: 135.5726\n","Epoch [28/100], Loss: 134.6431\n","Epoch [29/100], Loss: 134.2596\n","Epoch [30/100], Loss: 134.2274\n","Epoch [31/100], Loss: 134.8611\n","Epoch [32/100], Loss: 134.3428\n","Epoch [33/100], Loss: 134.3223\n","Epoch [34/100], Loss: 133.7708\n","Epoch [35/100], Loss: 133.9810\n","Epoch [36/100], Loss: 133.9357\n","Epoch [37/100], Loss: 134.5024\n","Epoch [38/100], Loss: 132.5248\n","Epoch [39/100], Loss: 133.5221\n","Epoch [40/100], Loss: 133.2170\n","Epoch [41/100], Loss: 132.9162\n","Epoch [42/100], Loss: 132.5906\n","Epoch [43/100], Loss: 132.9823\n","Epoch [44/100], Loss: 133.1440\n","Epoch [45/100], Loss: 132.1873\n","Epoch [46/100], Loss: 132.6664\n","Epoch [47/100], Loss: 132.3061\n","Epoch [48/100], Loss: 132.9771\n","Epoch [49/100], Loss: 133.1485\n","Epoch [50/100], Loss: 131.4394\n","Epoch [51/100], Loss: 131.8189\n","Epoch [52/100], Loss: 132.0028\n","Epoch [53/100], Loss: 132.3893\n","Epoch [54/100], Loss: 131.0854\n","Epoch [55/100], Loss: 131.9205\n","Epoch [56/100], Loss: 131.2917\n","Epoch [57/100], Loss: 130.7171\n","Epoch [58/100], Loss: 131.2849\n","Epoch [59/100], Loss: 129.5434\n","Epoch [60/100], Loss: 131.5490\n","Epoch [61/100], Loss: 130.9426\n","Epoch [62/100], Loss: 130.6826\n","Epoch [63/100], Loss: 130.8915\n","Epoch [64/100], Loss: 130.5807\n","Epoch [65/100], Loss: 129.4344\n","Epoch [66/100], Loss: 130.0498\n","Epoch [67/100], Loss: 129.7935\n","Epoch [68/100], Loss: 130.0477\n","Epoch [69/100], Loss: 131.1628\n","Epoch [70/100], Loss: 131.3047\n","Epoch [71/100], Loss: 131.1775\n","Epoch [72/100], Loss: 129.7930\n","Epoch [73/100], Loss: 130.4676\n","Epoch [74/100], Loss: 130.4903\n","Epoch [75/100], Loss: 129.8660\n","Epoch [76/100], Loss: 129.4255\n","Epoch [77/100], Loss: 130.4001\n","Epoch [78/100], Loss: 129.9924\n","Epoch [79/100], Loss: 130.5558\n","Epoch [80/100], Loss: 129.9921\n","Epoch [81/100], Loss: 129.4829\n","Epoch [82/100], Loss: 129.5758\n","Epoch [83/100], Loss: 130.6156\n","Epoch [84/100], Loss: 129.9131\n","Epoch [85/100], Loss: 129.8898\n","Epoch [86/100], Loss: 130.0667\n","Epoch [87/100], Loss: 129.8322\n","Epoch [88/100], Loss: 129.8646\n","Epoch [89/100], Loss: 129.7744\n","Epoch [90/100], Loss: 129.4870\n","Epoch [91/100], Loss: 129.8007\n","Epoch [92/100], Loss: 129.1649\n","Epoch [93/100], Loss: 129.5746\n","Epoch [94/100], Loss: 129.2404\n","Epoch [95/100], Loss: 129.6814\n","Epoch [96/100], Loss: 129.0624\n","Epoch [97/100], Loss: 129.2451\n","Epoch [98/100], Loss: 129.2599\n","Epoch [99/100], Loss: 128.5017\n","Epoch [100/100], Loss: 129.2225\n","Processed humantonsil successfully.\n","Epoch [1/100], Loss: 737.4930\n","Epoch [2/100], Loss: 344.8236\n","Epoch [3/100], Loss: 304.6823\n","Epoch [4/100], Loss: 282.6069\n","Epoch [5/100], Loss: 267.2249\n","Epoch [6/100], Loss: 255.8438\n","Epoch [7/100], Loss: 247.9787\n","Epoch [8/100], Loss: 241.4562\n","Epoch [9/100], Loss: 237.0801\n","Epoch [10/100], Loss: 234.2454\n","Epoch [11/100], Loss: 231.9963\n","Epoch [12/100], Loss: 229.9072\n","Epoch [13/100], Loss: 229.2169\n","Epoch [14/100], Loss: 227.7981\n","Epoch [15/100], Loss: 225.6590\n","Epoch [16/100], Loss: 224.1677\n","Epoch [17/100], Loss: 224.7840\n","Epoch [18/100], Loss: 222.4460\n","Epoch [19/100], Loss: 222.5721\n","Epoch [20/100], Loss: 221.8462\n","Epoch [21/100], Loss: 221.9789\n","Epoch [22/100], Loss: 221.3799\n","Epoch [23/100], Loss: 221.1577\n","Epoch [24/100], Loss: 220.8448\n","Epoch [25/100], Loss: 219.8580\n","Epoch [26/100], Loss: 219.9720\n","Epoch [27/100], Loss: 219.7246\n","Epoch [28/100], Loss: 219.5027\n","Epoch [29/100], Loss: 219.0538\n","Epoch [30/100], Loss: 219.0095\n","Epoch [31/100], Loss: 218.9215\n","Epoch [32/100], Loss: 218.9018\n","Epoch [33/100], Loss: 217.7551\n","Epoch [34/100], Loss: 218.2803\n","Epoch [35/100], Loss: 218.2948\n","Epoch [36/100], Loss: 218.5104\n","Epoch [37/100], Loss: 216.7951\n","Epoch [38/100], Loss: 217.0774\n","Epoch [39/100], Loss: 217.2821\n","Epoch [40/100], Loss: 216.4969\n","Epoch [41/100], Loss: 217.4854\n","Epoch [42/100], Loss: 216.8504\n","Epoch [43/100], Loss: 216.5849\n","Epoch [44/100], Loss: 217.0843\n","Epoch [45/100], Loss: 216.7691\n","Epoch [46/100], Loss: 216.3513\n","Epoch [47/100], Loss: 216.0251\n","Epoch [48/100], Loss: 216.3316\n","Epoch [49/100], Loss: 216.1396\n","Epoch [50/100], Loss: 215.3015\n","Epoch [51/100], Loss: 215.8281\n","Epoch [52/100], Loss: 216.3047\n","Epoch [53/100], Loss: 216.3994\n","Epoch [54/100], Loss: 215.9642\n","Epoch [55/100], Loss: 216.2236\n","Epoch [56/100], Loss: 216.1714\n","Epoch [57/100], Loss: 215.7291\n","Epoch [58/100], Loss: 216.0410\n","Epoch [59/100], Loss: 215.5443\n","Epoch [60/100], Loss: 215.9354\n","Epoch [61/100], Loss: 215.7423\n","Epoch [62/100], Loss: 215.5785\n","Epoch [63/100], Loss: 215.8607\n","Epoch [64/100], Loss: 214.4548\n","Epoch [65/100], Loss: 215.0051\n","Epoch [66/100], Loss: 215.6736\n","Epoch [67/100], Loss: 215.4133\n","Epoch [68/100], Loss: 214.9521\n","Epoch [69/100], Loss: 214.4892\n","Epoch [70/100], Loss: 214.9726\n","Epoch [71/100], Loss: 215.4360\n","Epoch [72/100], Loss: 214.8840\n","Epoch [73/100], Loss: 214.7695\n","Epoch [74/100], Loss: 214.4773\n","Epoch [75/100], Loss: 214.4749\n","Epoch [76/100], Loss: 215.0477\n","Epoch [77/100], Loss: 214.8536\n","Epoch [78/100], Loss: 213.9986\n","Epoch [79/100], Loss: 214.2861\n","Epoch [80/100], Loss: 214.1692\n","Epoch [81/100], Loss: 213.7652\n","Epoch [82/100], Loss: 214.3718\n","Epoch [83/100], Loss: 213.8985\n","Epoch [84/100], Loss: 213.9505\n","Epoch [85/100], Loss: 214.4109\n","Epoch [86/100], Loss: 214.7529\n","Epoch [87/100], Loss: 213.6924\n","Epoch [88/100], Loss: 214.2562\n","Epoch [89/100], Loss: 214.4687\n","Epoch [90/100], Loss: 213.6882\n","Epoch [91/100], Loss: 214.7199\n","Epoch [92/100], Loss: 214.3293\n","Epoch [93/100], Loss: 213.6579\n","Epoch [94/100], Loss: 213.3696\n","Epoch [95/100], Loss: 214.1437\n","Epoch [96/100], Loss: 213.9534\n","Epoch [97/100], Loss: 213.4339\n","Epoch [98/100], Loss: 214.3928\n","Epoch [99/100], Loss: 214.3237\n","Epoch [100/100], Loss: 213.1835\n","Processed mousekidney successfully.\n","Epoch [1/100], Loss: 1153.4341\n","Epoch [2/100], Loss: 540.5246\n","Epoch [3/100], Loss: 456.6028\n","Epoch [4/100], Loss: 431.1328\n","Epoch [5/100], Loss: 415.5814\n","Epoch [6/100], Loss: 402.9375\n","Epoch [7/100], Loss: 392.2315\n","Epoch [8/100], Loss: 382.0090\n","Epoch [9/100], Loss: 370.5490\n","Epoch [10/100], Loss: 369.0892\n","Epoch [11/100], Loss: 360.6007\n","Epoch [12/100], Loss: 355.5696\n","Epoch [13/100], Loss: 346.2146\n","Epoch [14/100], Loss: 346.8856\n","Epoch [15/100], Loss: 344.4341\n","Epoch [16/100], Loss: 341.3007\n","Epoch [17/100], Loss: 336.4313\n","Epoch [18/100], Loss: 334.3522\n","Epoch [19/100], Loss: 330.4224\n","Epoch [20/100], Loss: 330.8373\n","Epoch [21/100], Loss: 329.5662\n","Epoch [22/100], Loss: 328.0261\n","Epoch [23/100], Loss: 326.3760\n","Epoch [24/100], Loss: 324.1861\n","Epoch [25/100], Loss: 326.6500\n","Epoch [26/100], Loss: 320.8089\n","Epoch [27/100], Loss: 321.3846\n","Epoch [28/100], Loss: 319.4478\n","Epoch [29/100], Loss: 317.4500\n","Epoch [30/100], Loss: 310.2009\n","Epoch [31/100], Loss: 301.8718\n","Epoch [32/100], Loss: 293.2595\n","Epoch [33/100], Loss: 289.2852\n","Epoch [34/100], Loss: 283.8774\n","Epoch [35/100], Loss: 279.2447\n","Epoch [36/100], Loss: 273.1000\n","Epoch [37/100], Loss: 271.3730\n","Epoch [38/100], Loss: 270.0668\n","Epoch [39/100], Loss: 267.0364\n","Epoch [40/100], Loss: 266.0514\n","Epoch [41/100], Loss: 265.3372\n","Epoch [42/100], Loss: 262.2001\n","Epoch [43/100], Loss: 262.6150\n","Epoch [44/100], Loss: 260.6935\n","Epoch [45/100], Loss: 260.0840\n","Epoch [46/100], Loss: 259.7264\n","Epoch [47/100], Loss: 259.6372\n","Epoch [48/100], Loss: 257.4872\n","Epoch [49/100], Loss: 255.7822\n","Epoch [50/100], Loss: 258.5489\n","Epoch [51/100], Loss: 256.7785\n","Epoch [52/100], Loss: 254.1221\n","Epoch [53/100], Loss: 255.0262\n","Epoch [54/100], Loss: 255.3164\n","Epoch [55/100], Loss: 254.9833\n","Epoch [56/100], Loss: 252.1763\n","Epoch [57/100], Loss: 254.0119\n","Epoch [58/100], Loss: 252.5842\n","Epoch [59/100], Loss: 251.6927\n","Epoch [60/100], Loss: 251.5661\n","Epoch [61/100], Loss: 252.1596\n","Epoch [62/100], Loss: 251.2433\n","Epoch [63/100], Loss: 251.2102\n","Epoch [64/100], Loss: 251.0184\n","Epoch [65/100], Loss: 249.5589\n","Epoch [66/100], Loss: 249.9478\n","Epoch [67/100], Loss: 250.7115\n","Epoch [68/100], Loss: 248.9323\n","Epoch [69/100], Loss: 249.7353\n","Epoch [70/100], Loss: 249.5063\n","Epoch [71/100], Loss: 249.6673\n","Epoch [72/100], Loss: 248.2591\n","Epoch [73/100], Loss: 248.1878\n","Epoch [74/100], Loss: 249.1661\n","Epoch [75/100], Loss: 247.8950\n","Epoch [76/100], Loss: 248.7739\n","Epoch [77/100], Loss: 247.5168\n","Epoch [78/100], Loss: 246.7797\n","Epoch [79/100], Loss: 248.5269\n","Epoch [80/100], Loss: 247.5736\n","Epoch [81/100], Loss: 246.5341\n","Epoch [82/100], Loss: 246.1244\n","Epoch [83/100], Loss: 245.9339\n","Epoch [84/100], Loss: 247.2410\n","Epoch [85/100], Loss: 247.9599\n","Epoch [86/100], Loss: 246.8325\n","Epoch [87/100], Loss: 246.8230\n","Epoch [88/100], Loss: 246.2946\n","Epoch [89/100], Loss: 247.0605\n","Epoch [90/100], Loss: 246.3734\n","Epoch [91/100], Loss: 247.3440\n","Epoch [92/100], Loss: 245.5343\n","Epoch [93/100], Loss: 246.7920\n","Epoch [94/100], Loss: 245.7576\n","Epoch [95/100], Loss: 245.1485\n","Epoch [96/100], Loss: 245.7157\n","Epoch [97/100], Loss: 246.1372\n","Epoch [98/100], Loss: 243.8986\n","Epoch [99/100], Loss: 245.1597\n","Epoch [100/100], Loss: 244.3832\n","Processed mouseintestine successfully.\n","Epoch [1/100], Loss: 892.9813\n","Epoch [2/100], Loss: 412.4819\n","Epoch [3/100], Loss: 374.6312\n","Epoch [4/100], Loss: 350.2343\n","Epoch [5/100], Loss: 332.4951\n","Epoch [6/100], Loss: 319.1644\n","Epoch [7/100], Loss: 310.2348\n","Epoch [8/100], Loss: 306.9866\n","Epoch [9/100], Loss: 300.1066\n","Epoch [10/100], Loss: 293.7973\n","Epoch [11/100], Loss: 291.1389\n","Epoch [12/100], Loss: 289.4014\n","Epoch [13/100], Loss: 286.1815\n","Epoch [14/100], Loss: 282.6467\n","Epoch [15/100], Loss: 284.8534\n","Epoch [16/100], Loss: 280.8584\n","Epoch [17/100], Loss: 280.8863\n","Epoch [18/100], Loss: 280.1205\n","Epoch [19/100], Loss: 277.9667\n","Epoch [20/100], Loss: 277.0456\n","Epoch [21/100], Loss: 277.8610\n","Epoch [22/100], Loss: 276.3802\n","Epoch [23/100], Loss: 276.1219\n","Epoch [24/100], Loss: 274.3402\n","Epoch [25/100], Loss: 274.5123\n","Epoch [26/100], Loss: 274.3524\n","Epoch [27/100], Loss: 274.8472\n","Epoch [28/100], Loss: 271.7088\n","Epoch [29/100], Loss: 270.1560\n","Epoch [30/100], Loss: 267.1799\n","Epoch [31/100], Loss: 263.3978\n","Epoch [32/100], Loss: 262.1421\n","Epoch [33/100], Loss: 260.1772\n","Epoch [34/100], Loss: 259.1754\n","Epoch [35/100], Loss: 257.3314\n","Epoch [36/100], Loss: 254.1739\n","Epoch [37/100], Loss: 252.5951\n","Epoch [38/100], Loss: 252.0343\n","Epoch [39/100], Loss: 251.4059\n","Epoch [40/100], Loss: 249.1753\n","Epoch [41/100], Loss: 249.6971\n","Epoch [42/100], Loss: 248.2653\n","Epoch [43/100], Loss: 247.8125\n","Epoch [44/100], Loss: 246.8580\n","Epoch [45/100], Loss: 247.4493\n","Epoch [46/100], Loss: 246.1085\n","Epoch [47/100], Loss: 245.9676\n","Epoch [48/100], Loss: 244.7434\n","Epoch [49/100], Loss: 244.9871\n","Epoch [50/100], Loss: 244.4245\n","Epoch [51/100], Loss: 244.0930\n","Epoch [52/100], Loss: 243.7842\n","Epoch [53/100], Loss: 243.8534\n","Epoch [54/100], Loss: 243.9194\n","Epoch [55/100], Loss: 244.0760\n","Epoch [56/100], Loss: 243.2024\n","Epoch [57/100], Loss: 242.7720\n","Epoch [58/100], Loss: 243.6785\n","Epoch [59/100], Loss: 242.9615\n","Epoch [60/100], Loss: 243.1471\n","Epoch [61/100], Loss: 242.8801\n","Epoch [62/100], Loss: 242.7604\n","Epoch [63/100], Loss: 242.5060\n","Epoch [64/100], Loss: 241.6692\n","Epoch [65/100], Loss: 242.6202\n","Epoch [66/100], Loss: 241.3614\n","Epoch [67/100], Loss: 240.9708\n","Epoch [68/100], Loss: 241.3445\n","Epoch [69/100], Loss: 241.7561\n","Epoch [70/100], Loss: 241.6090\n","Epoch [71/100], Loss: 241.7980\n","Epoch [72/100], Loss: 241.0804\n","Epoch [73/100], Loss: 239.9844\n","Epoch [74/100], Loss: 240.2173\n","Epoch [75/100], Loss: 241.2824\n","Epoch [76/100], Loss: 240.1955\n","Epoch [77/100], Loss: 240.7314\n","Epoch [78/100], Loss: 241.1044\n","Epoch [79/100], Loss: 239.9212\n","Epoch [80/100], Loss: 240.8672\n","Epoch [81/100], Loss: 239.6860\n","Epoch [82/100], Loss: 240.2339\n","Epoch [83/100], Loss: 241.1330\n","Epoch [84/100], Loss: 239.7813\n","Epoch [85/100], Loss: 240.3417\n","Epoch [86/100], Loss: 240.4284\n","Epoch [87/100], Loss: 240.0490\n","Epoch [88/100], Loss: 239.9115\n","Epoch [89/100], Loss: 239.2687\n","Epoch [90/100], Loss: 239.6338\n","Epoch [91/100], Loss: 240.1150\n","Epoch [92/100], Loss: 239.9203\n","Epoch [93/100], Loss: 240.0435\n","Epoch [94/100], Loss: 238.9426\n","Epoch [95/100], Loss: 239.5995\n","Epoch [96/100], Loss: 239.1338\n","Epoch [97/100], Loss: 238.4442\n","Epoch [98/100], Loss: 238.4642\n","Epoch [99/100], Loss: 239.1117\n","Epoch [100/100], Loss: 240.0424\n","Processed mousecolon successfully.\n","Epoch [1/100], Loss: 1004.6285\n","Epoch [2/100], Loss: 494.0198\n","Epoch [3/100], Loss: 449.8614\n","Epoch [4/100], Loss: 427.5703\n","Epoch [5/100], Loss: 411.4775\n","Epoch [6/100], Loss: 396.9354\n","Epoch [7/100], Loss: 386.4069\n","Epoch [8/100], Loss: 375.9925\n","Epoch [9/100], Loss: 367.3813\n","Epoch [10/100], Loss: 362.9784\n","Epoch [11/100], Loss: 359.9123\n","Epoch [12/100], Loss: 356.1930\n","Epoch [13/100], Loss: 352.5851\n","Epoch [14/100], Loss: 349.0776\n","Epoch [15/100], Loss: 346.9078\n","Epoch [16/100], Loss: 344.9531\n","Epoch [17/100], Loss: 343.3381\n","Epoch [18/100], Loss: 343.6616\n","Epoch [19/100], Loss: 341.8252\n","Epoch [20/100], Loss: 338.6639\n","Epoch [21/100], Loss: 337.4256\n","Epoch [22/100], Loss: 337.2517\n","Epoch [23/100], Loss: 336.7641\n","Epoch [24/100], Loss: 336.4467\n","Epoch [25/100], Loss: 332.4758\n","Epoch [26/100], Loss: 334.2019\n","Epoch [27/100], Loss: 334.9265\n","Epoch [28/100], Loss: 333.6512\n","Epoch [29/100], Loss: 333.3638\n","Epoch [30/100], Loss: 332.2796\n","Epoch [31/100], Loss: 332.3360\n","Epoch [32/100], Loss: 329.6405\n","Epoch [33/100], Loss: 331.9817\n","Epoch [34/100], Loss: 333.1987\n","Epoch [35/100], Loss: 331.2632\n","Epoch [36/100], Loss: 330.7991\n","Epoch [37/100], Loss: 328.1660\n","Epoch [38/100], Loss: 331.7546\n","Epoch [39/100], Loss: 328.5312\n","Epoch [40/100], Loss: 328.4723\n","Epoch [41/100], Loss: 330.3334\n","Epoch [42/100], Loss: 327.0477\n","Epoch [43/100], Loss: 326.3330\n","Epoch [44/100], Loss: 323.6988\n","Epoch [45/100], Loss: 325.4115\n","Epoch [46/100], Loss: 322.9637\n","Epoch [47/100], Loss: 318.7250\n","Epoch [48/100], Loss: 316.6353\n","Epoch [49/100], Loss: 315.7101\n","Epoch [50/100], Loss: 313.8076\n","Epoch [51/100], Loss: 313.5052\n","Epoch [52/100], Loss: 310.5237\n","Epoch [53/100], Loss: 310.3291\n","Epoch [54/100], Loss: 307.5291\n","Epoch [55/100], Loss: 307.1476\n","Epoch [56/100], Loss: 307.2643\n","Epoch [57/100], Loss: 307.0262\n","Epoch [58/100], Loss: 304.7729\n","Epoch [59/100], Loss: 305.6063\n","Epoch [60/100], Loss: 306.3840\n","Epoch [61/100], Loss: 305.1077\n","Epoch [62/100], Loss: 304.6545\n","Epoch [63/100], Loss: 303.9530\n","Epoch [64/100], Loss: 305.1370\n","Epoch [65/100], Loss: 305.1997\n","Epoch [66/100], Loss: 304.0155\n","Epoch [67/100], Loss: 302.6736\n","Epoch [68/100], Loss: 303.0856\n","Epoch [69/100], Loss: 302.4696\n","Epoch [70/100], Loss: 302.2969\n","Epoch [71/100], Loss: 303.1920\n","Epoch [72/100], Loss: 302.9431\n","Epoch [73/100], Loss: 301.9050\n","Epoch [74/100], Loss: 302.5594\n","Epoch [75/100], Loss: 301.7417\n","Epoch [76/100], Loss: 303.0370\n","Epoch [77/100], Loss: 301.5757\n","Epoch [78/100], Loss: 300.4112\n","Epoch [79/100], Loss: 301.2799\n","Epoch [80/100], Loss: 300.7600\n","Epoch [81/100], Loss: 301.3928\n","Epoch [82/100], Loss: 301.3780\n","Epoch [83/100], Loss: 301.7141\n","Epoch [84/100], Loss: 300.3793\n","Epoch [85/100], Loss: 300.5807\n","Epoch [86/100], Loss: 299.8391\n","Epoch [87/100], Loss: 300.9862\n","Epoch [88/100], Loss: 300.6682\n","Epoch [89/100], Loss: 299.7281\n","Epoch [90/100], Loss: 300.1814\n","Epoch [91/100], Loss: 300.8638\n","Epoch [92/100], Loss: 300.2149\n","Epoch [93/100], Loss: 299.2043\n","Epoch [94/100], Loss: 299.0758\n","Epoch [95/100], Loss: 300.0152\n","Epoch [96/100], Loss: 299.5317\n","Epoch [97/100], Loss: 299.9935\n","Epoch [98/100], Loss: 299.6022\n","Epoch [99/100], Loss: 299.3765\n","Epoch [100/100], Loss: 299.7041\n","Processed mousespleen successfully.\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","tissues= ['humanGBM', 'humanskin', 'humanthymus', 'humanspleen', 'humantonsil', 'mousekidney', 'mouseintestine', 'mousecolon', 'mousespleen']\n","\n","for tissue in tissues:\n","    rna_data = None\n","    protein_data = None\n","\n","    for filename in os.listdir(data_dir):\n","        file_path = os.path.join(data_dir, filename)\n","        if tissue in filename and filename.endswith(\"RNA.tsv.gz\"):\n","            rna_data = pd.read_csv(file_path, sep=\"\\t\")\n","        elif tissue in filename and filename.endswith(\"protein.tsv.gz\"):\n","            protein_data = pd.read_csv(file_path, sep=\"\\t\")\n","\n","    rna_data.columns = rna_data.columns.astype(str)\n","    protein_data.columns = protein_data.columns.astype(str)\n","\n","    rna_data = rna_data.sort_values(by='X')\n","    protein_data = protein_data.sort_values(by='X')\n","\n","    rna_data = rna_data.reset_index(drop=True)\n","    protein_data = protein_data.reset_index(drop=True)\n","    rna_data.index = rna_data.index.astype(str)\n","    protein_data.index = protein_data.index.astype(str)\n","\n","    rna_data[['X', 'Y']] = rna_data['X'].str.split('x', expand=True)\n","    rna_data['X'] = pd.to_numeric(rna_data['X'], errors='coerce')\n","    rna_data['Y'] = pd.to_numeric(rna_data['Y'], errors='coerce')\n","    spatial = rna_data[['X', 'Y']].copy()\n","    rna_data.drop(['X', 'Y'], axis=1, inplace=True)\n","    protein_data.drop(['X'], axis=1, inplace=True)\n","\n","    rna_train, rna_test = train_test_split(rna_data, test_size=0.2, random_state=42)\n","    protein_train = protein_data.loc[rna_train.index]\n","    protein_test = protein_data.loc[rna_test.index]\n","\n","    adata_rna_train = sc.AnnData(rna_train)\n","    sc.pp.normalize_total(adata_rna_train, target_sum=1e4)\n","    sc.pp.log1p(adata_rna_train)\n","    sc.pp.highly_variable_genes(adata_rna_train, n_top_genes=4000, flavor='seurat', subset=True)\n","    counts_norm = adata_rna_train.X\n","    rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n","\n","    adata_protein_train = sc.AnnData(protein_train)\n","    sc.pp.normalize_total(adata_protein_train, target_sum=1e4)\n","    sc.pp.log1p(adata_protein_train)\n","    counts_norm = adata_protein_train.X\n","    protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n","\n","    spatial_train = torch.FloatTensor(spatial.loc[rna_train.index].values).to(device)\n","    combined_data = torch.cat([rna_counts_norm, spatial_train], dim=1).to(device)\n","\n","    input_dim = combined_data.shape[1]\n","    output_dim = protein_counts_norm.shape[1]\n","    latent_dim = 64\n","    model = VAE2(input_dim, latent_dim, output_dim).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n","    num_epochs = 100\n","    batch_size = 64\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        permutation = torch.randperm(combined_data.size(0))\n","\n","        for i in range(0, combined_data.size(0), batch_size):\n","            optimizer.zero_grad()\n","            indices = permutation[i:i+batch_size]\n","            batch_data = combined_data[indices]\n","\n","            reconstructed_data, mean, logvar = model(batch_data)\n","            loss = vae_loss(reconstructed_data, protein_counts_norm[indices], mean, logvar, lambda_kl=0.001)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(combined_data):.4f}')\n","\n","    model.eval()\n","    with torch.no_grad():\n","        combined_data = torch.cat([rna_counts_norm, spatial_train], dim=1)\n","        reconstructed_protein_counts, mean, logvar = model(combined_data)\n","\n","        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n","        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n","        avg_corr_pearson = pcc.mean()\n","        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n","\n","        results_df = pd.DataFrame({\n","            'RMSE': [rmse],\n","            'Pearson Correlation': [avg_corr_pearson],\n","            'SSIM':ssim_val\n","        })\n","\n","        results_file_path = os.path.join(results_dir, f\"{tissue}_training_results.csv\")\n","        results_df.to_csv(results_file_path, index=False)\n","\n","        adata_rna_test = sc.AnnData(rna_test)\n","        sc.pp.normalize_total(adata_rna_test, target_sum=1e4)\n","        sc.pp.log1p(adata_rna_test)\n","        counts_norm = adata_rna_test[:,  adata_rna_train.var_names].X\n","        rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n","\n","        adata_protein_test = sc.AnnData(protein_test)\n","        sc.pp.normalize_total(adata_protein_test, target_sum=1e4)\n","        sc.pp.log1p(adata_protein_test)\n","        counts_norm = adata_protein_test.X\n","        protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n","\n","        spatial_test = torch.FloatTensor(spatial.loc[rna_test.index].values).to(device)\n","\n","        combined_data = torch.cat([rna_counts_norm, spatial_test], dim=1)\n","        reconstructed_protein_counts, mean, logvar = model(combined_data)\n","\n","        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n","        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n","        avg_corr_pearson = pcc.mean()\n","        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n","\n","        results_df = pd.DataFrame({\n","            'RMSE': [rmse],\n","            'Pearson Correlation': [avg_corr_pearson],\n","            'SSIM':ssim_val\n","        })\n","\n","    results_file_path = os.path.join(results_dir, f\"{tissue}_results.csv\")\n","    results_df.to_csv(results_file_path, index=False)\n","\n","    print(f\"Processed {tissue} successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSwLOcc5kFue"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python","language":"python","name":"base"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}