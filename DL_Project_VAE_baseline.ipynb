{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 17:36:22.878928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scanpy as sc\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'spatial_datasets/GSE213264_RAW/'\n",
    "results_dir = 'results_baseline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Loss: 3821.4149\n",
      "Epoch [2/100], Loss: 2053.6703\n",
      "Epoch [3/100], Loss: 1360.8336\n",
      "Epoch [4/100], Loss: 1118.0095\n",
      "Epoch [5/100], Loss: 1018.4283\n",
      "Epoch [6/100], Loss: 967.1545\n",
      "Epoch [7/100], Loss: 936.8036\n",
      "Epoch [8/100], Loss: 915.0792\n",
      "Epoch [9/100], Loss: 903.2457\n",
      "Epoch [10/100], Loss: 896.2054\n",
      "Epoch [11/100], Loss: 888.2281\n",
      "Epoch [12/100], Loss: 884.2786\n",
      "Epoch [13/100], Loss: 881.9641\n",
      "Epoch [14/100], Loss: 876.8187\n",
      "Epoch [15/100], Loss: 875.1865\n",
      "Epoch [16/100], Loss: 874.3331\n",
      "Epoch [17/100], Loss: 869.5631\n",
      "Epoch [18/100], Loss: 870.4597\n",
      "Epoch [19/100], Loss: 869.9280\n",
      "Epoch [20/100], Loss: 869.0803\n",
      "Epoch [21/100], Loss: 866.9875\n",
      "Epoch [22/100], Loss: 865.7219\n",
      "Epoch [23/100], Loss: 865.9897\n",
      "Epoch [24/100], Loss: 862.6444\n",
      "Epoch [25/100], Loss: 863.4568\n",
      "Epoch [26/100], Loss: 861.6515\n",
      "Epoch [27/100], Loss: 859.9894\n",
      "Epoch [28/100], Loss: 860.4863\n",
      "Epoch [29/100], Loss: 861.4123\n",
      "Epoch [30/100], Loss: 859.5438\n",
      "Epoch [31/100], Loss: 859.0844\n",
      "Epoch [32/100], Loss: 857.8201\n",
      "Epoch [33/100], Loss: 856.9850\n",
      "Epoch [34/100], Loss: 856.6153\n",
      "Epoch [35/100], Loss: 856.9625\n",
      "Epoch [36/100], Loss: 855.0223\n",
      "Epoch [37/100], Loss: 855.8594\n",
      "Epoch [38/100], Loss: 856.0660\n",
      "Epoch [39/100], Loss: 855.3163\n",
      "Epoch [40/100], Loss: 855.0146\n",
      "Epoch [41/100], Loss: 854.1706\n",
      "Epoch [42/100], Loss: 853.6130\n",
      "Epoch [43/100], Loss: 852.2592\n",
      "Epoch [44/100], Loss: 854.6264\n",
      "Epoch [45/100], Loss: 852.6873\n",
      "Epoch [46/100], Loss: 851.6935\n",
      "Epoch [47/100], Loss: 852.8428\n",
      "Epoch [48/100], Loss: 851.9388\n",
      "Epoch [49/100], Loss: 852.0839\n",
      "Epoch [50/100], Loss: 850.7301\n",
      "Epoch [51/100], Loss: 852.8606\n",
      "Epoch [52/100], Loss: 849.9963\n",
      "Epoch [53/100], Loss: 849.6469\n",
      "Epoch [54/100], Loss: 850.1715\n",
      "Epoch [55/100], Loss: 848.9113\n",
      "Epoch [56/100], Loss: 849.7055\n",
      "Epoch [57/100], Loss: 850.9624\n",
      "Epoch [58/100], Loss: 849.5631\n",
      "Epoch [59/100], Loss: 849.4312\n",
      "Epoch [60/100], Loss: 849.9035\n",
      "Epoch [61/100], Loss: 847.3259\n",
      "Epoch [62/100], Loss: 848.2476\n",
      "Epoch [63/100], Loss: 848.8795\n",
      "Epoch [64/100], Loss: 847.8325\n",
      "Epoch [65/100], Loss: 847.8136\n",
      "Epoch [66/100], Loss: 849.2769\n",
      "Epoch [67/100], Loss: 847.8748\n",
      "Epoch [68/100], Loss: 847.0218\n",
      "Epoch [69/100], Loss: 846.4882\n",
      "Epoch [70/100], Loss: 842.5179\n",
      "Epoch [71/100], Loss: 842.2066\n",
      "Epoch [72/100], Loss: 841.4117\n",
      "Epoch [73/100], Loss: 839.4135\n",
      "Epoch [74/100], Loss: 836.6158\n",
      "Epoch [75/100], Loss: 836.3338\n",
      "Epoch [76/100], Loss: 837.7893\n",
      "Epoch [77/100], Loss: 836.4720\n",
      "Epoch [78/100], Loss: 834.4446\n",
      "Epoch [79/100], Loss: 833.0568\n",
      "Epoch [80/100], Loss: 831.7432\n",
      "Epoch [81/100], Loss: 831.9722\n",
      "Epoch [82/100], Loss: 830.7121\n",
      "Epoch [83/100], Loss: 830.8900\n",
      "Epoch [84/100], Loss: 829.6553\n",
      "Epoch [85/100], Loss: 831.1938\n",
      "Epoch [86/100], Loss: 831.8738\n",
      "Epoch [87/100], Loss: 828.9879\n",
      "Epoch [88/100], Loss: 829.5603\n",
      "Epoch [89/100], Loss: 828.8850\n",
      "Epoch [90/100], Loss: 827.6612\n",
      "Epoch [91/100], Loss: 828.5226\n",
      "Epoch [92/100], Loss: 827.6563\n",
      "Epoch [93/100], Loss: 827.3670\n",
      "Epoch [94/100], Loss: 828.3389\n",
      "Epoch [95/100], Loss: 827.0096\n",
      "Epoch [96/100], Loss: 826.3652\n",
      "Epoch [97/100], Loss: 826.6832\n",
      "Epoch [98/100], Loss: 826.1324\n",
      "Epoch [99/100], Loss: 825.5396\n",
      "Epoch [100/100], Loss: 824.9787\n",
      "Processed humanGBM successfully.\n",
      "Epoch [1/100], Loss: 3849.2611\n",
      "Epoch [2/100], Loss: 2138.6535\n",
      "Epoch [3/100], Loss: 1468.6726\n",
      "Epoch [4/100], Loss: 1223.0221\n",
      "Epoch [5/100], Loss: 1123.8155\n",
      "Epoch [6/100], Loss: 1067.7322\n",
      "Epoch [7/100], Loss: 1037.5204\n",
      "Epoch [8/100], Loss: 1020.7517\n",
      "Epoch [9/100], Loss: 1004.5728\n",
      "Epoch [10/100], Loss: 994.1162\n",
      "Epoch [11/100], Loss: 988.9582\n",
      "Epoch [12/100], Loss: 985.2775\n",
      "Epoch [13/100], Loss: 982.8817\n",
      "Epoch [14/100], Loss: 978.1669\n",
      "Epoch [15/100], Loss: 976.9846\n",
      "Epoch [16/100], Loss: 973.2419\n",
      "Epoch [17/100], Loss: 972.4090\n",
      "Epoch [18/100], Loss: 971.3677\n",
      "Epoch [19/100], Loss: 969.7849\n",
      "Epoch [20/100], Loss: 969.8473\n",
      "Epoch [21/100], Loss: 968.4990\n",
      "Epoch [22/100], Loss: 965.7180\n",
      "Epoch [23/100], Loss: 967.2123\n",
      "Epoch [24/100], Loss: 965.8951\n",
      "Epoch [25/100], Loss: 964.0052\n",
      "Epoch [26/100], Loss: 965.0883\n",
      "Epoch [27/100], Loss: 961.8649\n",
      "Epoch [28/100], Loss: 964.3668\n",
      "Epoch [29/100], Loss: 961.9936\n",
      "Epoch [30/100], Loss: 960.6852\n",
      "Epoch [31/100], Loss: 960.5924\n",
      "Epoch [32/100], Loss: 962.4367\n",
      "Epoch [33/100], Loss: 959.8640\n",
      "Epoch [34/100], Loss: 959.2102\n",
      "Epoch [35/100], Loss: 959.2971\n",
      "Epoch [36/100], Loss: 960.7243\n",
      "Epoch [37/100], Loss: 956.4027\n",
      "Epoch [38/100], Loss: 954.6125\n",
      "Epoch [39/100], Loss: 942.8108\n",
      "Epoch [40/100], Loss: 927.5786\n",
      "Epoch [41/100], Loss: 921.4085\n",
      "Epoch [42/100], Loss: 917.6433\n",
      "Epoch [43/100], Loss: 914.0689\n",
      "Epoch [44/100], Loss: 911.0425\n",
      "Epoch [45/100], Loss: 910.4740\n",
      "Epoch [46/100], Loss: 909.1772\n",
      "Epoch [47/100], Loss: 908.3489\n",
      "Epoch [48/100], Loss: 906.6660\n",
      "Epoch [49/100], Loss: 905.3319\n",
      "Epoch [50/100], Loss: 903.3840\n",
      "Epoch [51/100], Loss: 903.3143\n",
      "Epoch [52/100], Loss: 901.7232\n",
      "Epoch [53/100], Loss: 901.9420\n",
      "Epoch [54/100], Loss: 901.7662\n",
      "Epoch [55/100], Loss: 899.9287\n",
      "Epoch [56/100], Loss: 895.5451\n",
      "Epoch [57/100], Loss: 895.7886\n",
      "Epoch [58/100], Loss: 894.1398\n",
      "Epoch [59/100], Loss: 891.7948\n",
      "Epoch [60/100], Loss: 890.3724\n",
      "Epoch [61/100], Loss: 888.0304\n",
      "Epoch [62/100], Loss: 888.4067\n",
      "Epoch [63/100], Loss: 885.4746\n",
      "Epoch [64/100], Loss: 884.9765\n",
      "Epoch [65/100], Loss: 884.1167\n",
      "Epoch [66/100], Loss: 882.9791\n",
      "Epoch [67/100], Loss: 882.7521\n",
      "Epoch [68/100], Loss: 881.7390\n",
      "Epoch [69/100], Loss: 881.9540\n",
      "Epoch [70/100], Loss: 881.3952\n",
      "Epoch [71/100], Loss: 879.3031\n",
      "Epoch [72/100], Loss: 877.5267\n",
      "Epoch [73/100], Loss: 879.2275\n",
      "Epoch [74/100], Loss: 878.6377\n",
      "Epoch [75/100], Loss: 876.4975\n",
      "Epoch [76/100], Loss: 877.8767\n",
      "Epoch [77/100], Loss: 877.3297\n",
      "Epoch [78/100], Loss: 876.4424\n",
      "Epoch [79/100], Loss: 877.1035\n",
      "Epoch [80/100], Loss: 874.8426\n",
      "Epoch [81/100], Loss: 874.9480\n",
      "Epoch [82/100], Loss: 874.3733\n",
      "Epoch [83/100], Loss: 874.2792\n",
      "Epoch [84/100], Loss: 873.0950\n",
      "Epoch [85/100], Loss: 874.1858\n",
      "Epoch [86/100], Loss: 873.7372\n",
      "Epoch [87/100], Loss: 873.4123\n",
      "Epoch [88/100], Loss: 873.1517\n",
      "Epoch [89/100], Loss: 872.3975\n",
      "Epoch [90/100], Loss: 872.0802\n",
      "Epoch [91/100], Loss: 871.7490\n",
      "Epoch [92/100], Loss: 872.0846\n",
      "Epoch [93/100], Loss: 870.0049\n",
      "Epoch [94/100], Loss: 870.7856\n",
      "Epoch [95/100], Loss: 871.9474\n",
      "Epoch [96/100], Loss: 871.2255\n",
      "Epoch [97/100], Loss: 869.6351\n",
      "Epoch [98/100], Loss: 870.3156\n",
      "Epoch [99/100], Loss: 869.1212\n",
      "Epoch [100/100], Loss: 869.6142\n",
      "Processed humanskin successfully.\n",
      "Epoch [1/100], Loss: 3240.5460\n",
      "Epoch [2/100], Loss: 1327.7828\n",
      "Epoch [3/100], Loss: 899.6575\n",
      "Epoch [4/100], Loss: 777.8847\n",
      "Epoch [5/100], Loss: 725.9291\n",
      "Epoch [6/100], Loss: 701.7823\n",
      "Epoch [7/100], Loss: 685.2027\n",
      "Epoch [8/100], Loss: 675.0880\n",
      "Epoch [9/100], Loss: 669.4153\n",
      "Epoch [10/100], Loss: 665.3108\n",
      "Epoch [11/100], Loss: 659.5807\n",
      "Epoch [12/100], Loss: 661.4625\n",
      "Epoch [13/100], Loss: 657.7923\n",
      "Epoch [14/100], Loss: 656.6894\n",
      "Epoch [15/100], Loss: 654.5220\n",
      "Epoch [16/100], Loss: 651.6776\n",
      "Epoch [17/100], Loss: 650.5073\n",
      "Epoch [18/100], Loss: 650.3809\n",
      "Epoch [19/100], Loss: 649.6922\n",
      "Epoch [20/100], Loss: 648.0589\n",
      "Epoch [21/100], Loss: 647.1201\n",
      "Epoch [22/100], Loss: 648.2656\n",
      "Epoch [23/100], Loss: 648.4371\n",
      "Epoch [24/100], Loss: 646.1264\n",
      "Epoch [25/100], Loss: 644.5095\n",
      "Epoch [26/100], Loss: 643.6971\n",
      "Epoch [27/100], Loss: 644.0557\n",
      "Epoch [28/100], Loss: 644.1869\n",
      "Epoch [29/100], Loss: 643.3658\n",
      "Epoch [30/100], Loss: 643.3478\n",
      "Epoch [31/100], Loss: 641.7336\n",
      "Epoch [32/100], Loss: 641.9751\n",
      "Epoch [33/100], Loss: 640.7520\n",
      "Epoch [34/100], Loss: 642.0308\n",
      "Epoch [35/100], Loss: 640.9310\n",
      "Epoch [36/100], Loss: 640.0871\n",
      "Epoch [37/100], Loss: 639.2980\n",
      "Epoch [38/100], Loss: 639.4552\n",
      "Epoch [39/100], Loss: 639.1206\n",
      "Epoch [40/100], Loss: 638.2016\n",
      "Epoch [41/100], Loss: 637.7682\n",
      "Epoch [42/100], Loss: 637.1313\n",
      "Epoch [43/100], Loss: 638.1404\n",
      "Epoch [44/100], Loss: 638.4954\n",
      "Epoch [45/100], Loss: 636.7411\n",
      "Epoch [46/100], Loss: 637.6663\n",
      "Epoch [47/100], Loss: 637.1902\n",
      "Epoch [48/100], Loss: 635.1423\n",
      "Epoch [49/100], Loss: 636.9934\n",
      "Epoch [50/100], Loss: 636.1099\n",
      "Epoch [51/100], Loss: 634.3076\n",
      "Epoch [52/100], Loss: 634.5961\n",
      "Epoch [53/100], Loss: 633.5949\n",
      "Epoch [54/100], Loss: 635.3385\n",
      "Epoch [55/100], Loss: 634.0118\n",
      "Epoch [56/100], Loss: 633.5641\n",
      "Epoch [57/100], Loss: 633.9668\n",
      "Epoch [58/100], Loss: 633.9144\n",
      "Epoch [59/100], Loss: 631.8465\n",
      "Epoch [60/100], Loss: 633.7196\n",
      "Epoch [61/100], Loss: 632.1570\n",
      "Epoch [62/100], Loss: 632.5199\n",
      "Epoch [63/100], Loss: 632.5680\n",
      "Epoch [64/100], Loss: 632.3485\n",
      "Epoch [65/100], Loss: 632.1379\n",
      "Epoch [66/100], Loss: 633.1751\n",
      "Epoch [67/100], Loss: 631.8929\n",
      "Epoch [68/100], Loss: 631.9534\n",
      "Epoch [69/100], Loss: 632.3343\n",
      "Epoch [70/100], Loss: 631.9027\n",
      "Epoch [71/100], Loss: 631.0371\n",
      "Epoch [72/100], Loss: 631.4793\n",
      "Epoch [73/100], Loss: 631.4697\n",
      "Epoch [74/100], Loss: 631.8562\n",
      "Epoch [75/100], Loss: 630.5622\n",
      "Epoch [76/100], Loss: 630.8432\n",
      "Epoch [77/100], Loss: 630.1153\n",
      "Epoch [78/100], Loss: 629.4707\n",
      "Epoch [79/100], Loss: 629.9349\n",
      "Epoch [80/100], Loss: 630.2135\n",
      "Epoch [81/100], Loss: 628.6955\n",
      "Epoch [82/100], Loss: 628.9591\n",
      "Epoch [83/100], Loss: 628.2794\n",
      "Epoch [84/100], Loss: 629.7541\n",
      "Epoch [85/100], Loss: 629.1901\n",
      "Epoch [86/100], Loss: 629.8492\n",
      "Epoch [87/100], Loss: 629.8455\n",
      "Epoch [88/100], Loss: 628.3053\n",
      "Epoch [89/100], Loss: 628.4545\n",
      "Epoch [90/100], Loss: 628.0257\n",
      "Epoch [91/100], Loss: 627.8504\n",
      "Epoch [92/100], Loss: 626.8345\n",
      "Epoch [93/100], Loss: 627.9408\n",
      "Epoch [94/100], Loss: 627.6411\n",
      "Epoch [95/100], Loss: 627.2611\n",
      "Epoch [96/100], Loss: 627.4772\n",
      "Epoch [97/100], Loss: 626.7474\n",
      "Epoch [98/100], Loss: 626.9815\n",
      "Epoch [99/100], Loss: 627.3012\n",
      "Epoch [100/100], Loss: 626.8674\n",
      "Processed humanthymus successfully.\n",
      "Epoch [1/100], Loss: 3181.8793\n",
      "Epoch [2/100], Loss: 1340.9579\n",
      "Epoch [3/100], Loss: 917.9415\n",
      "Epoch [4/100], Loss: 804.1935\n",
      "Epoch [5/100], Loss: 754.5059\n",
      "Epoch [6/100], Loss: 729.5890\n",
      "Epoch [7/100], Loss: 715.7916\n",
      "Epoch [8/100], Loss: 706.9133\n",
      "Epoch [9/100], Loss: 701.9113\n",
      "Epoch [10/100], Loss: 700.2996\n",
      "Epoch [11/100], Loss: 695.7874\n",
      "Epoch [12/100], Loss: 695.7442\n",
      "Epoch [13/100], Loss: 691.7870\n",
      "Epoch [14/100], Loss: 694.2567\n",
      "Epoch [15/100], Loss: 690.8101\n",
      "Epoch [16/100], Loss: 688.2338\n",
      "Epoch [17/100], Loss: 687.2625\n",
      "Epoch [18/100], Loss: 687.1572\n",
      "Epoch [19/100], Loss: 686.3955\n",
      "Epoch [20/100], Loss: 685.1869\n",
      "Epoch [21/100], Loss: 685.2534\n",
      "Epoch [22/100], Loss: 683.6891\n",
      "Epoch [23/100], Loss: 682.5557\n",
      "Epoch [24/100], Loss: 683.7026\n",
      "Epoch [25/100], Loss: 681.9780\n",
      "Epoch [26/100], Loss: 681.6112\n",
      "Epoch [27/100], Loss: 681.6391\n",
      "Epoch [28/100], Loss: 682.9350\n",
      "Epoch [29/100], Loss: 679.8615\n",
      "Epoch [30/100], Loss: 680.6712\n",
      "Epoch [31/100], Loss: 678.7932\n",
      "Epoch [32/100], Loss: 680.2401\n",
      "Epoch [33/100], Loss: 678.9979\n",
      "Epoch [34/100], Loss: 677.5338\n",
      "Epoch [35/100], Loss: 678.0287\n",
      "Epoch [36/100], Loss: 678.4386\n",
      "Epoch [37/100], Loss: 677.0261\n",
      "Epoch [38/100], Loss: 676.2401\n",
      "Epoch [39/100], Loss: 676.9422\n",
      "Epoch [40/100], Loss: 676.2814\n",
      "Epoch [41/100], Loss: 676.8366\n",
      "Epoch [42/100], Loss: 675.4855\n",
      "Epoch [43/100], Loss: 675.9062\n",
      "Epoch [44/100], Loss: 675.8218\n",
      "Epoch [45/100], Loss: 675.7729\n",
      "Epoch [46/100], Loss: 675.7717\n",
      "Epoch [47/100], Loss: 675.4213\n",
      "Epoch [48/100], Loss: 675.5508\n",
      "Epoch [49/100], Loss: 674.6778\n",
      "Epoch [50/100], Loss: 674.8096\n",
      "Epoch [51/100], Loss: 674.9878\n",
      "Epoch [52/100], Loss: 673.6333\n",
      "Epoch [53/100], Loss: 672.4272\n",
      "Epoch [54/100], Loss: 672.9834\n",
      "Epoch [55/100], Loss: 673.2594\n",
      "Epoch [56/100], Loss: 672.3601\n",
      "Epoch [57/100], Loss: 671.5660\n",
      "Epoch [58/100], Loss: 668.4325\n",
      "Epoch [59/100], Loss: 666.8971\n",
      "Epoch [60/100], Loss: 664.5452\n",
      "Epoch [61/100], Loss: 664.3659\n",
      "Epoch [62/100], Loss: 662.6588\n",
      "Epoch [63/100], Loss: 661.2063\n",
      "Epoch [64/100], Loss: 660.2675\n",
      "Epoch [65/100], Loss: 659.5467\n",
      "Epoch [66/100], Loss: 659.6991\n",
      "Epoch [67/100], Loss: 659.6718\n",
      "Epoch [68/100], Loss: 658.6162\n",
      "Epoch [69/100], Loss: 657.7094\n",
      "Epoch [70/100], Loss: 658.4280\n",
      "Epoch [71/100], Loss: 658.1703\n",
      "Epoch [72/100], Loss: 657.0832\n",
      "Epoch [73/100], Loss: 657.7305\n",
      "Epoch [74/100], Loss: 656.9851\n",
      "Epoch [75/100], Loss: 655.5775\n",
      "Epoch [76/100], Loss: 656.1264\n",
      "Epoch [77/100], Loss: 655.1295\n",
      "Epoch [78/100], Loss: 655.7473\n",
      "Epoch [79/100], Loss: 655.1526\n",
      "Epoch [80/100], Loss: 655.6478\n",
      "Epoch [81/100], Loss: 655.3700\n",
      "Epoch [82/100], Loss: 654.4479\n",
      "Epoch [83/100], Loss: 654.7338\n",
      "Epoch [84/100], Loss: 654.6016\n",
      "Epoch [85/100], Loss: 654.5936\n",
      "Epoch [86/100], Loss: 653.7358\n",
      "Epoch [87/100], Loss: 654.6427\n",
      "Epoch [88/100], Loss: 652.9748\n",
      "Epoch [89/100], Loss: 653.5837\n",
      "Epoch [90/100], Loss: 653.2537\n",
      "Epoch [91/100], Loss: 652.5199\n",
      "Epoch [92/100], Loss: 652.7333\n",
      "Epoch [93/100], Loss: 652.7528\n",
      "Epoch [94/100], Loss: 652.7407\n",
      "Epoch [95/100], Loss: 652.1437\n",
      "Epoch [96/100], Loss: 652.6935\n",
      "Epoch [97/100], Loss: 651.8174\n",
      "Epoch [98/100], Loss: 651.3622\n",
      "Epoch [99/100], Loss: 651.8827\n",
      "Epoch [100/100], Loss: 652.0606\n",
      "Processed humanspleen successfully.\n",
      "Epoch [1/100], Loss: 3108.9939\n",
      "Epoch [2/100], Loss: 1211.6575\n",
      "Epoch [3/100], Loss: 788.0864\n",
      "Epoch [4/100], Loss: 664.7855\n",
      "Epoch [5/100], Loss: 615.1037\n",
      "Epoch [6/100], Loss: 589.1379\n",
      "Epoch [7/100], Loss: 574.1158\n",
      "Epoch [8/100], Loss: 564.6950\n",
      "Epoch [9/100], Loss: 559.2268\n",
      "Epoch [10/100], Loss: 555.1349\n",
      "Epoch [11/100], Loss: 551.1165\n",
      "Epoch [12/100], Loss: 550.6843\n",
      "Epoch [13/100], Loss: 547.6522\n",
      "Epoch [14/100], Loss: 544.7476\n",
      "Epoch [15/100], Loss: 545.5752\n",
      "Epoch [16/100], Loss: 543.6456\n",
      "Epoch [17/100], Loss: 543.1111\n",
      "Epoch [18/100], Loss: 541.8978\n",
      "Epoch [19/100], Loss: 541.8359\n",
      "Epoch [20/100], Loss: 540.5320\n",
      "Epoch [21/100], Loss: 540.2786\n",
      "Epoch [22/100], Loss: 539.1809\n",
      "Epoch [23/100], Loss: 538.2091\n",
      "Epoch [24/100], Loss: 539.1041\n",
      "Epoch [25/100], Loss: 537.9459\n",
      "Epoch [26/100], Loss: 536.9108\n",
      "Epoch [27/100], Loss: 534.9521\n",
      "Epoch [28/100], Loss: 536.2202\n",
      "Epoch [29/100], Loss: 535.4877\n",
      "Epoch [30/100], Loss: 536.8461\n",
      "Epoch [31/100], Loss: 533.8500\n",
      "Epoch [32/100], Loss: 533.2136\n",
      "Epoch [33/100], Loss: 532.3857\n",
      "Epoch [34/100], Loss: 534.1137\n",
      "Epoch [35/100], Loss: 531.8215\n",
      "Epoch [36/100], Loss: 532.6118\n",
      "Epoch [37/100], Loss: 531.4152\n",
      "Epoch [38/100], Loss: 532.3864\n",
      "Epoch [39/100], Loss: 531.7083\n",
      "Epoch [40/100], Loss: 531.9321\n",
      "Epoch [41/100], Loss: 530.8605\n",
      "Epoch [42/100], Loss: 530.3039\n",
      "Epoch [43/100], Loss: 531.0300\n",
      "Epoch [44/100], Loss: 530.6622\n",
      "Epoch [45/100], Loss: 530.0856\n",
      "Epoch [46/100], Loss: 528.5848\n",
      "Epoch [47/100], Loss: 529.0675\n",
      "Epoch [48/100], Loss: 527.9120\n",
      "Epoch [49/100], Loss: 529.0846\n",
      "Epoch [50/100], Loss: 527.7161\n",
      "Epoch [51/100], Loss: 528.6841\n",
      "Epoch [52/100], Loss: 527.8037\n",
      "Epoch [53/100], Loss: 529.0261\n",
      "Epoch [54/100], Loss: 528.1866\n",
      "Epoch [55/100], Loss: 526.7146\n",
      "Epoch [56/100], Loss: 527.7690\n",
      "Epoch [57/100], Loss: 526.5577\n",
      "Epoch [58/100], Loss: 527.2505\n",
      "Epoch [59/100], Loss: 527.0189\n",
      "Epoch [60/100], Loss: 525.7235\n",
      "Epoch [61/100], Loss: 526.5517\n",
      "Epoch [62/100], Loss: 526.2952\n",
      "Epoch [63/100], Loss: 525.9901\n",
      "Epoch [64/100], Loss: 525.7703\n",
      "Epoch [65/100], Loss: 525.8488\n",
      "Epoch [66/100], Loss: 526.0218\n",
      "Epoch [67/100], Loss: 526.0536\n",
      "Epoch [68/100], Loss: 524.7604\n",
      "Epoch [69/100], Loss: 525.3273\n",
      "Epoch [70/100], Loss: 524.4831\n",
      "Epoch [71/100], Loss: 523.6139\n",
      "Epoch [72/100], Loss: 525.1857\n",
      "Epoch [73/100], Loss: 523.5788\n",
      "Epoch [74/100], Loss: 524.2022\n",
      "Epoch [75/100], Loss: 524.4442\n",
      "Epoch [76/100], Loss: 523.1062\n",
      "Epoch [77/100], Loss: 524.6044\n",
      "Epoch [78/100], Loss: 523.0466\n",
      "Epoch [79/100], Loss: 523.9306\n",
      "Epoch [80/100], Loss: 522.6314\n",
      "Epoch [81/100], Loss: 522.8206\n",
      "Epoch [82/100], Loss: 522.2378\n",
      "Epoch [83/100], Loss: 522.9852\n",
      "Epoch [84/100], Loss: 522.0874\n",
      "Epoch [85/100], Loss: 522.9030\n",
      "Epoch [86/100], Loss: 522.8564\n",
      "Epoch [87/100], Loss: 522.4993\n",
      "Epoch [88/100], Loss: 522.4616\n",
      "Epoch [89/100], Loss: 522.2143\n",
      "Epoch [90/100], Loss: 522.0700\n",
      "Epoch [91/100], Loss: 523.1346\n",
      "Epoch [92/100], Loss: 521.8320\n",
      "Epoch [93/100], Loss: 521.1643\n",
      "Epoch [94/100], Loss: 521.4183\n",
      "Epoch [95/100], Loss: 521.8756\n",
      "Epoch [96/100], Loss: 520.8580\n",
      "Epoch [97/100], Loss: 520.7603\n",
      "Epoch [98/100], Loss: 520.3559\n",
      "Epoch [99/100], Loss: 521.4482\n",
      "Epoch [100/100], Loss: 520.7207\n",
      "Processed humantonsil successfully.\n",
      "Epoch [1/100], Loss: 3162.9951\n",
      "Epoch [2/100], Loss: 1341.2924\n",
      "Epoch [3/100], Loss: 895.2708\n",
      "Epoch [4/100], Loss: 771.8450\n",
      "Epoch [5/100], Loss: 721.4255\n",
      "Epoch [6/100], Loss: 695.8209\n",
      "Epoch [7/100], Loss: 679.9053\n",
      "Epoch [8/100], Loss: 671.1575\n",
      "Epoch [9/100], Loss: 666.1429\n",
      "Epoch [10/100], Loss: 662.1864\n",
      "Epoch [11/100], Loss: 660.0209\n",
      "Epoch [12/100], Loss: 658.4259\n",
      "Epoch [13/100], Loss: 655.9684\n",
      "Epoch [14/100], Loss: 654.1490\n",
      "Epoch [15/100], Loss: 652.7971\n",
      "Epoch [16/100], Loss: 651.5923\n",
      "Epoch [17/100], Loss: 651.0641\n",
      "Epoch [18/100], Loss: 649.6798\n",
      "Epoch [19/100], Loss: 648.6766\n",
      "Epoch [20/100], Loss: 647.4175\n",
      "Epoch [21/100], Loss: 648.1797\n",
      "Epoch [22/100], Loss: 647.4113\n",
      "Epoch [23/100], Loss: 645.7248\n",
      "Epoch [24/100], Loss: 645.5904\n",
      "Epoch [25/100], Loss: 645.0550\n",
      "Epoch [26/100], Loss: 644.1388\n",
      "Epoch [27/100], Loss: 643.5904\n",
      "Epoch [28/100], Loss: 642.6713\n",
      "Epoch [29/100], Loss: 642.9250\n",
      "Epoch [30/100], Loss: 642.5520\n",
      "Epoch [31/100], Loss: 642.1775\n",
      "Epoch [32/100], Loss: 642.0543\n",
      "Epoch [33/100], Loss: 641.4078\n",
      "Epoch [34/100], Loss: 640.3943\n",
      "Epoch [35/100], Loss: 640.5601\n",
      "Epoch [36/100], Loss: 639.0472\n",
      "Epoch [37/100], Loss: 639.1218\n",
      "Epoch [38/100], Loss: 639.0304\n",
      "Epoch [39/100], Loss: 639.0454\n",
      "Epoch [40/100], Loss: 638.2329\n",
      "Epoch [41/100], Loss: 637.6009\n",
      "Epoch [42/100], Loss: 638.3289\n",
      "Epoch [43/100], Loss: 637.5949\n",
      "Epoch [44/100], Loss: 637.2644\n",
      "Epoch [45/100], Loss: 636.5935\n",
      "Epoch [46/100], Loss: 637.2253\n",
      "Epoch [47/100], Loss: 636.1065\n",
      "Epoch [48/100], Loss: 635.4027\n",
      "Epoch [49/100], Loss: 636.4807\n",
      "Epoch [50/100], Loss: 635.6216\n",
      "Epoch [51/100], Loss: 636.0490\n",
      "Epoch [52/100], Loss: 635.5402\n",
      "Epoch [53/100], Loss: 635.3568\n",
      "Epoch [54/100], Loss: 636.0319\n",
      "Epoch [55/100], Loss: 634.1476\n",
      "Epoch [56/100], Loss: 634.2042\n",
      "Epoch [57/100], Loss: 633.9331\n",
      "Epoch [58/100], Loss: 634.0307\n",
      "Epoch [59/100], Loss: 634.1649\n",
      "Epoch [60/100], Loss: 634.8359\n",
      "Epoch [61/100], Loss: 634.0118\n",
      "Epoch [62/100], Loss: 634.0816\n",
      "Epoch [63/100], Loss: 633.3984\n",
      "Epoch [64/100], Loss: 633.2882\n",
      "Epoch [65/100], Loss: 633.8352\n",
      "Epoch [66/100], Loss: 633.3648\n",
      "Epoch [67/100], Loss: 633.4203\n",
      "Epoch [68/100], Loss: 632.6467\n",
      "Epoch [69/100], Loss: 633.2090\n",
      "Epoch [70/100], Loss: 632.6352\n",
      "Epoch [71/100], Loss: 633.0015\n",
      "Epoch [72/100], Loss: 633.1172\n",
      "Epoch [73/100], Loss: 632.0964\n",
      "Epoch [74/100], Loss: 632.0143\n",
      "Epoch [75/100], Loss: 631.5057\n",
      "Epoch [76/100], Loss: 631.7424\n",
      "Epoch [77/100], Loss: 631.5583\n",
      "Epoch [78/100], Loss: 631.5760\n",
      "Epoch [79/100], Loss: 630.8173\n",
      "Epoch [80/100], Loss: 630.7494\n",
      "Epoch [81/100], Loss: 631.4419\n",
      "Epoch [82/100], Loss: 631.4652\n",
      "Epoch [83/100], Loss: 631.7481\n",
      "Epoch [84/100], Loss: 630.8595\n",
      "Epoch [85/100], Loss: 630.4846\n",
      "Epoch [86/100], Loss: 630.7749\n",
      "Epoch [87/100], Loss: 629.8649\n",
      "Epoch [88/100], Loss: 629.2542\n",
      "Epoch [89/100], Loss: 629.3892\n",
      "Epoch [90/100], Loss: 627.7498\n",
      "Epoch [91/100], Loss: 626.9284\n",
      "Epoch [92/100], Loss: 627.0029\n",
      "Epoch [93/100], Loss: 625.7670\n",
      "Epoch [94/100], Loss: 625.5618\n",
      "Epoch [95/100], Loss: 624.8782\n",
      "Epoch [96/100], Loss: 625.3303\n",
      "Epoch [97/100], Loss: 623.8162\n",
      "Epoch [98/100], Loss: 623.2198\n",
      "Epoch [99/100], Loss: 623.3279\n",
      "Epoch [100/100], Loss: 623.6104\n",
      "Processed mousekidney successfully.\n",
      "Epoch [1/100], Loss: 4226.6874\n",
      "Epoch [2/100], Loss: 2772.8835\n",
      "Epoch [3/100], Loss: 1978.0327\n",
      "Epoch [4/100], Loss: 1491.2499\n",
      "Epoch [5/100], Loss: 1214.8678\n",
      "Epoch [6/100], Loss: 1062.0990\n",
      "Epoch [7/100], Loss: 968.6975\n",
      "Epoch [8/100], Loss: 908.7442\n",
      "Epoch [9/100], Loss: 867.9658\n",
      "Epoch [10/100], Loss: 840.8991\n",
      "Epoch [11/100], Loss: 818.3461\n",
      "Epoch [12/100], Loss: 804.3074\n",
      "Epoch [13/100], Loss: 791.5350\n",
      "Epoch [14/100], Loss: 781.1618\n",
      "Epoch [15/100], Loss: 776.0299\n",
      "Epoch [16/100], Loss: 770.2478\n",
      "Epoch [17/100], Loss: 766.1811\n",
      "Epoch [18/100], Loss: 759.2649\n",
      "Epoch [19/100], Loss: 756.7869\n",
      "Epoch [20/100], Loss: 757.2812\n",
      "Epoch [21/100], Loss: 751.7742\n",
      "Epoch [22/100], Loss: 744.9029\n",
      "Epoch [23/100], Loss: 743.9683\n",
      "Epoch [24/100], Loss: 747.6117\n",
      "Epoch [25/100], Loss: 744.7403\n",
      "Epoch [26/100], Loss: 743.1437\n",
      "Epoch [27/100], Loss: 742.5310\n",
      "Epoch [28/100], Loss: 741.6590\n",
      "Epoch [29/100], Loss: 739.8301\n",
      "Epoch [30/100], Loss: 740.1227\n",
      "Epoch [31/100], Loss: 739.9856\n",
      "Epoch [32/100], Loss: 740.8382\n",
      "Epoch [33/100], Loss: 738.0698\n",
      "Epoch [34/100], Loss: 738.1306\n",
      "Epoch [35/100], Loss: 739.4952\n",
      "Epoch [36/100], Loss: 737.6271\n",
      "Epoch [37/100], Loss: 732.1742\n",
      "Epoch [38/100], Loss: 734.4546\n",
      "Epoch [39/100], Loss: 730.7952\n",
      "Epoch [40/100], Loss: 733.4112\n",
      "Epoch [41/100], Loss: 733.5178\n",
      "Epoch [42/100], Loss: 731.5065\n",
      "Epoch [43/100], Loss: 731.2059\n",
      "Epoch [44/100], Loss: 737.1522\n",
      "Epoch [45/100], Loss: 732.6164\n",
      "Epoch [46/100], Loss: 732.1201\n",
      "Epoch [47/100], Loss: 731.7177\n",
      "Epoch [48/100], Loss: 729.6856\n",
      "Epoch [49/100], Loss: 729.2236\n",
      "Epoch [50/100], Loss: 728.8682\n",
      "Epoch [51/100], Loss: 728.2790\n",
      "Epoch [52/100], Loss: 733.0625\n",
      "Epoch [53/100], Loss: 730.6149\n",
      "Epoch [54/100], Loss: 727.4939\n",
      "Epoch [55/100], Loss: 730.8379\n",
      "Epoch [56/100], Loss: 728.2086\n",
      "Epoch [57/100], Loss: 729.7021\n",
      "Epoch [58/100], Loss: 728.2795\n",
      "Epoch [59/100], Loss: 725.6063\n",
      "Epoch [60/100], Loss: 720.7349\n",
      "Epoch [61/100], Loss: 712.7778\n",
      "Epoch [62/100], Loss: 702.9781\n",
      "Epoch [63/100], Loss: 692.2244\n",
      "Epoch [64/100], Loss: 686.5011\n",
      "Epoch [65/100], Loss: 682.3363\n",
      "Epoch [66/100], Loss: 682.5398\n",
      "Epoch [67/100], Loss: 678.8256\n",
      "Epoch [68/100], Loss: 676.6607\n",
      "Epoch [69/100], Loss: 674.8389\n",
      "Epoch [70/100], Loss: 673.4677\n",
      "Epoch [71/100], Loss: 670.6106\n",
      "Epoch [72/100], Loss: 673.9872\n",
      "Epoch [73/100], Loss: 673.4832\n",
      "Epoch [74/100], Loss: 670.9983\n",
      "Epoch [75/100], Loss: 670.0724\n",
      "Epoch [76/100], Loss: 672.1625\n",
      "Epoch [77/100], Loss: 667.3062\n",
      "Epoch [78/100], Loss: 667.3379\n",
      "Epoch [79/100], Loss: 668.9877\n",
      "Epoch [80/100], Loss: 668.8436\n",
      "Epoch [81/100], Loss: 667.4764\n",
      "Epoch [82/100], Loss: 667.2680\n",
      "Epoch [83/100], Loss: 666.4734\n",
      "Epoch [84/100], Loss: 665.7031\n",
      "Epoch [85/100], Loss: 665.6974\n",
      "Epoch [86/100], Loss: 664.4951\n",
      "Epoch [87/100], Loss: 663.1922\n",
      "Epoch [88/100], Loss: 663.7408\n",
      "Epoch [89/100], Loss: 664.3632\n",
      "Epoch [90/100], Loss: 663.0177\n",
      "Epoch [91/100], Loss: 663.5080\n",
      "Epoch [92/100], Loss: 662.0672\n",
      "Epoch [93/100], Loss: 661.9507\n",
      "Epoch [94/100], Loss: 662.3426\n",
      "Epoch [95/100], Loss: 660.4098\n",
      "Epoch [96/100], Loss: 660.8658\n",
      "Epoch [97/100], Loss: 660.0700\n",
      "Epoch [98/100], Loss: 660.2411\n",
      "Epoch [99/100], Loss: 660.2714\n",
      "Epoch [100/100], Loss: 657.5496\n",
      "Processed mouseintestine successfully.\n",
      "Epoch [1/100], Loss: 3411.0725\n",
      "Epoch [2/100], Loss: 1589.2676\n",
      "Epoch [3/100], Loss: 1049.5249\n",
      "Epoch [4/100], Loss: 887.7504\n",
      "Epoch [5/100], Loss: 818.5119\n",
      "Epoch [6/100], Loss: 784.4089\n",
      "Epoch [7/100], Loss: 766.0731\n",
      "Epoch [8/100], Loss: 751.7374\n",
      "Epoch [9/100], Loss: 742.8023\n",
      "Epoch [10/100], Loss: 739.0302\n",
      "Epoch [11/100], Loss: 735.3470\n",
      "Epoch [12/100], Loss: 730.2321\n",
      "Epoch [13/100], Loss: 727.7233\n",
      "Epoch [14/100], Loss: 727.4410\n",
      "Epoch [15/100], Loss: 725.8222\n",
      "Epoch [16/100], Loss: 724.9884\n",
      "Epoch [17/100], Loss: 722.9527\n",
      "Epoch [18/100], Loss: 721.9281\n",
      "Epoch [19/100], Loss: 720.6775\n",
      "Epoch [20/100], Loss: 719.6374\n",
      "Epoch [21/100], Loss: 717.4884\n",
      "Epoch [22/100], Loss: 717.0479\n",
      "Epoch [23/100], Loss: 716.9269\n",
      "Epoch [24/100], Loss: 717.1989\n",
      "Epoch [25/100], Loss: 715.9348\n",
      "Epoch [26/100], Loss: 714.8127\n",
      "Epoch [27/100], Loss: 716.6131\n",
      "Epoch [28/100], Loss: 715.0603\n",
      "Epoch [29/100], Loss: 715.4977\n",
      "Epoch [30/100], Loss: 714.9553\n",
      "Epoch [31/100], Loss: 712.5966\n",
      "Epoch [32/100], Loss: 713.1391\n",
      "Epoch [33/100], Loss: 712.2553\n",
      "Epoch [34/100], Loss: 712.1411\n",
      "Epoch [35/100], Loss: 712.5794\n",
      "Epoch [36/100], Loss: 710.3619\n",
      "Epoch [37/100], Loss: 707.2360\n",
      "Epoch [38/100], Loss: 700.5313\n",
      "Epoch [39/100], Loss: 699.1679\n",
      "Epoch [40/100], Loss: 696.4622\n",
      "Epoch [41/100], Loss: 693.0951\n",
      "Epoch [42/100], Loss: 692.9459\n",
      "Epoch [43/100], Loss: 689.6366\n",
      "Epoch [44/100], Loss: 688.6880\n",
      "Epoch [45/100], Loss: 688.4141\n",
      "Epoch [46/100], Loss: 685.7954\n",
      "Epoch [47/100], Loss: 686.4567\n",
      "Epoch [48/100], Loss: 685.7726\n",
      "Epoch [49/100], Loss: 685.1306\n",
      "Epoch [50/100], Loss: 682.3065\n",
      "Epoch [51/100], Loss: 682.4100\n",
      "Epoch [52/100], Loss: 682.3120\n",
      "Epoch [53/100], Loss: 680.0973\n",
      "Epoch [54/100], Loss: 680.5785\n",
      "Epoch [55/100], Loss: 679.3095\n",
      "Epoch [56/100], Loss: 679.9629\n",
      "Epoch [57/100], Loss: 678.6313\n",
      "Epoch [58/100], Loss: 677.2852\n",
      "Epoch [59/100], Loss: 676.4026\n",
      "Epoch [60/100], Loss: 675.1153\n",
      "Epoch [61/100], Loss: 672.3677\n",
      "Epoch [62/100], Loss: 671.9773\n",
      "Epoch [63/100], Loss: 667.0984\n",
      "Epoch [64/100], Loss: 666.5927\n",
      "Epoch [65/100], Loss: 663.7272\n",
      "Epoch [66/100], Loss: 663.9384\n",
      "Epoch [67/100], Loss: 662.5730\n",
      "Epoch [68/100], Loss: 661.4689\n",
      "Epoch [69/100], Loss: 660.0820\n",
      "Epoch [70/100], Loss: 659.5217\n",
      "Epoch [71/100], Loss: 659.2552\n",
      "Epoch [72/100], Loss: 658.4545\n",
      "Epoch [73/100], Loss: 657.9180\n",
      "Epoch [74/100], Loss: 657.7064\n",
      "Epoch [75/100], Loss: 656.6194\n",
      "Epoch [76/100], Loss: 656.2131\n",
      "Epoch [77/100], Loss: 657.1977\n",
      "Epoch [78/100], Loss: 656.9061\n",
      "Epoch [79/100], Loss: 655.3706\n",
      "Epoch [80/100], Loss: 654.9990\n",
      "Epoch [81/100], Loss: 657.1100\n",
      "Epoch [82/100], Loss: 655.4580\n",
      "Epoch [83/100], Loss: 654.7188\n",
      "Epoch [84/100], Loss: 654.2291\n",
      "Epoch [85/100], Loss: 654.0289\n",
      "Epoch [86/100], Loss: 654.0716\n",
      "Epoch [87/100], Loss: 652.7347\n",
      "Epoch [88/100], Loss: 652.1590\n",
      "Epoch [89/100], Loss: 653.5683\n",
      "Epoch [90/100], Loss: 650.9795\n",
      "Epoch [91/100], Loss: 652.9047\n",
      "Epoch [92/100], Loss: 651.6981\n",
      "Epoch [93/100], Loss: 651.6384\n",
      "Epoch [94/100], Loss: 651.7211\n",
      "Epoch [95/100], Loss: 651.1201\n",
      "Epoch [96/100], Loss: 650.9650\n",
      "Epoch [97/100], Loss: 651.4737\n",
      "Epoch [98/100], Loss: 651.6356\n",
      "Epoch [99/100], Loss: 650.4871\n",
      "Epoch [100/100], Loss: 649.4623\n",
      "Processed mousecolon successfully.\n",
      "Epoch [1/100], Loss: 4124.3875\n",
      "Epoch [2/100], Loss: 2536.1648\n",
      "Epoch [3/100], Loss: 1763.3946\n",
      "Epoch [4/100], Loss: 1395.5617\n",
      "Epoch [5/100], Loss: 1228.9137\n",
      "Epoch [6/100], Loss: 1146.9440\n",
      "Epoch [7/100], Loss: 1097.4607\n",
      "Epoch [8/100], Loss: 1067.0081\n",
      "Epoch [9/100], Loss: 1046.2510\n",
      "Epoch [10/100], Loss: 1030.7943\n",
      "Epoch [11/100], Loss: 1019.6111\n",
      "Epoch [12/100], Loss: 1009.3265\n",
      "Epoch [13/100], Loss: 1002.7904\n",
      "Epoch [14/100], Loss: 998.9331\n",
      "Epoch [15/100], Loss: 996.8569\n",
      "Epoch [16/100], Loss: 994.2960\n",
      "Epoch [17/100], Loss: 991.1353\n",
      "Epoch [18/100], Loss: 988.3290\n",
      "Epoch [19/100], Loss: 985.3730\n",
      "Epoch [20/100], Loss: 983.9407\n",
      "Epoch [21/100], Loss: 983.2805\n",
      "Epoch [22/100], Loss: 980.2313\n",
      "Epoch [23/100], Loss: 977.8620\n",
      "Epoch [24/100], Loss: 978.7119\n",
      "Epoch [25/100], Loss: 976.6648\n",
      "Epoch [26/100], Loss: 978.0027\n",
      "Epoch [27/100], Loss: 977.4456\n",
      "Epoch [28/100], Loss: 973.5057\n",
      "Epoch [29/100], Loss: 972.2075\n",
      "Epoch [30/100], Loss: 971.7034\n",
      "Epoch [31/100], Loss: 971.7557\n",
      "Epoch [32/100], Loss: 971.0005\n",
      "Epoch [33/100], Loss: 970.6979\n",
      "Epoch [34/100], Loss: 970.3843\n",
      "Epoch [35/100], Loss: 973.1093\n",
      "Epoch [36/100], Loss: 971.0899\n",
      "Epoch [37/100], Loss: 968.9819\n",
      "Epoch [38/100], Loss: 969.5182\n",
      "Epoch [39/100], Loss: 968.5667\n",
      "Epoch [40/100], Loss: 967.0848\n",
      "Epoch [41/100], Loss: 968.2886\n",
      "Epoch [42/100], Loss: 966.3542\n",
      "Epoch [43/100], Loss: 967.3726\n",
      "Epoch [44/100], Loss: 969.5578\n",
      "Epoch [45/100], Loss: 965.4455\n",
      "Epoch [46/100], Loss: 965.7218\n",
      "Epoch [47/100], Loss: 965.8664\n",
      "Epoch [48/100], Loss: 963.6636\n",
      "Epoch [49/100], Loss: 965.6717\n",
      "Epoch [50/100], Loss: 963.0014\n",
      "Epoch [51/100], Loss: 963.4837\n",
      "Epoch [52/100], Loss: 964.0151\n",
      "Epoch [53/100], Loss: 962.3154\n",
      "Epoch [54/100], Loss: 961.6169\n",
      "Epoch [55/100], Loss: 962.2390\n",
      "Epoch [56/100], Loss: 961.4652\n",
      "Epoch [57/100], Loss: 962.3412\n",
      "Epoch [58/100], Loss: 962.6751\n",
      "Epoch [59/100], Loss: 961.2595\n",
      "Epoch [60/100], Loss: 962.8251\n",
      "Epoch [61/100], Loss: 960.8088\n",
      "Epoch [62/100], Loss: 961.0331\n",
      "Epoch [63/100], Loss: 959.2243\n",
      "Epoch [64/100], Loss: 959.0562\n",
      "Epoch [65/100], Loss: 957.2583\n",
      "Epoch [66/100], Loss: 957.9262\n",
      "Epoch [67/100], Loss: 957.1500\n",
      "Epoch [68/100], Loss: 949.4504\n",
      "Epoch [69/100], Loss: 944.8967\n",
      "Epoch [70/100], Loss: 941.4138\n",
      "Epoch [71/100], Loss: 938.0235\n",
      "Epoch [72/100], Loss: 940.3730\n",
      "Epoch [73/100], Loss: 937.5158\n",
      "Epoch [74/100], Loss: 936.1859\n",
      "Epoch [75/100], Loss: 935.8782\n",
      "Epoch [76/100], Loss: 935.2406\n",
      "Epoch [77/100], Loss: 935.2300\n",
      "Epoch [78/100], Loss: 934.2349\n",
      "Epoch [79/100], Loss: 932.0201\n",
      "Epoch [80/100], Loss: 932.9316\n",
      "Epoch [81/100], Loss: 931.4882\n",
      "Epoch [82/100], Loss: 930.8551\n",
      "Epoch [83/100], Loss: 931.8084\n",
      "Epoch [84/100], Loss: 930.5179\n",
      "Epoch [85/100], Loss: 929.1356\n",
      "Epoch [86/100], Loss: 928.8754\n",
      "Epoch [87/100], Loss: 929.5681\n",
      "Epoch [88/100], Loss: 929.6320\n",
      "Epoch [89/100], Loss: 928.0863\n",
      "Epoch [90/100], Loss: 927.8757\n",
      "Epoch [91/100], Loss: 928.8581\n",
      "Epoch [92/100], Loss: 927.3642\n",
      "Epoch [93/100], Loss: 929.5055\n",
      "Epoch [94/100], Loss: 927.9313\n",
      "Epoch [95/100], Loss: 927.7973\n",
      "Epoch [96/100], Loss: 927.2540\n",
      "Epoch [97/100], Loss: 926.5562\n",
      "Epoch [98/100], Loss: 926.4267\n",
      "Epoch [99/100], Loss: 925.2287\n",
      "Epoch [100/100], Loss: 926.4051\n",
      "Processed mousespleen successfully.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tissues= ['humanGBM', 'humanskin', 'humanthymus', 'humanspleen', 'humantonsil', 'mousekidney', 'mouseintestine', 'mousecolon', 'mousespleen']\n",
    "\n",
    "for tissue in tissues:\n",
    "    rna_data = None\n",
    "    protein_data = None\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        if tissue in filename and filename.endswith(\"RNA.tsv.gz\"):\n",
    "            rna_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        elif tissue in filename and filename.endswith(\"protein.tsv.gz\"):\n",
    "            protein_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "            \n",
    "    rna_data.columns = rna_data.columns.astype(str)\n",
    "    protein_data.columns = protein_data.columns.astype(str)\n",
    "            \n",
    "    rna_data = rna_data.sort_values(by='X')\n",
    "    protein_data = protein_data.sort_values(by='X')\n",
    "    \n",
    "    rna_data = rna_data.reset_index(drop=True)\n",
    "    protein_data = protein_data.reset_index(drop=True)\n",
    "    rna_data.index = rna_data.index.astype(str)\n",
    "    protein_data.index = protein_data.index.astype(str)\n",
    "  \n",
    "    rna_data.drop(['X'], axis=1, inplace=True)\n",
    "    protein_data.drop(['X'], axis=1, inplace=True)\n",
    "    \n",
    "    rna_train, rna_test = train_test_split(rna_data, test_size=0.2, random_state=42)\n",
    "    protein_train = protein_data.loc[rna_train.index]\n",
    "    protein_test = protein_data.loc[rna_test.index]\n",
    "        \n",
    "    adata_rna_train = sc.AnnData(rna_train)\n",
    "    sc.pp.normalize_total(adata_rna_train, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_rna_train)\n",
    "    sc.pp.highly_variable_genes(adata_rna_train, n_top_genes=4000, flavor='seurat', subset=True)\n",
    "    counts_norm = adata_rna_train.X\n",
    "    rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "  \n",
    "    adata_protein_train = sc.AnnData(protein_train) \n",
    "    sc.pp.normalize_total(adata_protein_train, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_protein_train)\n",
    "    counts_norm = adata_protein_train.X\n",
    "    protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "    \n",
    "    combined_data = torch.cat([rna_counts_norm, protein_counts_norm], dim=1).to(device)\n",
    "\n",
    "    input_dim = combined_data.shape[1] \n",
    "    latent_dim = 32\n",
    "    model = VAE(input_dim, latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 100\n",
    "    \n",
    "    batch_size = 64\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        permutation = torch.randperm(combined_data.size(0))\n",
    "\n",
    "        for i in range(0, combined_data.size(0), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_data = combined_data[indices]\n",
    "\n",
    "            reconstructed_data, mean, logvar = model(batch_data)\n",
    "            loss = vae_loss(reconstructed_data, batch_data, mean, logvar, lambda_kl=0.0001)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(combined_data):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        combined_data = torch.cat([rna_counts_norm, torch.zeros(rna_counts_norm.shape[0], protein_counts_norm.shape[1]).to(device)], dim=1)\n",
    "        reconstructed_data, mean, logvar = model(combined_data)\n",
    "        reconstructed_protein_counts = reconstructed_data[:, rna_counts_norm.shape[1]:]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n",
    "        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n",
    "        avg_corr_pearson = pcc.mean()\n",
    "        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'RMSE': [rmse],\n",
    "            'Pearson Correlation': [avg_corr_pearson],\n",
    "            'SSIM':ssim_val\n",
    "        })\n",
    "        \n",
    "        results_file_path = os.path.join(results_dir, f\"{tissue}_training_results.csv\")\n",
    "        results_df.to_csv(results_file_path, index=False)\n",
    "        \n",
    "        adata_rna_test = sc.AnnData(rna_test) \n",
    "        sc.pp.normalize_total(adata_rna_test, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_rna_test)\n",
    "        counts_norm = adata_rna_test[:,  adata_rna_train.var_names].X\n",
    "        rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "    \n",
    "        adata_protein_test = sc.AnnData(protein_test) \n",
    "        sc.pp.normalize_total(adata_protein_test, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_protein_test)\n",
    "        counts_norm = adata_protein_test.X\n",
    "        protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "                \n",
    "        combined_data = torch.cat([rna_counts_norm, torch.zeros(rna_counts_norm.shape[0], protein_counts_norm.shape[1]).to(device)], dim=1)\n",
    "        reconstructed_data, mean, logvar = model(combined_data)\n",
    "        reconstructed_protein_counts = reconstructed_data[:, rna_counts_norm.shape[1]:]\n",
    "          \n",
    "        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n",
    "        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n",
    "        avg_corr_pearson = pcc.mean()\n",
    "        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'RMSE': [rmse],\n",
    "            'Pearson Correlation': [avg_corr_pearson],\n",
    "            'SSIM':ssim_val\n",
    "        })\n",
    "    \n",
    "    results_file_path = os.path.join(results_dir, f\"{tissue}_results.csv\")\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "    print(f\"Processed {tissue} successfully.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
