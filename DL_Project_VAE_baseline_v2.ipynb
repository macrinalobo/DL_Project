{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 19:08:23.333155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scanpy as sc\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'spatial_datasets/GSE213264_RAW/'\n",
    "results_dir = 'results_baseline_v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Loss: 697.9471\n",
      "Epoch [2/100], Loss: 317.1256\n",
      "Epoch [3/100], Loss: 266.1302\n",
      "Epoch [4/100], Loss: 244.9399\n",
      "Epoch [5/100], Loss: 232.2592\n",
      "Epoch [6/100], Loss: 225.2817\n",
      "Epoch [7/100], Loss: 218.3065\n",
      "Epoch [8/100], Loss: 216.9160\n",
      "Epoch [9/100], Loss: 215.3959\n",
      "Epoch [10/100], Loss: 211.2674\n",
      "Epoch [11/100], Loss: 209.0749\n",
      "Epoch [12/100], Loss: 207.7374\n",
      "Epoch [13/100], Loss: 207.8517\n",
      "Epoch [14/100], Loss: 206.1161\n",
      "Epoch [15/100], Loss: 206.1670\n",
      "Epoch [16/100], Loss: 202.7879\n",
      "Epoch [17/100], Loss: 205.0141\n",
      "Epoch [18/100], Loss: 204.3492\n",
      "Epoch [19/100], Loss: 202.8626\n",
      "Epoch [20/100], Loss: 203.0173\n",
      "Epoch [21/100], Loss: 202.3931\n",
      "Epoch [22/100], Loss: 201.8624\n",
      "Epoch [23/100], Loss: 201.0533\n",
      "Epoch [24/100], Loss: 202.3625\n",
      "Epoch [25/100], Loss: 200.3122\n",
      "Epoch [26/100], Loss: 201.3851\n",
      "Epoch [27/100], Loss: 199.1108\n",
      "Epoch [28/100], Loss: 200.7126\n",
      "Epoch [29/100], Loss: 199.4501\n",
      "Epoch [30/100], Loss: 201.8966\n",
      "Epoch [31/100], Loss: 200.2392\n",
      "Epoch [32/100], Loss: 199.7782\n",
      "Epoch [33/100], Loss: 200.6789\n",
      "Epoch [34/100], Loss: 200.7887\n",
      "Epoch [35/100], Loss: 200.4901\n",
      "Epoch [36/100], Loss: 199.5594\n",
      "Epoch [37/100], Loss: 199.9142\n",
      "Epoch [38/100], Loss: 198.5963\n",
      "Epoch [39/100], Loss: 198.7269\n",
      "Epoch [40/100], Loss: 199.6211\n",
      "Epoch [41/100], Loss: 199.1623\n",
      "Epoch [42/100], Loss: 198.8080\n",
      "Epoch [43/100], Loss: 199.4568\n",
      "Epoch [44/100], Loss: 199.9751\n",
      "Epoch [45/100], Loss: 198.7995\n",
      "Epoch [46/100], Loss: 199.3585\n",
      "Epoch [47/100], Loss: 199.3576\n",
      "Epoch [48/100], Loss: 198.2471\n",
      "Epoch [49/100], Loss: 197.8147\n",
      "Epoch [50/100], Loss: 198.0936\n",
      "Epoch [51/100], Loss: 197.6572\n",
      "Epoch [52/100], Loss: 197.7471\n",
      "Epoch [53/100], Loss: 198.0444\n",
      "Epoch [54/100], Loss: 197.9760\n",
      "Epoch [55/100], Loss: 196.2883\n",
      "Epoch [56/100], Loss: 196.7129\n",
      "Epoch [57/100], Loss: 197.4118\n",
      "Epoch [58/100], Loss: 198.2202\n",
      "Epoch [59/100], Loss: 198.5004\n",
      "Epoch [60/100], Loss: 196.8020\n",
      "Epoch [61/100], Loss: 197.9480\n",
      "Epoch [62/100], Loss: 197.0080\n",
      "Epoch [63/100], Loss: 196.7925\n",
      "Epoch [64/100], Loss: 197.9205\n",
      "Epoch [65/100], Loss: 197.5822\n",
      "Epoch [66/100], Loss: 196.7846\n",
      "Epoch [67/100], Loss: 197.3506\n",
      "Epoch [68/100], Loss: 196.4986\n",
      "Epoch [69/100], Loss: 196.8600\n",
      "Epoch [70/100], Loss: 198.0408\n",
      "Epoch [71/100], Loss: 197.1738\n",
      "Epoch [72/100], Loss: 197.2692\n",
      "Epoch [73/100], Loss: 195.1119\n",
      "Epoch [74/100], Loss: 193.7919\n",
      "Epoch [75/100], Loss: 192.7561\n",
      "Epoch [76/100], Loss: 192.6793\n",
      "Epoch [77/100], Loss: 192.3986\n",
      "Epoch [78/100], Loss: 189.9317\n",
      "Epoch [79/100], Loss: 190.2721\n",
      "Epoch [80/100], Loss: 188.0089\n",
      "Epoch [81/100], Loss: 188.1864\n",
      "Epoch [82/100], Loss: 188.6791\n",
      "Epoch [83/100], Loss: 189.6502\n",
      "Epoch [84/100], Loss: 187.6065\n",
      "Epoch [85/100], Loss: 187.2533\n",
      "Epoch [86/100], Loss: 186.9996\n",
      "Epoch [87/100], Loss: 185.4933\n",
      "Epoch [88/100], Loss: 186.3447\n",
      "Epoch [89/100], Loss: 187.1188\n",
      "Epoch [90/100], Loss: 186.4397\n",
      "Epoch [91/100], Loss: 185.6465\n",
      "Epoch [92/100], Loss: 186.0115\n",
      "Epoch [93/100], Loss: 185.8060\n",
      "Epoch [94/100], Loss: 185.6592\n",
      "Epoch [95/100], Loss: 184.7902\n",
      "Epoch [96/100], Loss: 185.3539\n",
      "Epoch [97/100], Loss: 185.1034\n",
      "Epoch [98/100], Loss: 185.6043\n",
      "Epoch [99/100], Loss: 184.5730\n",
      "Epoch [100/100], Loss: 184.4218\n",
      "Processed humanGBM successfully.\n",
      "Epoch [1/100], Loss: 900.0127\n",
      "Epoch [2/100], Loss: 602.5841\n",
      "Epoch [3/100], Loss: 546.0519\n",
      "Epoch [4/100], Loss: 512.2085\n",
      "Epoch [5/100], Loss: 500.9141\n",
      "Epoch [6/100], Loss: 491.7444\n",
      "Epoch [7/100], Loss: 485.9897\n",
      "Epoch [8/100], Loss: 481.3126\n",
      "Epoch [9/100], Loss: 474.6446\n",
      "Epoch [10/100], Loss: 470.9838\n",
      "Epoch [11/100], Loss: 461.1940\n",
      "Epoch [12/100], Loss: 450.4913\n",
      "Epoch [13/100], Loss: 444.6816\n",
      "Epoch [14/100], Loss: 436.9994\n",
      "Epoch [15/100], Loss: 433.7346\n",
      "Epoch [16/100], Loss: 432.8373\n",
      "Epoch [17/100], Loss: 429.8639\n",
      "Epoch [18/100], Loss: 426.4248\n",
      "Epoch [19/100], Loss: 425.3808\n",
      "Epoch [20/100], Loss: 424.1366\n",
      "Epoch [21/100], Loss: 422.1914\n",
      "Epoch [22/100], Loss: 422.0818\n",
      "Epoch [23/100], Loss: 420.3565\n",
      "Epoch [24/100], Loss: 420.0934\n",
      "Epoch [25/100], Loss: 419.7026\n",
      "Epoch [26/100], Loss: 418.8205\n",
      "Epoch [27/100], Loss: 417.8264\n",
      "Epoch [28/100], Loss: 416.7210\n",
      "Epoch [29/100], Loss: 415.6418\n",
      "Epoch [30/100], Loss: 417.5174\n",
      "Epoch [31/100], Loss: 416.2031\n",
      "Epoch [32/100], Loss: 416.1867\n",
      "Epoch [33/100], Loss: 416.0343\n",
      "Epoch [34/100], Loss: 415.2759\n",
      "Epoch [35/100], Loss: 414.1740\n",
      "Epoch [36/100], Loss: 416.3381\n",
      "Epoch [37/100], Loss: 414.3923\n",
      "Epoch [38/100], Loss: 412.7107\n",
      "Epoch [39/100], Loss: 412.5467\n",
      "Epoch [40/100], Loss: 412.4931\n",
      "Epoch [41/100], Loss: 411.9690\n",
      "Epoch [42/100], Loss: 412.2466\n",
      "Epoch [43/100], Loss: 411.5466\n",
      "Epoch [44/100], Loss: 410.9888\n",
      "Epoch [45/100], Loss: 411.3900\n",
      "Epoch [46/100], Loss: 410.3512\n",
      "Epoch [47/100], Loss: 411.3963\n",
      "Epoch [48/100], Loss: 410.7029\n",
      "Epoch [49/100], Loss: 410.5090\n",
      "Epoch [50/100], Loss: 411.5290\n",
      "Epoch [51/100], Loss: 409.5582\n",
      "Epoch [52/100], Loss: 409.1880\n",
      "Epoch [53/100], Loss: 409.6722\n",
      "Epoch [54/100], Loss: 409.2329\n",
      "Epoch [55/100], Loss: 410.1705\n",
      "Epoch [56/100], Loss: 408.3544\n",
      "Epoch [57/100], Loss: 409.2782\n",
      "Epoch [58/100], Loss: 408.9694\n",
      "Epoch [59/100], Loss: 408.4883\n",
      "Epoch [60/100], Loss: 408.3011\n",
      "Epoch [61/100], Loss: 408.3809\n",
      "Epoch [62/100], Loss: 409.1543\n",
      "Epoch [63/100], Loss: 408.1936\n",
      "Epoch [64/100], Loss: 408.4914\n",
      "Epoch [65/100], Loss: 408.5447\n",
      "Epoch [66/100], Loss: 407.5335\n",
      "Epoch [67/100], Loss: 408.6735\n",
      "Epoch [68/100], Loss: 407.4183\n",
      "Epoch [69/100], Loss: 407.2422\n",
      "Epoch [70/100], Loss: 408.3478\n",
      "Epoch [71/100], Loss: 407.0010\n",
      "Epoch [72/100], Loss: 406.3393\n",
      "Epoch [73/100], Loss: 407.7440\n",
      "Epoch [74/100], Loss: 406.9270\n",
      "Epoch [75/100], Loss: 407.2278\n",
      "Epoch [76/100], Loss: 405.9479\n",
      "Epoch [77/100], Loss: 406.3443\n",
      "Epoch [78/100], Loss: 405.8771\n",
      "Epoch [79/100], Loss: 405.0054\n",
      "Epoch [80/100], Loss: 404.2065\n",
      "Epoch [81/100], Loss: 404.9180\n",
      "Epoch [82/100], Loss: 404.3058\n",
      "Epoch [83/100], Loss: 403.9322\n",
      "Epoch [84/100], Loss: 402.0169\n",
      "Epoch [85/100], Loss: 400.8265\n",
      "Epoch [86/100], Loss: 401.4173\n",
      "Epoch [87/100], Loss: 399.3688\n",
      "Epoch [88/100], Loss: 398.8906\n",
      "Epoch [89/100], Loss: 399.4874\n",
      "Epoch [90/100], Loss: 398.5672\n",
      "Epoch [91/100], Loss: 398.8241\n",
      "Epoch [92/100], Loss: 399.3527\n",
      "Epoch [93/100], Loss: 398.2580\n",
      "Epoch [94/100], Loss: 397.9661\n",
      "Epoch [95/100], Loss: 397.7003\n",
      "Epoch [96/100], Loss: 397.2715\n",
      "Epoch [97/100], Loss: 398.7431\n",
      "Epoch [98/100], Loss: 397.0516\n",
      "Epoch [99/100], Loss: 396.6795\n",
      "Epoch [100/100], Loss: 396.3895\n",
      "Processed humanskin successfully.\n",
      "Epoch [1/100], Loss: 641.6078\n",
      "Epoch [2/100], Loss: 251.6875\n",
      "Epoch [3/100], Loss: 197.2072\n",
      "Epoch [4/100], Loss: 175.2739\n",
      "Epoch [5/100], Loss: 162.4060\n",
      "Epoch [6/100], Loss: 155.8298\n",
      "Epoch [7/100], Loss: 150.4060\n",
      "Epoch [8/100], Loss: 147.0448\n",
      "Epoch [9/100], Loss: 144.6610\n",
      "Epoch [10/100], Loss: 143.5198\n",
      "Epoch [11/100], Loss: 141.9495\n",
      "Epoch [12/100], Loss: 140.9726\n",
      "Epoch [13/100], Loss: 139.8087\n",
      "Epoch [14/100], Loss: 139.3510\n",
      "Epoch [15/100], Loss: 138.3018\n",
      "Epoch [16/100], Loss: 137.7660\n",
      "Epoch [17/100], Loss: 136.0476\n",
      "Epoch [18/100], Loss: 135.0836\n",
      "Epoch [19/100], Loss: 135.0181\n",
      "Epoch [20/100], Loss: 135.7092\n",
      "Epoch [21/100], Loss: 135.6042\n",
      "Epoch [22/100], Loss: 135.7411\n",
      "Epoch [23/100], Loss: 134.4417\n",
      "Epoch [24/100], Loss: 135.4180\n",
      "Epoch [25/100], Loss: 134.8476\n",
      "Epoch [26/100], Loss: 135.0191\n",
      "Epoch [27/100], Loss: 134.0621\n",
      "Epoch [28/100], Loss: 132.8255\n",
      "Epoch [29/100], Loss: 133.6691\n",
      "Epoch [30/100], Loss: 134.4086\n",
      "Epoch [31/100], Loss: 133.1939\n",
      "Epoch [32/100], Loss: 134.3737\n",
      "Epoch [33/100], Loss: 133.1697\n",
      "Epoch [34/100], Loss: 132.5088\n",
      "Epoch [35/100], Loss: 132.6568\n",
      "Epoch [36/100], Loss: 132.5826\n",
      "Epoch [37/100], Loss: 132.3826\n",
      "Epoch [38/100], Loss: 132.4666\n",
      "Epoch [39/100], Loss: 131.5871\n",
      "Epoch [40/100], Loss: 132.3319\n",
      "Epoch [41/100], Loss: 132.2353\n",
      "Epoch [42/100], Loss: 132.3906\n",
      "Epoch [43/100], Loss: 132.6236\n",
      "Epoch [44/100], Loss: 131.3311\n",
      "Epoch [45/100], Loss: 132.0135\n",
      "Epoch [46/100], Loss: 131.7554\n",
      "Epoch [47/100], Loss: 131.8883\n",
      "Epoch [48/100], Loss: 132.0317\n",
      "Epoch [49/100], Loss: 130.8639\n",
      "Epoch [50/100], Loss: 130.8050\n",
      "Epoch [51/100], Loss: 130.7918\n",
      "Epoch [52/100], Loss: 130.2635\n",
      "Epoch [53/100], Loss: 130.9730\n",
      "Epoch [54/100], Loss: 131.1889\n",
      "Epoch [55/100], Loss: 131.1586\n",
      "Epoch [56/100], Loss: 130.9761\n",
      "Epoch [57/100], Loss: 130.4379\n",
      "Epoch [58/100], Loss: 130.4431\n",
      "Epoch [59/100], Loss: 130.3625\n",
      "Epoch [60/100], Loss: 131.0572\n",
      "Epoch [61/100], Loss: 131.2975\n",
      "Epoch [62/100], Loss: 129.4946\n",
      "Epoch [63/100], Loss: 130.2280\n",
      "Epoch [64/100], Loss: 129.4660\n",
      "Epoch [65/100], Loss: 129.2351\n",
      "Epoch [66/100], Loss: 130.1086\n",
      "Epoch [67/100], Loss: 129.5056\n",
      "Epoch [68/100], Loss: 130.2299\n",
      "Epoch [69/100], Loss: 129.0799\n",
      "Epoch [70/100], Loss: 130.1370\n",
      "Epoch [71/100], Loss: 129.5350\n",
      "Epoch [72/100], Loss: 128.7692\n",
      "Epoch [73/100], Loss: 128.7298\n",
      "Epoch [74/100], Loss: 129.1732\n",
      "Epoch [75/100], Loss: 129.6518\n",
      "Epoch [76/100], Loss: 128.9436\n",
      "Epoch [77/100], Loss: 129.3160\n",
      "Epoch [78/100], Loss: 128.7089\n",
      "Epoch [79/100], Loss: 128.5588\n",
      "Epoch [80/100], Loss: 128.2918\n",
      "Epoch [81/100], Loss: 128.0153\n",
      "Epoch [82/100], Loss: 128.6582\n",
      "Epoch [83/100], Loss: 128.0017\n",
      "Epoch [84/100], Loss: 128.0513\n",
      "Epoch [85/100], Loss: 128.3239\n",
      "Epoch [86/100], Loss: 128.2686\n",
      "Epoch [87/100], Loss: 128.1086\n",
      "Epoch [88/100], Loss: 128.0228\n",
      "Epoch [89/100], Loss: 128.1826\n",
      "Epoch [90/100], Loss: 127.8041\n",
      "Epoch [91/100], Loss: 128.4473\n",
      "Epoch [92/100], Loss: 127.7251\n",
      "Epoch [93/100], Loss: 128.4855\n",
      "Epoch [94/100], Loss: 127.9656\n",
      "Epoch [95/100], Loss: 127.8700\n",
      "Epoch [96/100], Loss: 127.3506\n",
      "Epoch [97/100], Loss: 127.9045\n",
      "Epoch [98/100], Loss: 126.9752\n",
      "Epoch [99/100], Loss: 127.4116\n",
      "Epoch [100/100], Loss: 127.0103\n",
      "Processed humanthymus successfully.\n",
      "Epoch [1/100], Loss: 719.4981\n",
      "Epoch [2/100], Loss: 422.6729\n",
      "Epoch [3/100], Loss: 373.3575\n",
      "Epoch [4/100], Loss: 352.2762\n",
      "Epoch [5/100], Loss: 342.1599\n",
      "Epoch [6/100], Loss: 333.6769\n",
      "Epoch [7/100], Loss: 331.4810\n",
      "Epoch [8/100], Loss: 327.1996\n",
      "Epoch [9/100], Loss: 326.1059\n",
      "Epoch [10/100], Loss: 324.6296\n",
      "Epoch [11/100], Loss: 324.2371\n",
      "Epoch [12/100], Loss: 322.3135\n",
      "Epoch [13/100], Loss: 320.9017\n",
      "Epoch [14/100], Loss: 321.2194\n",
      "Epoch [15/100], Loss: 320.5790\n",
      "Epoch [16/100], Loss: 318.9794\n",
      "Epoch [17/100], Loss: 319.7486\n",
      "Epoch [18/100], Loss: 319.4726\n",
      "Epoch [19/100], Loss: 318.9601\n",
      "Epoch [20/100], Loss: 318.1412\n",
      "Epoch [21/100], Loss: 317.5406\n",
      "Epoch [22/100], Loss: 317.1434\n",
      "Epoch [23/100], Loss: 317.1323\n",
      "Epoch [24/100], Loss: 317.2690\n",
      "Epoch [25/100], Loss: 317.0732\n",
      "Epoch [26/100], Loss: 315.2897\n",
      "Epoch [27/100], Loss: 315.9506\n",
      "Epoch [28/100], Loss: 315.0306\n",
      "Epoch [29/100], Loss: 313.8839\n",
      "Epoch [30/100], Loss: 312.8389\n",
      "Epoch [31/100], Loss: 311.4496\n",
      "Epoch [32/100], Loss: 308.7908\n",
      "Epoch [33/100], Loss: 306.7258\n",
      "Epoch [34/100], Loss: 305.9454\n",
      "Epoch [35/100], Loss: 305.9981\n",
      "Epoch [36/100], Loss: 305.3374\n",
      "Epoch [37/100], Loss: 303.7667\n",
      "Epoch [38/100], Loss: 303.2877\n",
      "Epoch [39/100], Loss: 302.9417\n",
      "Epoch [40/100], Loss: 303.5861\n",
      "Epoch [41/100], Loss: 302.3372\n",
      "Epoch [42/100], Loss: 301.8904\n",
      "Epoch [43/100], Loss: 300.9491\n",
      "Epoch [44/100], Loss: 300.9938\n",
      "Epoch [45/100], Loss: 299.8239\n",
      "Epoch [46/100], Loss: 300.1823\n",
      "Epoch [47/100], Loss: 300.5575\n",
      "Epoch [48/100], Loss: 300.1287\n",
      "Epoch [49/100], Loss: 299.6231\n",
      "Epoch [50/100], Loss: 300.3408\n",
      "Epoch [51/100], Loss: 299.1182\n",
      "Epoch [52/100], Loss: 298.3148\n",
      "Epoch [53/100], Loss: 298.4581\n",
      "Epoch [54/100], Loss: 298.7794\n",
      "Epoch [55/100], Loss: 298.9194\n",
      "Epoch [56/100], Loss: 297.9631\n",
      "Epoch [57/100], Loss: 298.1716\n",
      "Epoch [58/100], Loss: 298.2096\n",
      "Epoch [59/100], Loss: 298.0236\n",
      "Epoch [60/100], Loss: 297.5373\n",
      "Epoch [61/100], Loss: 297.6678\n",
      "Epoch [62/100], Loss: 297.6126\n",
      "Epoch [63/100], Loss: 297.7150\n",
      "Epoch [64/100], Loss: 296.8188\n",
      "Epoch [65/100], Loss: 296.9308\n",
      "Epoch [66/100], Loss: 296.9805\n",
      "Epoch [67/100], Loss: 296.7395\n",
      "Epoch [68/100], Loss: 296.6901\n",
      "Epoch [69/100], Loss: 297.2474\n",
      "Epoch [70/100], Loss: 297.0723\n",
      "Epoch [71/100], Loss: 296.3510\n",
      "Epoch [72/100], Loss: 296.4612\n",
      "Epoch [73/100], Loss: 296.6673\n",
      "Epoch [74/100], Loss: 296.2882\n",
      "Epoch [75/100], Loss: 296.2612\n",
      "Epoch [76/100], Loss: 296.1005\n",
      "Epoch [77/100], Loss: 296.1285\n",
      "Epoch [78/100], Loss: 295.6438\n",
      "Epoch [79/100], Loss: 296.2645\n",
      "Epoch [80/100], Loss: 295.1682\n",
      "Epoch [81/100], Loss: 295.4365\n",
      "Epoch [82/100], Loss: 295.9835\n",
      "Epoch [83/100], Loss: 295.0454\n",
      "Epoch [84/100], Loss: 294.9797\n",
      "Epoch [85/100], Loss: 294.8180\n",
      "Epoch [86/100], Loss: 295.3307\n",
      "Epoch [87/100], Loss: 295.2088\n",
      "Epoch [88/100], Loss: 294.9142\n",
      "Epoch [89/100], Loss: 294.6752\n",
      "Epoch [90/100], Loss: 294.2569\n",
      "Epoch [91/100], Loss: 294.6575\n",
      "Epoch [92/100], Loss: 294.6585\n",
      "Epoch [93/100], Loss: 294.9055\n",
      "Epoch [94/100], Loss: 294.7122\n",
      "Epoch [95/100], Loss: 294.7549\n",
      "Epoch [96/100], Loss: 294.4676\n",
      "Epoch [97/100], Loss: 294.9187\n",
      "Epoch [98/100], Loss: 294.3429\n",
      "Epoch [99/100], Loss: 294.4543\n",
      "Epoch [100/100], Loss: 293.8446\n",
      "Processed humanspleen successfully.\n",
      "Epoch [1/100], Loss: 614.2673\n",
      "Epoch [2/100], Loss: 242.9201\n",
      "Epoch [3/100], Loss: 192.0659\n",
      "Epoch [4/100], Loss: 171.4698\n",
      "Epoch [5/100], Loss: 160.0904\n",
      "Epoch [6/100], Loss: 153.6908\n",
      "Epoch [7/100], Loss: 148.7264\n",
      "Epoch [8/100], Loss: 145.7540\n",
      "Epoch [9/100], Loss: 143.4933\n",
      "Epoch [10/100], Loss: 143.3793\n",
      "Epoch [11/100], Loss: 140.8602\n",
      "Epoch [12/100], Loss: 139.2353\n",
      "Epoch [13/100], Loss: 139.1532\n",
      "Epoch [14/100], Loss: 138.1577\n",
      "Epoch [15/100], Loss: 137.7339\n",
      "Epoch [16/100], Loss: 137.2204\n",
      "Epoch [17/100], Loss: 136.7264\n",
      "Epoch [18/100], Loss: 136.6460\n",
      "Epoch [19/100], Loss: 136.1518\n",
      "Epoch [20/100], Loss: 134.7918\n",
      "Epoch [21/100], Loss: 135.8726\n",
      "Epoch [22/100], Loss: 135.5132\n",
      "Epoch [23/100], Loss: 135.2978\n",
      "Epoch [24/100], Loss: 134.7086\n",
      "Epoch [25/100], Loss: 134.9283\n",
      "Epoch [26/100], Loss: 132.9101\n",
      "Epoch [27/100], Loss: 134.9121\n",
      "Epoch [28/100], Loss: 133.8885\n",
      "Epoch [29/100], Loss: 134.4715\n",
      "Epoch [30/100], Loss: 133.7271\n",
      "Epoch [31/100], Loss: 133.8338\n",
      "Epoch [32/100], Loss: 133.6981\n",
      "Epoch [33/100], Loss: 133.3478\n",
      "Epoch [34/100], Loss: 133.0741\n",
      "Epoch [35/100], Loss: 133.2795\n",
      "Epoch [36/100], Loss: 133.8057\n",
      "Epoch [37/100], Loss: 132.8547\n",
      "Epoch [38/100], Loss: 132.2582\n",
      "Epoch [39/100], Loss: 132.2109\n",
      "Epoch [40/100], Loss: 131.8583\n",
      "Epoch [41/100], Loss: 132.7471\n",
      "Epoch [42/100], Loss: 131.6536\n",
      "Epoch [43/100], Loss: 131.7938\n",
      "Epoch [44/100], Loss: 132.1249\n",
      "Epoch [45/100], Loss: 131.3072\n",
      "Epoch [46/100], Loss: 131.7167\n",
      "Epoch [47/100], Loss: 132.8391\n",
      "Epoch [48/100], Loss: 132.2394\n",
      "Epoch [49/100], Loss: 130.6861\n",
      "Epoch [50/100], Loss: 130.8996\n",
      "Epoch [51/100], Loss: 130.1783\n",
      "Epoch [52/100], Loss: 130.6964\n",
      "Epoch [53/100], Loss: 131.2032\n",
      "Epoch [54/100], Loss: 130.4020\n",
      "Epoch [55/100], Loss: 130.9375\n",
      "Epoch [56/100], Loss: 132.2711\n",
      "Epoch [57/100], Loss: 130.7741\n",
      "Epoch [58/100], Loss: 130.7178\n",
      "Epoch [59/100], Loss: 131.0546\n",
      "Epoch [60/100], Loss: 130.6539\n",
      "Epoch [61/100], Loss: 130.9624\n",
      "Epoch [62/100], Loss: 129.7984\n",
      "Epoch [63/100], Loss: 130.6501\n",
      "Epoch [64/100], Loss: 129.5807\n",
      "Epoch [65/100], Loss: 129.8325\n",
      "Epoch [66/100], Loss: 130.1300\n",
      "Epoch [67/100], Loss: 130.1247\n",
      "Epoch [68/100], Loss: 130.3896\n",
      "Epoch [69/100], Loss: 129.3607\n",
      "Epoch [70/100], Loss: 129.4245\n",
      "Epoch [71/100], Loss: 129.5904\n",
      "Epoch [72/100], Loss: 129.7777\n",
      "Epoch [73/100], Loss: 128.8532\n",
      "Epoch [74/100], Loss: 130.4603\n",
      "Epoch [75/100], Loss: 129.9176\n",
      "Epoch [76/100], Loss: 129.3212\n",
      "Epoch [77/100], Loss: 128.9053\n",
      "Epoch [78/100], Loss: 129.0418\n",
      "Epoch [79/100], Loss: 128.7000\n",
      "Epoch [80/100], Loss: 129.1043\n",
      "Epoch [81/100], Loss: 128.8916\n",
      "Epoch [82/100], Loss: 128.8109\n",
      "Epoch [83/100], Loss: 128.7877\n",
      "Epoch [84/100], Loss: 129.3222\n",
      "Epoch [85/100], Loss: 128.6941\n",
      "Epoch [86/100], Loss: 127.7697\n",
      "Epoch [87/100], Loss: 128.5390\n",
      "Epoch [88/100], Loss: 128.1883\n",
      "Epoch [89/100], Loss: 128.1356\n",
      "Epoch [90/100], Loss: 127.8247\n",
      "Epoch [91/100], Loss: 128.1861\n",
      "Epoch [92/100], Loss: 128.3073\n",
      "Epoch [93/100], Loss: 127.7591\n",
      "Epoch [94/100], Loss: 128.9399\n",
      "Epoch [95/100], Loss: 128.2250\n",
      "Epoch [96/100], Loss: 128.1915\n",
      "Epoch [97/100], Loss: 127.8487\n",
      "Epoch [98/100], Loss: 127.8698\n",
      "Epoch [99/100], Loss: 127.5808\n",
      "Epoch [100/100], Loss: 126.8874\n",
      "Processed humantonsil successfully.\n",
      "Epoch [1/100], Loss: 580.0115\n",
      "Epoch [2/100], Loss: 300.9368\n",
      "Epoch [3/100], Loss: 264.5098\n",
      "Epoch [4/100], Loss: 248.4460\n",
      "Epoch [5/100], Loss: 240.2536\n",
      "Epoch [6/100], Loss: 234.0097\n",
      "Epoch [7/100], Loss: 231.4028\n",
      "Epoch [8/100], Loss: 229.1635\n",
      "Epoch [9/100], Loss: 226.9307\n",
      "Epoch [10/100], Loss: 225.5762\n",
      "Epoch [11/100], Loss: 225.1100\n",
      "Epoch [12/100], Loss: 222.6695\n",
      "Epoch [13/100], Loss: 221.6158\n",
      "Epoch [14/100], Loss: 222.0704\n",
      "Epoch [15/100], Loss: 220.3313\n",
      "Epoch [16/100], Loss: 221.3364\n",
      "Epoch [17/100], Loss: 219.8190\n",
      "Epoch [18/100], Loss: 220.4709\n",
      "Epoch [19/100], Loss: 219.0728\n",
      "Epoch [20/100], Loss: 218.2830\n",
      "Epoch [21/100], Loss: 219.2213\n",
      "Epoch [22/100], Loss: 219.2195\n",
      "Epoch [23/100], Loss: 218.3572\n",
      "Epoch [24/100], Loss: 219.6805\n",
      "Epoch [25/100], Loss: 218.6374\n",
      "Epoch [26/100], Loss: 218.2592\n",
      "Epoch [27/100], Loss: 217.7702\n",
      "Epoch [28/100], Loss: 217.9100\n",
      "Epoch [29/100], Loss: 218.1140\n",
      "Epoch [30/100], Loss: 216.7895\n",
      "Epoch [31/100], Loss: 217.1976\n",
      "Epoch [32/100], Loss: 218.1807\n",
      "Epoch [33/100], Loss: 216.2252\n",
      "Epoch [34/100], Loss: 217.1531\n",
      "Epoch [35/100], Loss: 217.2656\n",
      "Epoch [36/100], Loss: 216.3786\n",
      "Epoch [37/100], Loss: 215.7666\n",
      "Epoch [38/100], Loss: 216.6219\n",
      "Epoch [39/100], Loss: 216.1712\n",
      "Epoch [40/100], Loss: 216.2380\n",
      "Epoch [41/100], Loss: 216.1303\n",
      "Epoch [42/100], Loss: 216.3587\n",
      "Epoch [43/100], Loss: 215.9970\n",
      "Epoch [44/100], Loss: 216.3673\n",
      "Epoch [45/100], Loss: 215.4907\n",
      "Epoch [46/100], Loss: 216.0968\n",
      "Epoch [47/100], Loss: 215.3904\n",
      "Epoch [48/100], Loss: 215.7484\n",
      "Epoch [49/100], Loss: 215.4686\n",
      "Epoch [50/100], Loss: 215.2597\n",
      "Epoch [51/100], Loss: 214.9361\n",
      "Epoch [52/100], Loss: 215.4414\n",
      "Epoch [53/100], Loss: 214.9883\n",
      "Epoch [54/100], Loss: 215.1117\n",
      "Epoch [55/100], Loss: 215.1670\n",
      "Epoch [56/100], Loss: 214.4740\n",
      "Epoch [57/100], Loss: 214.6535\n",
      "Epoch [58/100], Loss: 214.3170\n",
      "Epoch [59/100], Loss: 214.6192\n",
      "Epoch [60/100], Loss: 214.6927\n",
      "Epoch [61/100], Loss: 215.0571\n",
      "Epoch [62/100], Loss: 214.0986\n",
      "Epoch [63/100], Loss: 214.3514\n",
      "Epoch [64/100], Loss: 214.2576\n",
      "Epoch [65/100], Loss: 213.5466\n",
      "Epoch [66/100], Loss: 213.8599\n",
      "Epoch [67/100], Loss: 213.9015\n",
      "Epoch [68/100], Loss: 214.3502\n",
      "Epoch [69/100], Loss: 213.3297\n",
      "Epoch [70/100], Loss: 213.9080\n",
      "Epoch [71/100], Loss: 213.2808\n",
      "Epoch [72/100], Loss: 212.6782\n",
      "Epoch [73/100], Loss: 212.8266\n",
      "Epoch [74/100], Loss: 211.9113\n",
      "Epoch [75/100], Loss: 211.2771\n",
      "Epoch [76/100], Loss: 211.0214\n",
      "Epoch [77/100], Loss: 210.4162\n",
      "Epoch [78/100], Loss: 209.8496\n",
      "Epoch [79/100], Loss: 208.9838\n",
      "Epoch [80/100], Loss: 209.0435\n",
      "Epoch [81/100], Loss: 208.2173\n",
      "Epoch [82/100], Loss: 208.7271\n",
      "Epoch [83/100], Loss: 208.6323\n",
      "Epoch [84/100], Loss: 207.8903\n",
      "Epoch [85/100], Loss: 207.8424\n",
      "Epoch [86/100], Loss: 207.6324\n",
      "Epoch [87/100], Loss: 207.0481\n",
      "Epoch [88/100], Loss: 207.5400\n",
      "Epoch [89/100], Loss: 207.0801\n",
      "Epoch [90/100], Loss: 207.8121\n",
      "Epoch [91/100], Loss: 206.7440\n",
      "Epoch [92/100], Loss: 207.2362\n",
      "Epoch [93/100], Loss: 206.9813\n",
      "Epoch [94/100], Loss: 206.5124\n",
      "Epoch [95/100], Loss: 206.4847\n",
      "Epoch [96/100], Loss: 206.0453\n",
      "Epoch [97/100], Loss: 206.6241\n",
      "Epoch [98/100], Loss: 206.1681\n",
      "Epoch [99/100], Loss: 206.5430\n",
      "Epoch [100/100], Loss: 205.9228\n",
      "Processed mousekidney successfully.\n",
      "Epoch [1/100], Loss: 887.4796\n",
      "Epoch [2/100], Loss: 496.5887\n",
      "Epoch [3/100], Loss: 427.1554\n",
      "Epoch [4/100], Loss: 392.0851\n",
      "Epoch [5/100], Loss: 378.4982\n",
      "Epoch [6/100], Loss: 364.4701\n",
      "Epoch [7/100], Loss: 354.0787\n",
      "Epoch [8/100], Loss: 345.8355\n",
      "Epoch [9/100], Loss: 342.4261\n",
      "Epoch [10/100], Loss: 337.5047\n",
      "Epoch [11/100], Loss: 332.6797\n",
      "Epoch [12/100], Loss: 326.9805\n",
      "Epoch [13/100], Loss: 325.2650\n",
      "Epoch [14/100], Loss: 328.0148\n",
      "Epoch [15/100], Loss: 320.9994\n",
      "Epoch [16/100], Loss: 327.3875\n",
      "Epoch [17/100], Loss: 324.5268\n",
      "Epoch [18/100], Loss: 322.2127\n",
      "Epoch [19/100], Loss: 320.1233\n",
      "Epoch [20/100], Loss: 308.4456\n",
      "Epoch [21/100], Loss: 293.7642\n",
      "Epoch [22/100], Loss: 285.2814\n",
      "Epoch [23/100], Loss: 278.8944\n",
      "Epoch [24/100], Loss: 277.9344\n",
      "Epoch [25/100], Loss: 270.9585\n",
      "Epoch [26/100], Loss: 268.4151\n",
      "Epoch [27/100], Loss: 267.7854\n",
      "Epoch [28/100], Loss: 262.8236\n",
      "Epoch [29/100], Loss: 263.4415\n",
      "Epoch [30/100], Loss: 261.6821\n",
      "Epoch [31/100], Loss: 260.8793\n",
      "Epoch [32/100], Loss: 258.6856\n",
      "Epoch [33/100], Loss: 258.4188\n",
      "Epoch [34/100], Loss: 257.3488\n",
      "Epoch [35/100], Loss: 256.3402\n",
      "Epoch [36/100], Loss: 256.8691\n",
      "Epoch [37/100], Loss: 256.8845\n",
      "Epoch [38/100], Loss: 255.2989\n",
      "Epoch [39/100], Loss: 253.8687\n",
      "Epoch [40/100], Loss: 253.6979\n",
      "Epoch [41/100], Loss: 253.3787\n",
      "Epoch [42/100], Loss: 253.3837\n",
      "Epoch [43/100], Loss: 253.5364\n",
      "Epoch [44/100], Loss: 253.4466\n",
      "Epoch [45/100], Loss: 253.7962\n",
      "Epoch [46/100], Loss: 250.5476\n",
      "Epoch [47/100], Loss: 249.1720\n",
      "Epoch [48/100], Loss: 253.2910\n",
      "Epoch [49/100], Loss: 251.6904\n",
      "Epoch [50/100], Loss: 249.9825\n",
      "Epoch [51/100], Loss: 250.6361\n",
      "Epoch [52/100], Loss: 250.8528\n",
      "Epoch [53/100], Loss: 249.6576\n",
      "Epoch [54/100], Loss: 248.6094\n",
      "Epoch [55/100], Loss: 248.7672\n",
      "Epoch [56/100], Loss: 251.4612\n",
      "Epoch [57/100], Loss: 249.2444\n",
      "Epoch [58/100], Loss: 249.2229\n",
      "Epoch [59/100], Loss: 248.5499\n",
      "Epoch [60/100], Loss: 250.3190\n",
      "Epoch [61/100], Loss: 248.5967\n",
      "Epoch [62/100], Loss: 248.1890\n",
      "Epoch [63/100], Loss: 248.4947\n",
      "Epoch [64/100], Loss: 248.7789\n",
      "Epoch [65/100], Loss: 246.2137\n",
      "Epoch [66/100], Loss: 247.0792\n",
      "Epoch [67/100], Loss: 247.3108\n",
      "Epoch [68/100], Loss: 245.7397\n",
      "Epoch [69/100], Loss: 246.8377\n",
      "Epoch [70/100], Loss: 245.8317\n",
      "Epoch [71/100], Loss: 246.5464\n",
      "Epoch [72/100], Loss: 249.3162\n",
      "Epoch [73/100], Loss: 246.7980\n",
      "Epoch [74/100], Loss: 246.4761\n",
      "Epoch [75/100], Loss: 245.5148\n",
      "Epoch [76/100], Loss: 245.8100\n",
      "Epoch [77/100], Loss: 246.0587\n",
      "Epoch [78/100], Loss: 246.1564\n",
      "Epoch [79/100], Loss: 245.0501\n",
      "Epoch [80/100], Loss: 244.9515\n",
      "Epoch [81/100], Loss: 245.6544\n",
      "Epoch [82/100], Loss: 244.9418\n",
      "Epoch [83/100], Loss: 246.5042\n",
      "Epoch [84/100], Loss: 244.1666\n",
      "Epoch [85/100], Loss: 244.4492\n",
      "Epoch [86/100], Loss: 244.7709\n",
      "Epoch [87/100], Loss: 243.7824\n",
      "Epoch [88/100], Loss: 243.7508\n",
      "Epoch [89/100], Loss: 242.8212\n",
      "Epoch [90/100], Loss: 242.4733\n",
      "Epoch [91/100], Loss: 244.1060\n",
      "Epoch [92/100], Loss: 244.7143\n",
      "Epoch [93/100], Loss: 243.0699\n",
      "Epoch [94/100], Loss: 242.1635\n",
      "Epoch [95/100], Loss: 241.6535\n",
      "Epoch [96/100], Loss: 242.9129\n",
      "Epoch [97/100], Loss: 240.8848\n",
      "Epoch [98/100], Loss: 242.2015\n",
      "Epoch [99/100], Loss: 241.6870\n",
      "Epoch [100/100], Loss: 238.2087\n",
      "Processed mouseintestine successfully.\n",
      "Epoch [1/100], Loss: 704.7739\n",
      "Epoch [2/100], Loss: 374.7646\n",
      "Epoch [3/100], Loss: 331.0333\n",
      "Epoch [4/100], Loss: 314.6029\n",
      "Epoch [5/100], Loss: 300.1172\n",
      "Epoch [6/100], Loss: 293.8915\n",
      "Epoch [7/100], Loss: 287.6770\n",
      "Epoch [8/100], Loss: 289.5110\n",
      "Epoch [9/100], Loss: 282.9238\n",
      "Epoch [10/100], Loss: 282.0819\n",
      "Epoch [11/100], Loss: 279.9111\n",
      "Epoch [12/100], Loss: 278.0017\n",
      "Epoch [13/100], Loss: 278.8583\n",
      "Epoch [14/100], Loss: 279.0701\n",
      "Epoch [15/100], Loss: 276.0922\n",
      "Epoch [16/100], Loss: 274.4973\n",
      "Epoch [17/100], Loss: 275.7569\n",
      "Epoch [18/100], Loss: 275.5011\n",
      "Epoch [19/100], Loss: 275.6597\n",
      "Epoch [20/100], Loss: 275.3059\n",
      "Epoch [21/100], Loss: 273.9960\n",
      "Epoch [22/100], Loss: 269.2013\n",
      "Epoch [23/100], Loss: 264.3460\n",
      "Epoch [24/100], Loss: 262.0509\n",
      "Epoch [25/100], Loss: 259.2855\n",
      "Epoch [26/100], Loss: 256.8588\n",
      "Epoch [27/100], Loss: 255.5224\n",
      "Epoch [28/100], Loss: 253.9333\n",
      "Epoch [29/100], Loss: 252.2485\n",
      "Epoch [30/100], Loss: 250.4887\n",
      "Epoch [31/100], Loss: 248.6978\n",
      "Epoch [32/100], Loss: 249.5936\n",
      "Epoch [33/100], Loss: 248.4850\n",
      "Epoch [34/100], Loss: 247.3280\n",
      "Epoch [35/100], Loss: 247.6268\n",
      "Epoch [36/100], Loss: 247.0290\n",
      "Epoch [37/100], Loss: 246.0287\n",
      "Epoch [38/100], Loss: 246.0406\n",
      "Epoch [39/100], Loss: 245.5805\n",
      "Epoch [40/100], Loss: 244.5890\n",
      "Epoch [41/100], Loss: 245.8369\n",
      "Epoch [42/100], Loss: 245.3242\n",
      "Epoch [43/100], Loss: 244.0720\n",
      "Epoch [44/100], Loss: 243.5574\n",
      "Epoch [45/100], Loss: 243.8608\n",
      "Epoch [46/100], Loss: 244.2711\n",
      "Epoch [47/100], Loss: 242.6841\n",
      "Epoch [48/100], Loss: 244.1758\n",
      "Epoch [49/100], Loss: 243.4411\n",
      "Epoch [50/100], Loss: 242.5877\n",
      "Epoch [51/100], Loss: 242.2266\n",
      "Epoch [52/100], Loss: 242.2803\n",
      "Epoch [53/100], Loss: 242.8276\n",
      "Epoch [54/100], Loss: 241.1921\n",
      "Epoch [55/100], Loss: 242.2982\n",
      "Epoch [56/100], Loss: 241.3770\n",
      "Epoch [57/100], Loss: 242.0855\n",
      "Epoch [58/100], Loss: 241.8603\n",
      "Epoch [59/100], Loss: 242.2246\n",
      "Epoch [60/100], Loss: 241.7475\n",
      "Epoch [61/100], Loss: 241.4206\n",
      "Epoch [62/100], Loss: 241.3676\n",
      "Epoch [63/100], Loss: 241.6300\n",
      "Epoch [64/100], Loss: 240.8768\n",
      "Epoch [65/100], Loss: 241.4704\n",
      "Epoch [66/100], Loss: 240.3390\n",
      "Epoch [67/100], Loss: 241.0411\n",
      "Epoch [68/100], Loss: 240.7394\n",
      "Epoch [69/100], Loss: 241.0052\n",
      "Epoch [70/100], Loss: 239.7714\n",
      "Epoch [71/100], Loss: 240.4260\n",
      "Epoch [72/100], Loss: 239.5937\n",
      "Epoch [73/100], Loss: 239.6418\n",
      "Epoch [74/100], Loss: 240.0951\n",
      "Epoch [75/100], Loss: 240.3183\n",
      "Epoch [76/100], Loss: 239.6368\n",
      "Epoch [77/100], Loss: 240.3037\n",
      "Epoch [78/100], Loss: 239.8356\n",
      "Epoch [79/100], Loss: 239.4488\n",
      "Epoch [80/100], Loss: 239.1627\n",
      "Epoch [81/100], Loss: 239.7908\n",
      "Epoch [82/100], Loss: 239.4416\n",
      "Epoch [83/100], Loss: 239.7386\n",
      "Epoch [84/100], Loss: 238.6560\n",
      "Epoch [85/100], Loss: 238.7170\n",
      "Epoch [86/100], Loss: 239.6369\n",
      "Epoch [87/100], Loss: 239.0373\n",
      "Epoch [88/100], Loss: 238.0376\n",
      "Epoch [89/100], Loss: 237.7907\n",
      "Epoch [90/100], Loss: 238.2923\n",
      "Epoch [91/100], Loss: 237.9039\n",
      "Epoch [92/100], Loss: 237.5200\n",
      "Epoch [93/100], Loss: 238.1540\n",
      "Epoch [94/100], Loss: 238.5235\n",
      "Epoch [95/100], Loss: 237.6941\n",
      "Epoch [96/100], Loss: 238.5190\n",
      "Epoch [97/100], Loss: 238.1389\n",
      "Epoch [98/100], Loss: 237.6619\n",
      "Epoch [99/100], Loss: 237.6918\n",
      "Epoch [100/100], Loss: 237.7505\n",
      "Processed mousecolon successfully.\n",
      "Epoch [1/100], Loss: 799.1113\n",
      "Epoch [2/100], Loss: 468.5891\n",
      "Epoch [3/100], Loss: 414.0078\n",
      "Epoch [4/100], Loss: 389.6311\n",
      "Epoch [5/100], Loss: 373.3926\n",
      "Epoch [6/100], Loss: 363.1007\n",
      "Epoch [7/100], Loss: 354.7934\n",
      "Epoch [8/100], Loss: 349.1053\n",
      "Epoch [9/100], Loss: 346.8312\n",
      "Epoch [10/100], Loss: 343.8215\n",
      "Epoch [11/100], Loss: 340.7799\n",
      "Epoch [12/100], Loss: 339.7615\n",
      "Epoch [13/100], Loss: 338.2374\n",
      "Epoch [14/100], Loss: 337.7984\n",
      "Epoch [15/100], Loss: 336.4117\n",
      "Epoch [16/100], Loss: 334.7136\n",
      "Epoch [17/100], Loss: 334.0368\n",
      "Epoch [18/100], Loss: 332.6293\n",
      "Epoch [19/100], Loss: 333.5812\n",
      "Epoch [20/100], Loss: 332.4571\n",
      "Epoch [21/100], Loss: 332.1921\n",
      "Epoch [22/100], Loss: 328.9318\n",
      "Epoch [23/100], Loss: 329.8342\n",
      "Epoch [24/100], Loss: 330.3072\n",
      "Epoch [25/100], Loss: 330.1432\n",
      "Epoch [26/100], Loss: 332.6184\n",
      "Epoch [27/100], Loss: 329.4978\n",
      "Epoch [28/100], Loss: 330.7337\n",
      "Epoch [29/100], Loss: 328.9850\n",
      "Epoch [30/100], Loss: 323.6445\n",
      "Epoch [31/100], Loss: 320.8575\n",
      "Epoch [32/100], Loss: 316.0882\n",
      "Epoch [33/100], Loss: 315.2601\n",
      "Epoch [34/100], Loss: 315.4369\n",
      "Epoch [35/100], Loss: 311.2084\n",
      "Epoch [36/100], Loss: 312.2313\n",
      "Epoch [37/100], Loss: 310.3820\n",
      "Epoch [38/100], Loss: 308.7008\n",
      "Epoch [39/100], Loss: 307.0623\n",
      "Epoch [40/100], Loss: 307.6905\n",
      "Epoch [41/100], Loss: 307.0935\n",
      "Epoch [42/100], Loss: 306.9462\n",
      "Epoch [43/100], Loss: 307.3339\n",
      "Epoch [44/100], Loss: 305.9283\n",
      "Epoch [45/100], Loss: 305.0808\n",
      "Epoch [46/100], Loss: 304.5249\n",
      "Epoch [47/100], Loss: 305.0648\n",
      "Epoch [48/100], Loss: 304.3720\n",
      "Epoch [49/100], Loss: 304.5319\n",
      "Epoch [50/100], Loss: 303.3360\n",
      "Epoch [51/100], Loss: 302.9463\n",
      "Epoch [52/100], Loss: 303.6200\n",
      "Epoch [53/100], Loss: 303.1113\n",
      "Epoch [54/100], Loss: 302.6125\n",
      "Epoch [55/100], Loss: 302.3402\n",
      "Epoch [56/100], Loss: 302.0802\n",
      "Epoch [57/100], Loss: 301.4618\n",
      "Epoch [58/100], Loss: 303.0329\n",
      "Epoch [59/100], Loss: 302.5392\n",
      "Epoch [60/100], Loss: 302.1733\n",
      "Epoch [61/100], Loss: 302.2791\n",
      "Epoch [62/100], Loss: 301.5951\n",
      "Epoch [63/100], Loss: 299.9712\n",
      "Epoch [64/100], Loss: 301.9309\n",
      "Epoch [65/100], Loss: 300.8262\n",
      "Epoch [66/100], Loss: 300.5310\n",
      "Epoch [67/100], Loss: 299.6099\n",
      "Epoch [68/100], Loss: 300.7697\n",
      "Epoch [69/100], Loss: 300.6436\n",
      "Epoch [70/100], Loss: 300.8319\n",
      "Epoch [71/100], Loss: 299.7262\n",
      "Epoch [72/100], Loss: 300.0374\n",
      "Epoch [73/100], Loss: 301.0885\n",
      "Epoch [74/100], Loss: 300.3671\n",
      "Epoch [75/100], Loss: 300.7009\n",
      "Epoch [76/100], Loss: 299.1898\n",
      "Epoch [77/100], Loss: 299.2502\n",
      "Epoch [78/100], Loss: 299.7983\n",
      "Epoch [79/100], Loss: 300.5607\n",
      "Epoch [80/100], Loss: 299.7020\n",
      "Epoch [81/100], Loss: 299.0820\n",
      "Epoch [82/100], Loss: 300.2434\n",
      "Epoch [83/100], Loss: 298.9390\n",
      "Epoch [84/100], Loss: 299.4841\n",
      "Epoch [85/100], Loss: 298.4199\n",
      "Epoch [86/100], Loss: 299.3800\n",
      "Epoch [87/100], Loss: 299.2063\n",
      "Epoch [88/100], Loss: 298.1822\n",
      "Epoch [89/100], Loss: 300.2704\n",
      "Epoch [90/100], Loss: 298.4065\n",
      "Epoch [91/100], Loss: 298.4777\n",
      "Epoch [92/100], Loss: 298.1214\n",
      "Epoch [93/100], Loss: 298.3163\n",
      "Epoch [94/100], Loss: 298.1756\n",
      "Epoch [95/100], Loss: 298.2123\n",
      "Epoch [96/100], Loss: 297.5623\n",
      "Epoch [97/100], Loss: 298.1644\n",
      "Epoch [98/100], Loss: 298.6194\n",
      "Epoch [99/100], Loss: 297.2304\n",
      "Epoch [100/100], Loss: 297.5357\n",
      "Processed mousespleen successfully.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tissues= ['humanGBM', 'humanskin', 'humanthymus', 'humanspleen', 'humantonsil', 'mousekidney', 'mouseintestine', 'mousecolon', 'mousespleen']\n",
    "\n",
    "for tissue in tissues:\n",
    "    rna_data = None\n",
    "    protein_data = None\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        if tissue in filename and filename.endswith(\"RNA.tsv.gz\"):\n",
    "            rna_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        elif tissue in filename and filename.endswith(\"protein.tsv.gz\"):\n",
    "            protein_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "            \n",
    "    rna_data.columns = rna_data.columns.astype(str)\n",
    "    protein_data.columns = protein_data.columns.astype(str)\n",
    "            \n",
    "    rna_data = rna_data.sort_values(by='X')\n",
    "    protein_data = protein_data.sort_values(by='X')\n",
    "    \n",
    "    rna_data = rna_data.reset_index(drop=True)\n",
    "    protein_data = protein_data.reset_index(drop=True)\n",
    "    rna_data.index = rna_data.index.astype(str)\n",
    "    protein_data.index = protein_data.index.astype(str)\n",
    "  \n",
    "    rna_data.drop(['X'], axis=1, inplace=True)\n",
    "    protein_data.drop(['X'], axis=1, inplace=True)\n",
    "    \n",
    "    rna_train, rna_test = train_test_split(rna_data, test_size=0.2, random_state=42)\n",
    "    protein_train = protein_data.loc[rna_train.index]\n",
    "    protein_test = protein_data.loc[rna_test.index]\n",
    "        \n",
    "    adata_rna_train = sc.AnnData(rna_train)\n",
    "    sc.pp.normalize_total(adata_rna_train, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_rna_train)\n",
    "    sc.pp.highly_variable_genes(adata_rna_train, n_top_genes=4000, flavor='seurat', subset=True)\n",
    "    counts_norm = adata_rna_train.X\n",
    "    rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "  \n",
    "    adata_protein_train = sc.AnnData(protein_train) \n",
    "    sc.pp.normalize_total(adata_protein_train, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_protein_train)\n",
    "    counts_norm = adata_protein_train.X\n",
    "    protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "    \n",
    "    input_dim = rna_counts_norm.shape[1]\n",
    "    output_dim = protein_counts_norm.shape[1]\n",
    "    latent_dim = 32\n",
    "    model = VAE2(input_dim, latent_dim, output_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 100\n",
    "    \n",
    "    batch_size = 64\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        permutation = torch.randperm(rna_counts_norm.size(0))\n",
    "\n",
    "        for i in range(0, rna_counts_norm.size(0), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "\n",
    "            reconstructed_data, mean, logvar = model(rna_counts_norm[indices])\n",
    "            loss = vae_loss(reconstructed_data, protein_counts_norm[indices], mean, logvar, lambda_kl=0.0001)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(rna_counts_norm):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_protein_counts, mean, logvar = model(rna_counts_norm)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n",
    "        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n",
    "        avg_corr_pearson = pcc.mean()\n",
    "        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'RMSE': [rmse],\n",
    "            'Pearson Correlation': [avg_corr_pearson],\n",
    "            'SSIM':ssim_val\n",
    "        })\n",
    "        \n",
    "        results_file_path = os.path.join(results_dir, f\"{tissue}_training_results.csv\")\n",
    "        results_df.to_csv(results_file_path, index=False)\n",
    "        \n",
    "        adata_rna_test = sc.AnnData(rna_test) \n",
    "        sc.pp.normalize_total(adata_rna_test, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_rna_test)\n",
    "        counts_norm = adata_rna_test[:,  adata_rna_train.var_names].X\n",
    "        rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "    \n",
    "        adata_protein_test = sc.AnnData(protein_test) \n",
    "        sc.pp.normalize_total(adata_protein_test, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_protein_test)\n",
    "        counts_norm = adata_protein_test.X\n",
    "        protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "                \n",
    "        reconstructed_protein_counts, mean, logvar = model(rna_counts_norm)\n",
    "          \n",
    "        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n",
    "        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n",
    "        avg_corr_pearson = pcc.mean()\n",
    "        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'RMSE': [rmse],\n",
    "            'Pearson Correlation': [avg_corr_pearson],\n",
    "            'SSIM':ssim_val\n",
    "        })\n",
    "    \n",
    "    results_file_path = os.path.join(results_dir, f\"{tissue}_results.csv\")\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "    print(f\"Processed {tissue} successfully.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
