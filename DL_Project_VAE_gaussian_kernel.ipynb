{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 18:15:15.362717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'spatial_datasets/GSE213264_RAW/'\n",
    "results_dir = 'results_gaussian_kernel/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Loss: 3797.9838\n",
      "Epoch [2/100], Loss: 2003.4914\n",
      "Epoch [3/100], Loss: 1349.2242\n",
      "Epoch [4/100], Loss: 1121.2832\n",
      "Epoch [5/100], Loss: 1017.8816\n",
      "Epoch [6/100], Loss: 968.3776\n",
      "Epoch [7/100], Loss: 937.1915\n",
      "Epoch [8/100], Loss: 916.0068\n",
      "Epoch [9/100], Loss: 904.1262\n",
      "Epoch [10/100], Loss: 895.5392\n",
      "Epoch [11/100], Loss: 890.8970\n",
      "Epoch [12/100], Loss: 883.9315\n",
      "Epoch [13/100], Loss: 880.5289\n",
      "Epoch [14/100], Loss: 876.2786\n",
      "Epoch [15/100], Loss: 874.0564\n",
      "Epoch [16/100], Loss: 872.8640\n",
      "Epoch [17/100], Loss: 870.2869\n",
      "Epoch [18/100], Loss: 870.8903\n",
      "Epoch [19/100], Loss: 867.1765\n",
      "Epoch [20/100], Loss: 866.9323\n",
      "Epoch [21/100], Loss: 865.2348\n",
      "Epoch [22/100], Loss: 865.2389\n",
      "Epoch [23/100], Loss: 864.8189\n",
      "Epoch [24/100], Loss: 863.8785\n",
      "Epoch [25/100], Loss: 862.1352\n",
      "Epoch [26/100], Loss: 861.4419\n",
      "Epoch [27/100], Loss: 861.1279\n",
      "Epoch [28/100], Loss: 859.9118\n",
      "Epoch [29/100], Loss: 858.9208\n",
      "Epoch [30/100], Loss: 859.2019\n",
      "Epoch [31/100], Loss: 857.7293\n",
      "Epoch [32/100], Loss: 859.2215\n",
      "Epoch [33/100], Loss: 857.3909\n",
      "Epoch [34/100], Loss: 858.2811\n",
      "Epoch [35/100], Loss: 856.7886\n",
      "Epoch [36/100], Loss: 854.6823\n",
      "Epoch [37/100], Loss: 856.3208\n",
      "Epoch [38/100], Loss: 853.9240\n",
      "Epoch [39/100], Loss: 853.9592\n",
      "Epoch [40/100], Loss: 852.8031\n",
      "Epoch [41/100], Loss: 853.8188\n",
      "Epoch [42/100], Loss: 852.2835\n",
      "Epoch [43/100], Loss: 853.9157\n",
      "Epoch [44/100], Loss: 851.8677\n",
      "Epoch [45/100], Loss: 851.4575\n",
      "Epoch [46/100], Loss: 851.5411\n",
      "Epoch [47/100], Loss: 852.9128\n",
      "Epoch [48/100], Loss: 851.7845\n",
      "Epoch [49/100], Loss: 851.1228\n",
      "Epoch [50/100], Loss: 851.1590\n",
      "Epoch [51/100], Loss: 848.6752\n",
      "Epoch [52/100], Loss: 851.2011\n",
      "Epoch [53/100], Loss: 849.3203\n",
      "Epoch [54/100], Loss: 849.8123\n",
      "Epoch [55/100], Loss: 850.8375\n",
      "Epoch [56/100], Loss: 849.3014\n",
      "Epoch [57/100], Loss: 848.7962\n",
      "Epoch [58/100], Loss: 848.9396\n",
      "Epoch [59/100], Loss: 848.8716\n",
      "Epoch [60/100], Loss: 848.6534\n",
      "Epoch [61/100], Loss: 848.4241\n",
      "Epoch [62/100], Loss: 850.3552\n",
      "Epoch [63/100], Loss: 847.4036\n",
      "Epoch [64/100], Loss: 846.3023\n",
      "Epoch [65/100], Loss: 847.0558\n",
      "Epoch [66/100], Loss: 847.6010\n",
      "Epoch [67/100], Loss: 849.2731\n",
      "Epoch [68/100], Loss: 847.7992\n",
      "Epoch [69/100], Loss: 846.8846\n",
      "Epoch [70/100], Loss: 846.8804\n",
      "Epoch [71/100], Loss: 847.5970\n",
      "Epoch [72/100], Loss: 845.5346\n",
      "Epoch [73/100], Loss: 844.2321\n",
      "Epoch [74/100], Loss: 842.2967\n",
      "Epoch [75/100], Loss: 840.6306\n",
      "Epoch [76/100], Loss: 838.8197\n",
      "Epoch [77/100], Loss: 837.2697\n",
      "Epoch [78/100], Loss: 836.5619\n",
      "Epoch [79/100], Loss: 835.4174\n",
      "Epoch [80/100], Loss: 832.6237\n",
      "Epoch [81/100], Loss: 834.1281\n",
      "Epoch [82/100], Loss: 832.8148\n",
      "Epoch [83/100], Loss: 832.6506\n",
      "Epoch [84/100], Loss: 832.0500\n",
      "Epoch [85/100], Loss: 830.3221\n",
      "Epoch [86/100], Loss: 830.6521\n",
      "Epoch [87/100], Loss: 830.6539\n",
      "Epoch [88/100], Loss: 830.0610\n",
      "Epoch [89/100], Loss: 828.9220\n",
      "Epoch [90/100], Loss: 828.2586\n",
      "Epoch [91/100], Loss: 829.6999\n",
      "Epoch [92/100], Loss: 828.4916\n",
      "Epoch [93/100], Loss: 826.6265\n",
      "Epoch [94/100], Loss: 826.2852\n",
      "Epoch [95/100], Loss: 827.1300\n",
      "Epoch [96/100], Loss: 826.1203\n",
      "Epoch [97/100], Loss: 826.8489\n",
      "Epoch [98/100], Loss: 826.1178\n",
      "Epoch [99/100], Loss: 825.7965\n",
      "Epoch [100/100], Loss: 825.2716\n",
      "Processed humanGBM successfully.\n",
      "Epoch [1/100], Loss: 3841.8332\n",
      "Epoch [2/100], Loss: 2143.6793\n",
      "Epoch [3/100], Loss: 1461.7468\n",
      "Epoch [4/100], Loss: 1221.7515\n",
      "Epoch [5/100], Loss: 1120.4615\n",
      "Epoch [6/100], Loss: 1065.6648\n",
      "Epoch [7/100], Loss: 1037.7694\n",
      "Epoch [8/100], Loss: 1018.8282\n",
      "Epoch [9/100], Loss: 1005.4756\n",
      "Epoch [10/100], Loss: 997.1925\n",
      "Epoch [11/100], Loss: 990.1886\n",
      "Epoch [12/100], Loss: 985.1360\n",
      "Epoch [13/100], Loss: 981.7756\n",
      "Epoch [14/100], Loss: 979.2380\n",
      "Epoch [15/100], Loss: 976.1763\n",
      "Epoch [16/100], Loss: 976.0597\n",
      "Epoch [17/100], Loss: 974.4199\n",
      "Epoch [18/100], Loss: 971.4337\n",
      "Epoch [19/100], Loss: 968.5989\n",
      "Epoch [20/100], Loss: 970.4167\n",
      "Epoch [21/100], Loss: 968.1630\n",
      "Epoch [22/100], Loss: 965.8265\n",
      "Epoch [23/100], Loss: 968.7972\n",
      "Epoch [24/100], Loss: 965.5813\n",
      "Epoch [25/100], Loss: 966.0237\n",
      "Epoch [26/100], Loss: 961.2568\n",
      "Epoch [27/100], Loss: 961.3609\n",
      "Epoch [28/100], Loss: 953.5203\n",
      "Epoch [29/100], Loss: 936.7136\n",
      "Epoch [30/100], Loss: 925.0382\n",
      "Epoch [31/100], Loss: 924.0586\n",
      "Epoch [32/100], Loss: 918.6830\n",
      "Epoch [33/100], Loss: 915.9188\n",
      "Epoch [34/100], Loss: 916.2900\n",
      "Epoch [35/100], Loss: 913.9322\n",
      "Epoch [36/100], Loss: 911.6736\n",
      "Epoch [37/100], Loss: 910.8760\n",
      "Epoch [38/100], Loss: 907.7562\n",
      "Epoch [39/100], Loss: 906.7106\n",
      "Epoch [40/100], Loss: 906.3897\n",
      "Epoch [41/100], Loss: 905.9825\n",
      "Epoch [42/100], Loss: 904.1433\n",
      "Epoch [43/100], Loss: 903.2475\n",
      "Epoch [44/100], Loss: 901.8553\n",
      "Epoch [45/100], Loss: 901.3162\n",
      "Epoch [46/100], Loss: 900.6182\n",
      "Epoch [47/100], Loss: 899.3576\n",
      "Epoch [48/100], Loss: 897.4563\n",
      "Epoch [49/100], Loss: 892.3801\n",
      "Epoch [50/100], Loss: 893.0280\n",
      "Epoch [51/100], Loss: 892.8717\n",
      "Epoch [52/100], Loss: 890.6280\n",
      "Epoch [53/100], Loss: 890.0013\n",
      "Epoch [54/100], Loss: 888.7161\n",
      "Epoch [55/100], Loss: 887.6208\n",
      "Epoch [56/100], Loss: 888.4370\n",
      "Epoch [57/100], Loss: 885.6855\n",
      "Epoch [58/100], Loss: 883.9468\n",
      "Epoch [59/100], Loss: 882.4263\n",
      "Epoch [60/100], Loss: 883.4710\n",
      "Epoch [61/100], Loss: 882.9450\n",
      "Epoch [62/100], Loss: 882.4107\n",
      "Epoch [63/100], Loss: 881.0642\n",
      "Epoch [64/100], Loss: 880.3156\n",
      "Epoch [65/100], Loss: 879.6563\n",
      "Epoch [66/100], Loss: 878.1316\n",
      "Epoch [67/100], Loss: 878.1623\n",
      "Epoch [68/100], Loss: 876.9670\n",
      "Epoch [69/100], Loss: 877.0934\n",
      "Epoch [70/100], Loss: 876.9982\n",
      "Epoch [71/100], Loss: 876.0246\n",
      "Epoch [72/100], Loss: 876.8654\n",
      "Epoch [73/100], Loss: 875.5149\n",
      "Epoch [74/100], Loss: 874.6202\n",
      "Epoch [75/100], Loss: 876.0747\n",
      "Epoch [76/100], Loss: 874.4976\n",
      "Epoch [77/100], Loss: 874.1045\n",
      "Epoch [78/100], Loss: 874.1823\n",
      "Epoch [79/100], Loss: 872.8666\n",
      "Epoch [80/100], Loss: 872.6343\n",
      "Epoch [81/100], Loss: 872.1246\n",
      "Epoch [82/100], Loss: 871.7652\n",
      "Epoch [83/100], Loss: 872.3148\n",
      "Epoch [84/100], Loss: 870.4707\n",
      "Epoch [85/100], Loss: 872.7103\n",
      "Epoch [86/100], Loss: 870.7075\n",
      "Epoch [87/100], Loss: 870.1942\n",
      "Epoch [88/100], Loss: 872.4139\n",
      "Epoch [89/100], Loss: 871.7947\n",
      "Epoch [90/100], Loss: 870.7421\n",
      "Epoch [91/100], Loss: 869.9901\n",
      "Epoch [92/100], Loss: 870.0700\n",
      "Epoch [93/100], Loss: 868.1786\n",
      "Epoch [94/100], Loss: 868.5839\n",
      "Epoch [95/100], Loss: 868.5478\n",
      "Epoch [96/100], Loss: 869.7205\n",
      "Epoch [97/100], Loss: 869.1150\n",
      "Epoch [98/100], Loss: 868.6672\n",
      "Epoch [99/100], Loss: 868.3352\n",
      "Epoch [100/100], Loss: 866.4686\n",
      "Processed humanskin successfully.\n",
      "Epoch [1/100], Loss: 3245.3703\n",
      "Epoch [2/100], Loss: 1324.4086\n",
      "Epoch [3/100], Loss: 900.7414\n",
      "Epoch [4/100], Loss: 779.4358\n",
      "Epoch [5/100], Loss: 724.9524\n",
      "Epoch [6/100], Loss: 698.5285\n",
      "Epoch [7/100], Loss: 683.1709\n",
      "Epoch [8/100], Loss: 676.1565\n",
      "Epoch [9/100], Loss: 670.0903\n",
      "Epoch [10/100], Loss: 665.0595\n",
      "Epoch [11/100], Loss: 661.8815\n",
      "Epoch [12/100], Loss: 657.7948\n",
      "Epoch [13/100], Loss: 655.9395\n",
      "Epoch [14/100], Loss: 655.1297\n",
      "Epoch [15/100], Loss: 653.4081\n",
      "Epoch [16/100], Loss: 652.1277\n",
      "Epoch [17/100], Loss: 651.5772\n",
      "Epoch [18/100], Loss: 649.6900\n",
      "Epoch [19/100], Loss: 649.4212\n",
      "Epoch [20/100], Loss: 648.5517\n",
      "Epoch [21/100], Loss: 647.7255\n",
      "Epoch [22/100], Loss: 646.7169\n",
      "Epoch [23/100], Loss: 644.5716\n",
      "Epoch [24/100], Loss: 645.6807\n",
      "Epoch [25/100], Loss: 644.9319\n",
      "Epoch [26/100], Loss: 645.7078\n",
      "Epoch [27/100], Loss: 643.3193\n",
      "Epoch [28/100], Loss: 642.8429\n",
      "Epoch [29/100], Loss: 642.7826\n",
      "Epoch [30/100], Loss: 642.6354\n",
      "Epoch [31/100], Loss: 641.5728\n",
      "Epoch [32/100], Loss: 641.4092\n",
      "Epoch [33/100], Loss: 640.4705\n",
      "Epoch [34/100], Loss: 640.0137\n",
      "Epoch [35/100], Loss: 640.6363\n",
      "Epoch [36/100], Loss: 640.2728\n",
      "Epoch [37/100], Loss: 638.2531\n",
      "Epoch [38/100], Loss: 639.3149\n",
      "Epoch [39/100], Loss: 640.0671\n",
      "Epoch [40/100], Loss: 638.1330\n",
      "Epoch [41/100], Loss: 637.4546\n",
      "Epoch [42/100], Loss: 637.9339\n",
      "Epoch [43/100], Loss: 637.0941\n",
      "Epoch [44/100], Loss: 637.1665\n",
      "Epoch [45/100], Loss: 636.0120\n",
      "Epoch [46/100], Loss: 636.1652\n",
      "Epoch [47/100], Loss: 636.3726\n",
      "Epoch [48/100], Loss: 635.6238\n",
      "Epoch [49/100], Loss: 635.6907\n",
      "Epoch [50/100], Loss: 635.8246\n",
      "Epoch [51/100], Loss: 634.3117\n",
      "Epoch [52/100], Loss: 634.6228\n",
      "Epoch [53/100], Loss: 634.7512\n",
      "Epoch [54/100], Loss: 634.4067\n",
      "Epoch [55/100], Loss: 633.7233\n",
      "Epoch [56/100], Loss: 632.3631\n",
      "Epoch [57/100], Loss: 633.3517\n",
      "Epoch [58/100], Loss: 632.1344\n",
      "Epoch [59/100], Loss: 632.5504\n",
      "Epoch [60/100], Loss: 633.6231\n",
      "Epoch [61/100], Loss: 632.2381\n",
      "Epoch [62/100], Loss: 631.8586\n",
      "Epoch [63/100], Loss: 631.6683\n",
      "Epoch [64/100], Loss: 631.9575\n",
      "Epoch [65/100], Loss: 631.8461\n",
      "Epoch [66/100], Loss: 631.0852\n",
      "Epoch [67/100], Loss: 631.7160\n",
      "Epoch [68/100], Loss: 630.9823\n",
      "Epoch [69/100], Loss: 633.1112\n",
      "Epoch [70/100], Loss: 630.1693\n",
      "Epoch [71/100], Loss: 631.0266\n",
      "Epoch [72/100], Loss: 630.5282\n",
      "Epoch [73/100], Loss: 631.0048\n",
      "Epoch [74/100], Loss: 629.9538\n",
      "Epoch [75/100], Loss: 630.0804\n",
      "Epoch [76/100], Loss: 629.7602\n",
      "Epoch [77/100], Loss: 629.8558\n",
      "Epoch [78/100], Loss: 630.0169\n",
      "Epoch [79/100], Loss: 629.1512\n",
      "Epoch [80/100], Loss: 629.5612\n",
      "Epoch [81/100], Loss: 628.6950\n",
      "Epoch [82/100], Loss: 628.4394\n",
      "Epoch [83/100], Loss: 628.5620\n",
      "Epoch [84/100], Loss: 628.1284\n",
      "Epoch [85/100], Loss: 628.5678\n",
      "Epoch [86/100], Loss: 628.0833\n",
      "Epoch [87/100], Loss: 627.3743\n",
      "Epoch [88/100], Loss: 628.9615\n",
      "Epoch [89/100], Loss: 628.3242\n",
      "Epoch [90/100], Loss: 628.3893\n",
      "Epoch [91/100], Loss: 627.4052\n",
      "Epoch [92/100], Loss: 628.1347\n",
      "Epoch [93/100], Loss: 626.8522\n",
      "Epoch [94/100], Loss: 626.7311\n",
      "Epoch [95/100], Loss: 626.4418\n",
      "Epoch [96/100], Loss: 627.8911\n",
      "Epoch [97/100], Loss: 626.1959\n",
      "Epoch [98/100], Loss: 626.8130\n",
      "Epoch [99/100], Loss: 626.0984\n",
      "Epoch [100/100], Loss: 626.1340\n",
      "Processed humanthymus successfully.\n",
      "Epoch [1/100], Loss: 3201.8023\n",
      "Epoch [2/100], Loss: 1354.1374\n",
      "Epoch [3/100], Loss: 922.1415\n",
      "Epoch [4/100], Loss: 802.3362\n",
      "Epoch [5/100], Loss: 754.5503\n",
      "Epoch [6/100], Loss: 729.7050\n",
      "Epoch [7/100], Loss: 716.1409\n",
      "Epoch [8/100], Loss: 707.9239\n",
      "Epoch [9/100], Loss: 702.8574\n",
      "Epoch [10/100], Loss: 700.1502\n",
      "Epoch [11/100], Loss: 697.1497\n",
      "Epoch [12/100], Loss: 695.0874\n",
      "Epoch [13/100], Loss: 693.1888\n",
      "Epoch [14/100], Loss: 692.5991\n",
      "Epoch [15/100], Loss: 690.5056\n",
      "Epoch [16/100], Loss: 688.1768\n",
      "Epoch [17/100], Loss: 687.1391\n",
      "Epoch [18/100], Loss: 687.4215\n",
      "Epoch [19/100], Loss: 685.4405\n",
      "Epoch [20/100], Loss: 685.8051\n",
      "Epoch [21/100], Loss: 684.6554\n",
      "Epoch [22/100], Loss: 683.9191\n",
      "Epoch [23/100], Loss: 683.2617\n",
      "Epoch [24/100], Loss: 684.3814\n",
      "Epoch [25/100], Loss: 681.4513\n",
      "Epoch [26/100], Loss: 681.7922\n",
      "Epoch [27/100], Loss: 681.8284\n",
      "Epoch [28/100], Loss: 680.1282\n",
      "Epoch [29/100], Loss: 678.9882\n",
      "Epoch [30/100], Loss: 678.5351\n",
      "Epoch [31/100], Loss: 680.7139\n",
      "Epoch [32/100], Loss: 678.3251\n",
      "Epoch [33/100], Loss: 678.4288\n",
      "Epoch [34/100], Loss: 678.3092\n",
      "Epoch [35/100], Loss: 677.0762\n",
      "Epoch [36/100], Loss: 677.1196\n",
      "Epoch [37/100], Loss: 676.2083\n",
      "Epoch [38/100], Loss: 677.4540\n",
      "Epoch [39/100], Loss: 676.3239\n",
      "Epoch [40/100], Loss: 676.1034\n",
      "Epoch [41/100], Loss: 674.8843\n",
      "Epoch [42/100], Loss: 674.5105\n",
      "Epoch [43/100], Loss: 675.4293\n",
      "Epoch [44/100], Loss: 675.6527\n",
      "Epoch [45/100], Loss: 675.7272\n",
      "Epoch [46/100], Loss: 674.9425\n",
      "Epoch [47/100], Loss: 673.3675\n",
      "Epoch [48/100], Loss: 674.0089\n",
      "Epoch [49/100], Loss: 673.7378\n",
      "Epoch [50/100], Loss: 672.6365\n",
      "Epoch [51/100], Loss: 672.6914\n",
      "Epoch [52/100], Loss: 673.1701\n",
      "Epoch [53/100], Loss: 671.7079\n",
      "Epoch [54/100], Loss: 668.0035\n",
      "Epoch [55/100], Loss: 668.0993\n",
      "Epoch [56/100], Loss: 665.1831\n",
      "Epoch [57/100], Loss: 663.2631\n",
      "Epoch [58/100], Loss: 662.9351\n",
      "Epoch [59/100], Loss: 661.8657\n",
      "Epoch [60/100], Loss: 660.6618\n",
      "Epoch [61/100], Loss: 660.2414\n",
      "Epoch [62/100], Loss: 659.9325\n",
      "Epoch [63/100], Loss: 659.8808\n",
      "Epoch [64/100], Loss: 658.9229\n",
      "Epoch [65/100], Loss: 658.6617\n",
      "Epoch [66/100], Loss: 658.8087\n",
      "Epoch [67/100], Loss: 658.1570\n",
      "Epoch [68/100], Loss: 657.6744\n",
      "Epoch [69/100], Loss: 656.7114\n",
      "Epoch [70/100], Loss: 656.6911\n",
      "Epoch [71/100], Loss: 656.4820\n",
      "Epoch [72/100], Loss: 656.4928\n",
      "Epoch [73/100], Loss: 656.0957\n",
      "Epoch [74/100], Loss: 656.2013\n",
      "Epoch [75/100], Loss: 655.6250\n",
      "Epoch [76/100], Loss: 654.6528\n",
      "Epoch [77/100], Loss: 655.2040\n",
      "Epoch [78/100], Loss: 654.6683\n",
      "Epoch [79/100], Loss: 654.5017\n",
      "Epoch [80/100], Loss: 654.5746\n",
      "Epoch [81/100], Loss: 653.7747\n",
      "Epoch [82/100], Loss: 653.7358\n",
      "Epoch [83/100], Loss: 653.7930\n",
      "Epoch [84/100], Loss: 652.7596\n",
      "Epoch [85/100], Loss: 654.1761\n",
      "Epoch [86/100], Loss: 653.6152\n",
      "Epoch [87/100], Loss: 653.9746\n",
      "Epoch [88/100], Loss: 653.6955\n",
      "Epoch [89/100], Loss: 653.2980\n",
      "Epoch [90/100], Loss: 652.5310\n",
      "Epoch [91/100], Loss: 652.7115\n",
      "Epoch [92/100], Loss: 651.5455\n",
      "Epoch [93/100], Loss: 653.1925\n",
      "Epoch [94/100], Loss: 652.8627\n",
      "Epoch [95/100], Loss: 653.2057\n",
      "Epoch [96/100], Loss: 650.9112\n",
      "Epoch [97/100], Loss: 651.9190\n",
      "Epoch [98/100], Loss: 651.5544\n",
      "Epoch [99/100], Loss: 652.0144\n",
      "Epoch [100/100], Loss: 651.0362\n",
      "Processed humanspleen successfully.\n",
      "Epoch [1/100], Loss: 3070.3120\n",
      "Epoch [2/100], Loss: 1194.6431\n",
      "Epoch [3/100], Loss: 784.4004\n",
      "Epoch [4/100], Loss: 665.5952\n",
      "Epoch [5/100], Loss: 614.0362\n",
      "Epoch [6/100], Loss: 588.6017\n",
      "Epoch [7/100], Loss: 573.9200\n",
      "Epoch [8/100], Loss: 565.2638\n",
      "Epoch [9/100], Loss: 556.9841\n",
      "Epoch [10/100], Loss: 553.5052\n",
      "Epoch [11/100], Loss: 551.3181\n",
      "Epoch [12/100], Loss: 548.8945\n",
      "Epoch [13/100], Loss: 547.0581\n",
      "Epoch [14/100], Loss: 546.0743\n",
      "Epoch [15/100], Loss: 543.4914\n",
      "Epoch [16/100], Loss: 543.2831\n",
      "Epoch [17/100], Loss: 543.2270\n",
      "Epoch [18/100], Loss: 541.4022\n",
      "Epoch [19/100], Loss: 539.5100\n",
      "Epoch [20/100], Loss: 539.7836\n",
      "Epoch [21/100], Loss: 538.4778\n",
      "Epoch [22/100], Loss: 537.8059\n",
      "Epoch [23/100], Loss: 537.9763\n",
      "Epoch [24/100], Loss: 538.1666\n",
      "Epoch [25/100], Loss: 536.5053\n",
      "Epoch [26/100], Loss: 536.2347\n",
      "Epoch [27/100], Loss: 536.4855\n",
      "Epoch [28/100], Loss: 535.2564\n",
      "Epoch [29/100], Loss: 535.0335\n",
      "Epoch [30/100], Loss: 535.4009\n",
      "Epoch [31/100], Loss: 534.3862\n",
      "Epoch [32/100], Loss: 534.2283\n",
      "Epoch [33/100], Loss: 533.6132\n",
      "Epoch [34/100], Loss: 534.1050\n",
      "Epoch [35/100], Loss: 532.9583\n",
      "Epoch [36/100], Loss: 532.1971\n",
      "Epoch [37/100], Loss: 531.2697\n",
      "Epoch [38/100], Loss: 530.9721\n",
      "Epoch [39/100], Loss: 530.3104\n",
      "Epoch [40/100], Loss: 530.0650\n",
      "Epoch [41/100], Loss: 531.7247\n",
      "Epoch [42/100], Loss: 531.0505\n",
      "Epoch [43/100], Loss: 530.3357\n",
      "Epoch [44/100], Loss: 528.8042\n",
      "Epoch [45/100], Loss: 528.1994\n",
      "Epoch [46/100], Loss: 528.5104\n",
      "Epoch [47/100], Loss: 528.8123\n",
      "Epoch [48/100], Loss: 527.6301\n",
      "Epoch [49/100], Loss: 529.1343\n",
      "Epoch [50/100], Loss: 528.1182\n",
      "Epoch [51/100], Loss: 526.8690\n",
      "Epoch [52/100], Loss: 527.2997\n",
      "Epoch [53/100], Loss: 526.6257\n",
      "Epoch [54/100], Loss: 527.0694\n",
      "Epoch [55/100], Loss: 525.6666\n",
      "Epoch [56/100], Loss: 525.0298\n",
      "Epoch [57/100], Loss: 526.1473\n",
      "Epoch [58/100], Loss: 527.0314\n",
      "Epoch [59/100], Loss: 526.3534\n",
      "Epoch [60/100], Loss: 525.4256\n",
      "Epoch [61/100], Loss: 526.1589\n",
      "Epoch [62/100], Loss: 525.1359\n",
      "Epoch [63/100], Loss: 524.9983\n",
      "Epoch [64/100], Loss: 524.9743\n",
      "Epoch [65/100], Loss: 524.9363\n",
      "Epoch [66/100], Loss: 524.6883\n",
      "Epoch [67/100], Loss: 524.4806\n",
      "Epoch [68/100], Loss: 524.6319\n",
      "Epoch [69/100], Loss: 523.8179\n",
      "Epoch [70/100], Loss: 524.1193\n",
      "Epoch [71/100], Loss: 524.0877\n",
      "Epoch [72/100], Loss: 523.3368\n",
      "Epoch [73/100], Loss: 524.0913\n",
      "Epoch [74/100], Loss: 523.3339\n",
      "Epoch [75/100], Loss: 523.7992\n",
      "Epoch [76/100], Loss: 523.0990\n",
      "Epoch [77/100], Loss: 524.0742\n",
      "Epoch [78/100], Loss: 524.1762\n",
      "Epoch [79/100], Loss: 522.7369\n",
      "Epoch [80/100], Loss: 523.3772\n",
      "Epoch [81/100], Loss: 523.4983\n",
      "Epoch [82/100], Loss: 522.6248\n",
      "Epoch [83/100], Loss: 522.5293\n",
      "Epoch [84/100], Loss: 522.5072\n",
      "Epoch [85/100], Loss: 522.2685\n",
      "Epoch [86/100], Loss: 522.3436\n",
      "Epoch [87/100], Loss: 520.8285\n",
      "Epoch [88/100], Loss: 521.8503\n",
      "Epoch [89/100], Loss: 522.2940\n",
      "Epoch [90/100], Loss: 520.9991\n",
      "Epoch [91/100], Loss: 521.7846\n",
      "Epoch [92/100], Loss: 521.2182\n",
      "Epoch [93/100], Loss: 520.3296\n",
      "Epoch [94/100], Loss: 521.2098\n",
      "Epoch [95/100], Loss: 520.9198\n",
      "Epoch [96/100], Loss: 519.7294\n",
      "Epoch [97/100], Loss: 520.1748\n",
      "Epoch [98/100], Loss: 520.1661\n",
      "Epoch [99/100], Loss: 520.4175\n",
      "Epoch [100/100], Loss: 520.3712\n",
      "Processed humantonsil successfully.\n",
      "Epoch [1/100], Loss: 3152.4789\n",
      "Epoch [2/100], Loss: 1326.1025\n",
      "Epoch [3/100], Loss: 894.2576\n",
      "Epoch [4/100], Loss: 773.5230\n",
      "Epoch [5/100], Loss: 722.9105\n",
      "Epoch [6/100], Loss: 696.2749\n",
      "Epoch [7/100], Loss: 681.8618\n",
      "Epoch [8/100], Loss: 673.6680\n",
      "Epoch [9/100], Loss: 666.2696\n",
      "Epoch [10/100], Loss: 661.8503\n",
      "Epoch [11/100], Loss: 660.0102\n",
      "Epoch [12/100], Loss: 656.7434\n",
      "Epoch [13/100], Loss: 655.2788\n",
      "Epoch [14/100], Loss: 653.7481\n",
      "Epoch [15/100], Loss: 652.0131\n",
      "Epoch [16/100], Loss: 651.7722\n",
      "Epoch [17/100], Loss: 650.5774\n",
      "Epoch [18/100], Loss: 648.8621\n",
      "Epoch [19/100], Loss: 648.5319\n",
      "Epoch [20/100], Loss: 648.7592\n",
      "Epoch [21/100], Loss: 646.5405\n",
      "Epoch [22/100], Loss: 646.1187\n",
      "Epoch [23/100], Loss: 644.8546\n",
      "Epoch [24/100], Loss: 645.8385\n",
      "Epoch [25/100], Loss: 644.6772\n",
      "Epoch [26/100], Loss: 643.7564\n",
      "Epoch [27/100], Loss: 643.9101\n",
      "Epoch [28/100], Loss: 643.0280\n",
      "Epoch [29/100], Loss: 642.8289\n",
      "Epoch [30/100], Loss: 642.7955\n",
      "Epoch [31/100], Loss: 641.1151\n",
      "Epoch [32/100], Loss: 641.9922\n",
      "Epoch [33/100], Loss: 641.1758\n",
      "Epoch [34/100], Loss: 640.1701\n",
      "Epoch [35/100], Loss: 640.5186\n",
      "Epoch [36/100], Loss: 639.4662\n",
      "Epoch [37/100], Loss: 640.2603\n",
      "Epoch [38/100], Loss: 639.5093\n",
      "Epoch [39/100], Loss: 638.6110\n",
      "Epoch [40/100], Loss: 638.5286\n",
      "Epoch [41/100], Loss: 638.4821\n",
      "Epoch [42/100], Loss: 638.3969\n",
      "Epoch [43/100], Loss: 638.5291\n",
      "Epoch [44/100], Loss: 637.2400\n",
      "Epoch [45/100], Loss: 636.7723\n",
      "Epoch [46/100], Loss: 637.7988\n",
      "Epoch [47/100], Loss: 637.1123\n",
      "Epoch [48/100], Loss: 636.0974\n",
      "Epoch [49/100], Loss: 636.8347\n",
      "Epoch [50/100], Loss: 635.8824\n",
      "Epoch [51/100], Loss: 635.6522\n",
      "Epoch [52/100], Loss: 636.6609\n",
      "Epoch [53/100], Loss: 635.7506\n",
      "Epoch [54/100], Loss: 636.1341\n",
      "Epoch [55/100], Loss: 635.7903\n",
      "Epoch [56/100], Loss: 634.9303\n",
      "Epoch [57/100], Loss: 635.1628\n",
      "Epoch [58/100], Loss: 634.5822\n",
      "Epoch [59/100], Loss: 634.1551\n",
      "Epoch [60/100], Loss: 635.2122\n",
      "Epoch [61/100], Loss: 634.0044\n",
      "Epoch [62/100], Loss: 633.6879\n",
      "Epoch [63/100], Loss: 633.7215\n",
      "Epoch [64/100], Loss: 634.0849\n",
      "Epoch [65/100], Loss: 633.0453\n",
      "Epoch [66/100], Loss: 634.5952\n",
      "Epoch [67/100], Loss: 633.8141\n",
      "Epoch [68/100], Loss: 632.7756\n",
      "Epoch [69/100], Loss: 633.5580\n",
      "Epoch [70/100], Loss: 633.1210\n",
      "Epoch [71/100], Loss: 632.1344\n",
      "Epoch [72/100], Loss: 632.5230\n",
      "Epoch [73/100], Loss: 633.1284\n",
      "Epoch [74/100], Loss: 632.7924\n",
      "Epoch [75/100], Loss: 633.0614\n",
      "Epoch [76/100], Loss: 632.0401\n",
      "Epoch [77/100], Loss: 632.2098\n",
      "Epoch [78/100], Loss: 631.8627\n",
      "Epoch [79/100], Loss: 630.8902\n",
      "Epoch [80/100], Loss: 632.2323\n",
      "Epoch [81/100], Loss: 631.4957\n",
      "Epoch [82/100], Loss: 631.9061\n",
      "Epoch [83/100], Loss: 631.9117\n",
      "Epoch [84/100], Loss: 631.2295\n",
      "Epoch [85/100], Loss: 631.4816\n",
      "Epoch [86/100], Loss: 630.9107\n",
      "Epoch [87/100], Loss: 631.1497\n",
      "Epoch [88/100], Loss: 631.5221\n",
      "Epoch [89/100], Loss: 631.2451\n",
      "Epoch [90/100], Loss: 631.0515\n",
      "Epoch [91/100], Loss: 630.7023\n",
      "Epoch [92/100], Loss: 630.4217\n",
      "Epoch [93/100], Loss: 630.8810\n",
      "Epoch [94/100], Loss: 629.9452\n",
      "Epoch [95/100], Loss: 630.4358\n",
      "Epoch [96/100], Loss: 628.9690\n",
      "Epoch [97/100], Loss: 629.8215\n",
      "Epoch [98/100], Loss: 629.4529\n",
      "Epoch [99/100], Loss: 629.9259\n",
      "Epoch [100/100], Loss: 629.2858\n",
      "Processed mousekidney successfully.\n",
      "Epoch [1/100], Loss: 4206.4317\n",
      "Epoch [2/100], Loss: 2817.5026\n",
      "Epoch [3/100], Loss: 2056.3537\n",
      "Epoch [4/100], Loss: 1539.2746\n",
      "Epoch [5/100], Loss: 1247.5452\n",
      "Epoch [6/100], Loss: 1080.2542\n",
      "Epoch [7/100], Loss: 975.8962\n",
      "Epoch [8/100], Loss: 912.1494\n",
      "Epoch [9/100], Loss: 870.9665\n",
      "Epoch [10/100], Loss: 842.3260\n",
      "Epoch [11/100], Loss: 826.4010\n",
      "Epoch [12/100], Loss: 804.0844\n",
      "Epoch [13/100], Loss: 792.3732\n",
      "Epoch [14/100], Loss: 783.0860\n",
      "Epoch [15/100], Loss: 774.5395\n",
      "Epoch [16/100], Loss: 769.3851\n",
      "Epoch [17/100], Loss: 764.8070\n",
      "Epoch [18/100], Loss: 759.9007\n",
      "Epoch [19/100], Loss: 757.2228\n",
      "Epoch [20/100], Loss: 755.6085\n",
      "Epoch [21/100], Loss: 752.7598\n",
      "Epoch [22/100], Loss: 746.9962\n",
      "Epoch [23/100], Loss: 747.3197\n",
      "Epoch [24/100], Loss: 746.3966\n",
      "Epoch [25/100], Loss: 745.4537\n",
      "Epoch [26/100], Loss: 746.0874\n",
      "Epoch [27/100], Loss: 742.9232\n",
      "Epoch [28/100], Loss: 738.6586\n",
      "Epoch [29/100], Loss: 743.1595\n",
      "Epoch [30/100], Loss: 739.4789\n",
      "Epoch [31/100], Loss: 738.8281\n",
      "Epoch [32/100], Loss: 740.4515\n",
      "Epoch [33/100], Loss: 741.2077\n",
      "Epoch [34/100], Loss: 737.8211\n",
      "Epoch [35/100], Loss: 737.5420\n",
      "Epoch [36/100], Loss: 737.5883\n",
      "Epoch [37/100], Loss: 733.6442\n",
      "Epoch [38/100], Loss: 735.9807\n",
      "Epoch [39/100], Loss: 737.5641\n",
      "Epoch [40/100], Loss: 733.3283\n",
      "Epoch [41/100], Loss: 733.7771\n",
      "Epoch [42/100], Loss: 737.0805\n",
      "Epoch [43/100], Loss: 735.1744\n",
      "Epoch [44/100], Loss: 734.2176\n",
      "Epoch [45/100], Loss: 733.5758\n",
      "Epoch [46/100], Loss: 730.0777\n",
      "Epoch [47/100], Loss: 734.3017\n",
      "Epoch [48/100], Loss: 731.3882\n",
      "Epoch [49/100], Loss: 733.0145\n",
      "Epoch [50/100], Loss: 731.1589\n",
      "Epoch [51/100], Loss: 731.5826\n",
      "Epoch [52/100], Loss: 731.0362\n",
      "Epoch [53/100], Loss: 734.6815\n",
      "Epoch [54/100], Loss: 730.1756\n",
      "Epoch [55/100], Loss: 725.9223\n",
      "Epoch [56/100], Loss: 716.8138\n",
      "Epoch [57/100], Loss: 709.4864\n",
      "Epoch [58/100], Loss: 701.8910\n",
      "Epoch [59/100], Loss: 692.8371\n",
      "Epoch [60/100], Loss: 690.2843\n",
      "Epoch [61/100], Loss: 684.3216\n",
      "Epoch [62/100], Loss: 682.9502\n",
      "Epoch [63/100], Loss: 677.6456\n",
      "Epoch [64/100], Loss: 677.7670\n",
      "Epoch [65/100], Loss: 677.0111\n",
      "Epoch [66/100], Loss: 673.1449\n",
      "Epoch [67/100], Loss: 673.0447\n",
      "Epoch [68/100], Loss: 673.4812\n",
      "Epoch [69/100], Loss: 674.1685\n",
      "Epoch [70/100], Loss: 672.8323\n",
      "Epoch [71/100], Loss: 671.9582\n",
      "Epoch [72/100], Loss: 672.9205\n",
      "Epoch [73/100], Loss: 669.0368\n",
      "Epoch [74/100], Loss: 671.8896\n",
      "Epoch [75/100], Loss: 668.3173\n",
      "Epoch [76/100], Loss: 670.1063\n",
      "Epoch [77/100], Loss: 668.1378\n",
      "Epoch [78/100], Loss: 667.1243\n",
      "Epoch [79/100], Loss: 668.2108\n",
      "Epoch [80/100], Loss: 666.4181\n",
      "Epoch [81/100], Loss: 667.3549\n",
      "Epoch [82/100], Loss: 666.8666\n",
      "Epoch [83/100], Loss: 663.7798\n",
      "Epoch [84/100], Loss: 664.2922\n",
      "Epoch [85/100], Loss: 664.0637\n",
      "Epoch [86/100], Loss: 665.7954\n",
      "Epoch [87/100], Loss: 665.5038\n",
      "Epoch [88/100], Loss: 662.5894\n",
      "Epoch [89/100], Loss: 666.2495\n",
      "Epoch [90/100], Loss: 664.0916\n",
      "Epoch [91/100], Loss: 663.3800\n",
      "Epoch [92/100], Loss: 661.5631\n",
      "Epoch [93/100], Loss: 663.2439\n",
      "Epoch [94/100], Loss: 660.4428\n",
      "Epoch [95/100], Loss: 661.2322\n",
      "Epoch [96/100], Loss: 660.8634\n",
      "Epoch [97/100], Loss: 661.6529\n",
      "Epoch [98/100], Loss: 660.1006\n",
      "Epoch [99/100], Loss: 660.7859\n",
      "Epoch [100/100], Loss: 659.7319\n",
      "Processed mouseintestine successfully.\n",
      "Epoch [1/100], Loss: 3405.3727\n",
      "Epoch [2/100], Loss: 1587.6337\n",
      "Epoch [3/100], Loss: 1056.1632\n",
      "Epoch [4/100], Loss: 890.8176\n",
      "Epoch [5/100], Loss: 820.0556\n",
      "Epoch [6/100], Loss: 785.2752\n",
      "Epoch [7/100], Loss: 765.3357\n",
      "Epoch [8/100], Loss: 751.7024\n",
      "Epoch [9/100], Loss: 742.8003\n",
      "Epoch [10/100], Loss: 736.1722\n",
      "Epoch [11/100], Loss: 734.5396\n",
      "Epoch [12/100], Loss: 731.0843\n",
      "Epoch [13/100], Loss: 728.4417\n",
      "Epoch [14/100], Loss: 727.7114\n",
      "Epoch [15/100], Loss: 724.0239\n",
      "Epoch [16/100], Loss: 724.8037\n",
      "Epoch [17/100], Loss: 720.6754\n",
      "Epoch [18/100], Loss: 721.2261\n",
      "Epoch [19/100], Loss: 720.7175\n",
      "Epoch [20/100], Loss: 720.6758\n",
      "Epoch [21/100], Loss: 718.6333\n",
      "Epoch [22/100], Loss: 717.8750\n",
      "Epoch [23/100], Loss: 717.6521\n",
      "Epoch [24/100], Loss: 717.8977\n",
      "Epoch [25/100], Loss: 717.6046\n",
      "Epoch [26/100], Loss: 715.5962\n",
      "Epoch [27/100], Loss: 716.1846\n",
      "Epoch [28/100], Loss: 714.8403\n",
      "Epoch [29/100], Loss: 712.9745\n",
      "Epoch [30/100], Loss: 713.8321\n",
      "Epoch [31/100], Loss: 712.5800\n",
      "Epoch [32/100], Loss: 712.0985\n",
      "Epoch [33/100], Loss: 709.7090\n",
      "Epoch [34/100], Loss: 707.1837\n",
      "Epoch [35/100], Loss: 702.7880\n",
      "Epoch [36/100], Loss: 698.1626\n",
      "Epoch [37/100], Loss: 697.1033\n",
      "Epoch [38/100], Loss: 695.4784\n",
      "Epoch [39/100], Loss: 692.2711\n",
      "Epoch [40/100], Loss: 693.3296\n",
      "Epoch [41/100], Loss: 691.6271\n",
      "Epoch [42/100], Loss: 688.6072\n",
      "Epoch [43/100], Loss: 689.4392\n",
      "Epoch [44/100], Loss: 685.5452\n",
      "Epoch [45/100], Loss: 687.0411\n",
      "Epoch [46/100], Loss: 685.0226\n",
      "Epoch [47/100], Loss: 683.4275\n",
      "Epoch [48/100], Loss: 684.4492\n",
      "Epoch [49/100], Loss: 683.3189\n",
      "Epoch [50/100], Loss: 681.6169\n",
      "Epoch [51/100], Loss: 680.7174\n",
      "Epoch [52/100], Loss: 680.2554\n",
      "Epoch [53/100], Loss: 680.1967\n",
      "Epoch [54/100], Loss: 680.1920\n",
      "Epoch [55/100], Loss: 679.3055\n",
      "Epoch [56/100], Loss: 677.5783\n",
      "Epoch [57/100], Loss: 676.8341\n",
      "Epoch [58/100], Loss: 676.6618\n",
      "Epoch [59/100], Loss: 675.6460\n",
      "Epoch [60/100], Loss: 673.4191\n",
      "Epoch [61/100], Loss: 671.4663\n",
      "Epoch [62/100], Loss: 671.0464\n",
      "Epoch [63/100], Loss: 668.3473\n",
      "Epoch [64/100], Loss: 667.1443\n",
      "Epoch [65/100], Loss: 664.0366\n",
      "Epoch [66/100], Loss: 663.2322\n",
      "Epoch [67/100], Loss: 661.1146\n",
      "Epoch [68/100], Loss: 661.6352\n",
      "Epoch [69/100], Loss: 658.8895\n",
      "Epoch [70/100], Loss: 658.1462\n",
      "Epoch [71/100], Loss: 658.8198\n",
      "Epoch [72/100], Loss: 657.3676\n",
      "Epoch [73/100], Loss: 658.9446\n",
      "Epoch [74/100], Loss: 656.4466\n",
      "Epoch [75/100], Loss: 657.6865\n",
      "Epoch [76/100], Loss: 657.5918\n",
      "Epoch [77/100], Loss: 656.0836\n",
      "Epoch [78/100], Loss: 655.9319\n",
      "Epoch [79/100], Loss: 655.2747\n",
      "Epoch [80/100], Loss: 654.6640\n",
      "Epoch [81/100], Loss: 654.8618\n",
      "Epoch [82/100], Loss: 655.6724\n",
      "Epoch [83/100], Loss: 654.3905\n",
      "Epoch [84/100], Loss: 653.3116\n",
      "Epoch [85/100], Loss: 654.0806\n",
      "Epoch [86/100], Loss: 654.5911\n",
      "Epoch [87/100], Loss: 653.9844\n",
      "Epoch [88/100], Loss: 654.6601\n",
      "Epoch [89/100], Loss: 654.3710\n",
      "Epoch [90/100], Loss: 652.9583\n",
      "Epoch [91/100], Loss: 652.2569\n",
      "Epoch [92/100], Loss: 651.7966\n",
      "Epoch [93/100], Loss: 652.4289\n",
      "Epoch [94/100], Loss: 652.3006\n",
      "Epoch [95/100], Loss: 651.4066\n",
      "Epoch [96/100], Loss: 650.8747\n",
      "Epoch [97/100], Loss: 651.3800\n",
      "Epoch [98/100], Loss: 649.8179\n",
      "Epoch [99/100], Loss: 650.9444\n",
      "Epoch [100/100], Loss: 649.7171\n",
      "Processed mousecolon successfully.\n",
      "Epoch [1/100], Loss: 4132.3939\n",
      "Epoch [2/100], Loss: 2546.8730\n",
      "Epoch [3/100], Loss: 1758.5357\n",
      "Epoch [4/100], Loss: 1394.5482\n",
      "Epoch [5/100], Loss: 1232.0662\n",
      "Epoch [6/100], Loss: 1148.7488\n",
      "Epoch [7/100], Loss: 1094.5244\n",
      "Epoch [8/100], Loss: 1067.3085\n",
      "Epoch [9/100], Loss: 1045.5901\n",
      "Epoch [10/100], Loss: 1029.7445\n",
      "Epoch [11/100], Loss: 1018.1561\n",
      "Epoch [12/100], Loss: 1009.7528\n",
      "Epoch [13/100], Loss: 1006.7487\n",
      "Epoch [14/100], Loss: 1001.4516\n",
      "Epoch [15/100], Loss: 993.0564\n",
      "Epoch [16/100], Loss: 992.1070\n",
      "Epoch [17/100], Loss: 987.3786\n",
      "Epoch [18/100], Loss: 987.1855\n",
      "Epoch [19/100], Loss: 983.4085\n",
      "Epoch [20/100], Loss: 982.9021\n",
      "Epoch [21/100], Loss: 981.3291\n",
      "Epoch [22/100], Loss: 980.7789\n",
      "Epoch [23/100], Loss: 980.7951\n",
      "Epoch [24/100], Loss: 979.7959\n",
      "Epoch [25/100], Loss: 978.6863\n",
      "Epoch [26/100], Loss: 975.8130\n",
      "Epoch [27/100], Loss: 974.6879\n",
      "Epoch [28/100], Loss: 973.9444\n",
      "Epoch [29/100], Loss: 975.2283\n",
      "Epoch [30/100], Loss: 972.4914\n",
      "Epoch [31/100], Loss: 971.4655\n",
      "Epoch [32/100], Loss: 975.0724\n",
      "Epoch [33/100], Loss: 970.1149\n",
      "Epoch [34/100], Loss: 970.3581\n",
      "Epoch [35/100], Loss: 969.7585\n",
      "Epoch [36/100], Loss: 969.0390\n",
      "Epoch [37/100], Loss: 969.6155\n",
      "Epoch [38/100], Loss: 968.9231\n",
      "Epoch [39/100], Loss: 967.9770\n",
      "Epoch [40/100], Loss: 968.1172\n",
      "Epoch [41/100], Loss: 967.7012\n",
      "Epoch [42/100], Loss: 967.8232\n",
      "Epoch [43/100], Loss: 966.7657\n",
      "Epoch [44/100], Loss: 965.4220\n",
      "Epoch [45/100], Loss: 964.3787\n",
      "Epoch [46/100], Loss: 964.8656\n",
      "Epoch [47/100], Loss: 964.3499\n",
      "Epoch [48/100], Loss: 963.6025\n",
      "Epoch [49/100], Loss: 967.4465\n",
      "Epoch [50/100], Loss: 961.9950\n",
      "Epoch [51/100], Loss: 962.7911\n",
      "Epoch [52/100], Loss: 964.9009\n",
      "Epoch [53/100], Loss: 963.1821\n",
      "Epoch [54/100], Loss: 962.6167\n",
      "Epoch [55/100], Loss: 962.9282\n",
      "Epoch [56/100], Loss: 963.2919\n",
      "Epoch [57/100], Loss: 961.8059\n",
      "Epoch [58/100], Loss: 961.5046\n",
      "Epoch [59/100], Loss: 960.0399\n",
      "Epoch [60/100], Loss: 962.3627\n",
      "Epoch [61/100], Loss: 961.0649\n",
      "Epoch [62/100], Loss: 961.4017\n",
      "Epoch [63/100], Loss: 958.1222\n",
      "Epoch [64/100], Loss: 960.1863\n",
      "Epoch [65/100], Loss: 959.6515\n",
      "Epoch [66/100], Loss: 960.7273\n",
      "Epoch [67/100], Loss: 961.9042\n",
      "Epoch [68/100], Loss: 960.0262\n",
      "Epoch [69/100], Loss: 958.5564\n",
      "Epoch [70/100], Loss: 959.4414\n",
      "Epoch [71/100], Loss: 957.3786\n",
      "Epoch [72/100], Loss: 958.8497\n",
      "Epoch [73/100], Loss: 959.0414\n",
      "Epoch [74/100], Loss: 958.7693\n",
      "Epoch [75/100], Loss: 955.7123\n",
      "Epoch [76/100], Loss: 952.6732\n",
      "Epoch [77/100], Loss: 948.4237\n",
      "Epoch [78/100], Loss: 946.4749\n",
      "Epoch [79/100], Loss: 943.0729\n",
      "Epoch [80/100], Loss: 940.6161\n",
      "Epoch [81/100], Loss: 937.6370\n",
      "Epoch [82/100], Loss: 936.1265\n",
      "Epoch [83/100], Loss: 935.3172\n",
      "Epoch [84/100], Loss: 936.0570\n",
      "Epoch [85/100], Loss: 933.2093\n",
      "Epoch [86/100], Loss: 931.8105\n",
      "Epoch [87/100], Loss: 934.6017\n",
      "Epoch [88/100], Loss: 930.9725\n",
      "Epoch [89/100], Loss: 931.2305\n",
      "Epoch [90/100], Loss: 929.9285\n",
      "Epoch [91/100], Loss: 931.7224\n",
      "Epoch [92/100], Loss: 929.2056\n",
      "Epoch [93/100], Loss: 930.7913\n",
      "Epoch [94/100], Loss: 931.2605\n",
      "Epoch [95/100], Loss: 929.5039\n",
      "Epoch [96/100], Loss: 928.7505\n",
      "Epoch [97/100], Loss: 928.1845\n",
      "Epoch [98/100], Loss: 928.7733\n",
      "Epoch [99/100], Loss: 926.4897\n",
      "Epoch [100/100], Loss: 927.8506\n",
      "Processed mousespleen successfully.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def gaussian_kernel(distances, bandwidth):\n",
    "    return torch.exp(-0.5 * (distances / bandwidth) ** 2)\n",
    "\n",
    "def weighted_average_with_neighbors(rna_data, top_n=15, bandwidth=0.05):\n",
    "    coordinates = torch.tensor(rna_data[['X', 'Y']].values, dtype=torch.float32).to(device)\n",
    "    feature_columns = rna_data.columns.difference(['X', 'Y'])\n",
    "    feature_values = torch.tensor(rna_data[feature_columns].values, dtype=torch.float32).to(device)\n",
    "\n",
    "    distances = torch.cdist(coordinates, coordinates, p=2)\n",
    "\n",
    "    weighted_averages = torch.zeros((len(rna_data), len(feature_columns)), device=device)\n",
    "\n",
    "    for idx in range(len(rna_data)):\n",
    "        current_distances = distances[idx]\n",
    "        neighbor_indices = torch.argsort(current_distances)[:top_n + 1]\n",
    "        weights = gaussian_kernel(current_distances[neighbor_indices], bandwidth)\n",
    "        normalized_weights = weights / weights.sum()\n",
    "        weighted_average = torch.matmul(normalized_weights, feature_values[neighbor_indices])\n",
    "        weighted_averages[idx] = weighted_average.round().to(torch.int)\n",
    "\n",
    "    return pd.DataFrame(weighted_averages.cpu().numpy(), index=rna_data.index, columns=feature_columns)\n",
    "\n",
    "tissues= ['humanGBM', 'humanskin', 'humanthymus', 'humanspleen', 'humantonsil', 'mousekidney', 'mouseintestine', 'mousecolon', 'mousespleen']\n",
    "\n",
    "for tissue in tissues:\n",
    "    rna_data = None\n",
    "    protein_data = None\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        if tissue in filename and filename.endswith(\"RNA.tsv.gz\"):\n",
    "            rna_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        elif tissue in filename and filename.endswith(\"protein.tsv.gz\"):\n",
    "            protein_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "            \n",
    "    rna_data.columns = rna_data.columns.astype(str)\n",
    "    protein_data.columns = protein_data.columns.astype(str)\n",
    "            \n",
    "    rna_data = rna_data.sort_values(by='X')\n",
    "    protein_data = protein_data.sort_values(by='X')\n",
    "    \n",
    "    rna_data = rna_data.reset_index(drop=True)\n",
    "    protein_data = protein_data.reset_index(drop=True)\n",
    "    rna_data.index = rna_data.index.astype(str)\n",
    "    protein_data.index = protein_data.index.astype(str)\n",
    "  \n",
    "    rna_data[['X', 'Y']] = rna_data['X'].str.split('x', expand=True)\n",
    "    rna_data['X'] = pd.to_numeric(rna_data['X'], errors='coerce')\n",
    "    rna_data['Y'] = pd.to_numeric(rna_data['Y'], errors='coerce')\n",
    "    weighted_rna_data = weighted_average_with_neighbors(rna_data)\n",
    "    protein_data.drop(['X'], axis=1, inplace=True)\n",
    "    \n",
    "    rna_train, rna_test = train_test_split(weighted_rna_data, test_size=0.2, random_state=42)\n",
    "    protein_train = protein_data.loc[rna_train.index]\n",
    "    protein_test = protein_data.loc[rna_test.index]\n",
    "        \n",
    "    adata_rna_train = sc.AnnData(rna_train)\n",
    "    sc.pp.normalize_total(adata_rna_train, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_rna_train)\n",
    "    sc.pp.highly_variable_genes(adata_rna_train, n_top_genes=4000, flavor='seurat', subset=True)\n",
    "    counts_norm = adata_rna_train.X\n",
    "    rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "  \n",
    "    adata_protein_train = sc.AnnData(protein_train) \n",
    "    sc.pp.normalize_total(adata_protein_train, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_protein_train)\n",
    "    counts_norm = adata_protein_train.X\n",
    "    protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "    \n",
    "    combined_data = torch.cat([rna_counts_norm, protein_counts_norm], dim=1).to(device)\n",
    "\n",
    "    input_dim = combined_data.shape[1] \n",
    "    latent_dim = 32\n",
    "    model = VAE(input_dim, latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 100\n",
    "    \n",
    "    batch_size = 64\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        permutation = torch.randperm(combined_data.size(0))\n",
    "\n",
    "        for i in range(0, combined_data.size(0), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_data = combined_data[indices]\n",
    "\n",
    "            reconstructed_data, mean, logvar = model(batch_data)\n",
    "            loss = vae_loss(reconstructed_data, batch_data, mean, logvar, lambda_kl=0.0001)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(combined_data):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        combined_data = torch.cat([rna_counts_norm, torch.zeros(rna_counts_norm.shape[0], protein_counts_norm.shape[1]).to(device)], dim=1)\n",
    "        reconstructed_data, mean, logvar = model(combined_data)\n",
    "        reconstructed_protein_counts = reconstructed_data[:, rna_counts_norm.shape[1]:]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n",
    "        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n",
    "        avg_corr_pearson = pcc.mean()\n",
    "        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'RMSE': [rmse],\n",
    "            'Pearson Correlation': [avg_corr_pearson],\n",
    "            'SSIM':ssim_val\n",
    "        })\n",
    "        \n",
    "        results_file_path = os.path.join(results_dir, f\"{tissue}_training_results.csv\")\n",
    "        results_df.to_csv(results_file_path, index=False)\n",
    "        \n",
    "        adata_rna_test = sc.AnnData(rna_test) \n",
    "        sc.pp.normalize_total(adata_rna_test, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_rna_test)\n",
    "        counts_norm = adata_rna_test[:,  adata_rna_train.var_names].X\n",
    "        rna_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "    \n",
    "        adata_protein_test = sc.AnnData(protein_test) \n",
    "        sc.pp.normalize_total(adata_protein_test, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_protein_test)\n",
    "        counts_norm = adata_protein_test.X\n",
    "        protein_counts_norm = torch.FloatTensor(counts_norm).to(device)\n",
    "                \n",
    "        combined_data = torch.cat([rna_counts_norm, torch.zeros(rna_counts_norm.shape[0], protein_counts_norm.shape[1]).to(device)], dim=1)\n",
    "        reconstructed_data, mean, logvar = model(combined_data)\n",
    "        reconstructed_protein_counts = reconstructed_data[:, rna_counts_norm.shape[1]:]\n",
    "          \n",
    "        rmse = np.sqrt(mean_squared_error(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy()))\n",
    "        pcc = pd.DataFrame(protein_counts_norm.cpu().numpy()).corrwith(pd.DataFrame(reconstructed_protein_counts.cpu().numpy()), axis=1, method='pearson')\n",
    "        avg_corr_pearson = pcc.mean()\n",
    "        ssim_val = ssim(protein_counts_norm.cpu().numpy(), reconstructed_protein_counts.cpu().numpy(), data_range=reconstructed_protein_counts.cpu().numpy().max() - reconstructed_protein_counts.cpu().numpy().min())\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'RMSE': [rmse],\n",
    "            'Pearson Correlation': [avg_corr_pearson],\n",
    "            'SSIM':ssim_val\n",
    "        })\n",
    "    \n",
    "    results_file_path = os.path.join(results_dir, f\"{tissue}_results.csv\")\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "    print(f\"Processed {tissue} successfully.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
